# OIS模型

## OSI七层模型

>![img](https://img-blog.csdn.net/20160927104630528)
>
>- 建立七层模型最主要的目的：解决异种网络互连时所遇到的兼容性问题；
>- 优点：将服务、接口、协议分离开来（想接口），这样各层之间就有很强的独立性，各层各负其职。且一旦发生故障，可以立即定位；
>
>1. 物理层：想  两台主机之间传输比特流；
>2. 链路层：可以发比特流但是没有格式就会乱七八糟，于是就有了”**帧**”。采用了一种”帧”的数据块进行传输，为了确保数据通信的准确，实现数据有效的**差错控制**，加入了**检错**等功能；
>3. 网络层：前两层都是在于可以发数据，以及发的数据是否正确，然而如果连着两台电脑还行，多台电脑而又只想让其中一台可以通信，则需要**路由**。**选择性的发**，那每台电脑就得有自己的身份，于是出现了**IP协议**等；在位于不同地理位置的网络中的两个主机系统之间提供连接和路径选择；
>4. 传输层：定义传输数据的协议端口号，以及**流控**  和  **差错校验**；比特流传输的过程不可能会一直顺畅，偶尔出现中断很正常，如果人为制定出单位，分成一个个的信息段，从中又衍生了**报文**，结合上面几层，我们就可以有目标的发送正确数据给某台计算机了；
>5. 会话层：计算机收到了发送的数据，但是有那么多进程，具体哪个进程需要用到这个数据，则把他输送到那个进程。例如：如果80端口要用，所以系统内数据通信，将接收端口数据送至需求端口。
>6. 表示层：现在正确接收到了需要的数据，但是因为数据在传输过程中可能基于安全性，或者是算法上的压缩，还有就是网络类型不同。那就得有一个沟通的桥梁来整理整理，**还原出原本应该有的表示**，类似于一个拆快递的过程。
>7. 应用层：现在正确接收到了需要的数据，但是因为数据在传输过程中可能基于安全性，或者是算法上的压缩，还有就是网络类型不同。那就得有一个沟通的桥梁来整理整理，还原出原本应该有的表示，类似于一个拆快递的过程；

## TCP/IP 7层模型和4层模型

>![preview](https://pic4.zhimg.com/v2-4140cd55e3a6751407d4680c43382e63_r.jpg)

>1、`OSI`参考模型和`TCP/IP`模型之共同点：都是基于独立的协议栈的概念；它们的功能大体相似，在两个模型中，传输层及以上的各层都是为了通信的进程提供点到点、与网络无关的传输服务；`OSI`参考模型，与`TCP/IP`模型传输层以上的层都以应用为主导。
>
>2、`OSI`参考模型与`TCP/IP`模型的主要差别：`TCP/IP`一开始就考虑到多种异构网的互联问题，并将网际协议`IP`作为`TCP/IP`的霞要组成部门。但`ISO`最初只考虑到使用一种标准的公用数据网将各种不同的系统互联在一起。`TCP/IP`一开始就对面向连接和无连接并重，而`OSI`在开始时只强调面向连接服务。`TCP/IP`有较好的网络管理功能，而这方面`OSI`至后来才开始这个问题，两者有所不同。
>
>3、`OSI`参考模型与`TCP/IP`模型的相瓦关系：`OSI`模型是对发生在网络设备间的信息传输过程的一种理论化的描述，他仅仅是一种模型，并没有定义如何通过硬件和软件实现每一。层功能，但可以很有效地帮助我们理解数据传输的过程。



## 数据如何在各层传输的

>![img](https://img-blog.csdn.net/20180408150555706)
>
>![preview](https://pic3.zhimg.com/v2-dd0436af596dfb627a9cc63b288f738e_r.jpg)

>以发邮件为例子 在这里进行传输
>
>1. **应用层**：首先需要选择邮件应用比如163邮箱，或者qq邮箱，outlook等邮件应用
>2. **表示层**：邮件编辑好后，点击发送，这时候它会将需要传输的数据进行编码，加密，压缩等操作
>3. **会话层**：数据准备好后，邮件马上就需要进行发送，这里实际上就是建立了一个邮件发送者何接收者之间的会话，它是一个概念性质的，比如发送后如果执行撤销可以中断会话
>4. **传输层**：传输层会对五层数据包进行进一步的封装，为该数据包添加一个TCP/UDP头部，其中含有源端口号和目的端口号，源端口号就是邮件应用的端口号
>5. **网络层**：拿到传输层的数据包后，网络层会对该数据包添加一个IP包头，其中包含了目的地网络地址，用于指示沿途的路由器，再发送出去
>6. **数据链路层**：当上三层的数据包到了数据链路层，同样的给数据包加上头部(MAC地址)和尾部(FCS)封装成帧
>7. **物理层**：二层的数据帧包会被转化成一段连续的比特流，然后以电脉冲的形式传输到指定的交换机(数据链路层)
>8. 在传输过程中可能会遇到很多的中间节点，不断的经过路由器，交换机进行中转，最终到达接收端，经过层层解封装后展示在对方的收件箱。





# ==-------------------------------------------------------------------应用层==

# ==HTTP==



>HTTP超文本传输协议，是一个基于TCP/IP通信协议来传递数据的协议，传输的数据类型为HTML 文件,、图片文件, 查询结果等。
>
>**TPC/IP协议是传输层协议，主要解决数据如何在网络中传输，而HTTP是应用层协议，主要解决如何包装数据**
>
>`WEB`使用`HTTP`协议作应用层协议，以封装`HTTP `文本信息，然后使用TCP/IP做传输层协议将它发到网络上
>
>##### HTTP协议特点呗 
>
>1. http协议支持客户端/服务端模式，也是一种请求/响应模式的协议。
>2. 简单快速：客户向服务器请求服务时，只需传送请求方法和路径。请求方法常用的有GET、HEAD、POST。
>3. 灵活：HTTP允许传输任意类型的数据对象。传输的类型由Content-Type加以标记。
>4. 无连接：限制每次连接只处理一个请求。服务器处理完请求，并收到客户的应答后，即断开连接，但是却不利于客户端与服务器保持会话连接，为了弥补这种不足，产生了两项记录http状态的技术，一个叫做Cookie,一个叫做Session。
>5. 无状态：无状态是指协议对于事务处理没有记忆，后续处理需要前面的信息，则必须重传。



## HTTP的工作原理

>   一次HTTP操作称为一个事务，其工作整个过程如下：
>
>###### 1.地址解析
>
> 如用客户端浏览器请求这个页面：[http://localhost.com:8080/index.htm](http://localhost:8080/simple.htm)从中分解出协议名、主机名、端口、对象路径等部分，需要域名系统DNS解析域名localhost.com,得主机的IP地址。
>
>
>
>###### 2.封装HTTP请求数据包
>
>封装HTTP请求数据包 :把以上部分结合本机自己的信息，封装成一个HTTP请求数据包
>
>
>
>###### 3.装成TCP包，建立TCP连接（TCP的三次握手）
>
>在HTTP工作开始之前，客户机（Web浏览器）首先要通过网络与服务器建立连接，该连接是通过TCP来完成的，该协议与IP协议共同构建Internet，即著名的TCP/IP协议族，因此Internet又被称作是TCP/IP网络。HTTP是比TCP更高层次的应用层协议，根据规则，只有低层协议建立之后才能进行更高层协议的连接，因此，首先要建立TCP连接，一般TCP连接的端口号是80。这里是8080端口
>
>
>
>###### 4.客户机发送请求命令
>
>建立连接后，客户机发送一个请求给服务器，请求方式的格式为：统一资源标识符（URI：Uniform Resource Identifier）、协议版本号，后边是MIME信息包括请求修饰符、客户机信息和可能的内容。
>
>
>
>###### 5.服务器响应
>
>服务器接到请求后，给予相应的响应信息，其格式为一个状态行，包括信息的协议版本号、一个成功或错误的代码，后边是MIME信息包括服务器信息、实体信息和可能的内容。实体消息是服务器向浏览器发送头信息后，它会发送一个空白行来表示头信息的发送到此结束，接着，它就以Content-Type应答头信息所描述的格式发送用户所请求的实际数据
>
>
>
>###### 6.服务器关闭TCP连接
>
>   一般情况下，一旦Web服务器向浏览器发送了请求数据，它就要关闭TCP连接，然后如果浏览器或者服务器在其头信息加入了这行代码
>
>  Connection:keep-alive:TCP连接在发送后将仍然保持打开状态，于是，浏览器可以继续通过相同的连接发送请求。保持连接节省了为每个请求建立新连接所需的时间，还节约了网络带宽。







## 浏览器中输入网站  页面中输入url相应什么

>###### 1、客户端连接到Web服务器 建立TCP连接
>
>首先客户端浏览器通过DNS协议解析到例如www.baidu,com对应的IP地址  通过这个IP地址找到客户端和服务器的路径，客户端向服务器段发起HTTP会话，与Web服务器的HTTP端口（默认为80）建立一个TCP套接字连接
>
>###### 2、发送HTTP请求
>
>通过TCP套接字，客户端向Web服务器发送一个文本的请求报文，一个请求报文由请求行、请求头部、空行和请求数据4部分组成。
>
>###### 3、服务器接受请求并返回HTTP响应
>
>Web服务器解析请求，定位请求资源。服务器将资源复写到TCP套接字，由客户端读取。一个响应由状态行、响应头部、空行和响应数据4部分组成。
>
>###### 4、释放连接[TCP连接](https://www.jianshu.com/p/ef892323e68f)
>
>若connection 模式为close，则服务器主动关闭[TCP连接](https://www.jianshu.com/p/ef892323e68f)，客户端被动关闭连接，释放[TCP连接](https://www.jianshu.com/p/ef892323e68f);若connection 模式为keepalive，则该连接会保持一段时间，默认是两小时 ， 在该时间内可以继续接收请求;
>
>###### 5、客户端浏览器解析HTML内容
>
>客户端浏览器首先解析状态行，查看表明请求是否成功的状态代码。然后解析每一个响应头，响应头告知以下为若干字节的HTML文档和文档的字符集。客户端浏览器读取响应数据HTML、css、js，根据HTML的语法对其进行格式化，并在浏览器窗口中显示。



## HTTP1.0和HTTP1.1 、HTTP2.0区别

>HTTP1.0最早在网页中使用是在1996年，那个时候只是使用一些较为简单的网页上和网络请求上，而HTTP1.1则在1999年才开始广泛应用于现在的各大浏览器网络请求中，同时HTTP1.1也是当前使用最为广泛的HTTP协议。 主要区别主要体现在：
>
>HTTP1.0与HTTP2.0区别
>
>1. **缓存处理**:相比于HTTP1.0,HTTP1.1则引入了更多的缓存控制策略 **Cache-Control** ：客户端缓存多长时间
>3. **增加状态码 错误通知的管理**，在HTTP1.1中新增了24个错误状态响应码，410（Gone）表示服务器上的某个资源被永久性的删除。
>4. **虚拟主机 Host头处理**，在HTTP1.0中认为每台服务器都绑定一个唯一的IP地址，因此，请求消息中的URL并没有传递主机名（hostname）。但随着虚拟主机技术的发展，在一台物理服务器上可以存在多个虚拟主机（Multi-homed Web Servers），并且它们共享一个IP地址。HTTP1.1的请求消息和响应消息都应支持Host头域，且请求消息中如果没有Host头域会报告一个错误（400 Bad Request）。
>5. **支持长连接这个重要**，HTTP 1.1支持长连接（PersistentConnection）和请求的流水线（Pipelining）处理，在一个TCP连接上可以传送多个HTTP请求和响应，减少了建立和关闭连接的消耗和延迟，在HTTP1.1中默认开启Connection： keep-alive，一定程度上弥补了HTTP1.0每次请求都要创建连接的缺点。 Pipeling管道解决方式为，**若干个请求排队串行化单线程处理**，后面的请求等待前面请求的返回才能获得执行机会，一旦有某请求超时等，后续请求只能被阻塞，毫无办法，也就是人们常说的线头阻塞；

------

>HTTP2.0与 HTTP1.0区别
>
>1. **新的二进制格式**（Binary Format），HTTP1.x的解析是基于文本。基于文本协议的格式解析存在天然缺陷，文本的表现形式有多样性，要做到健壮性考虑的场景必然很多，二进制则不同，只认0和1的组合。基于这种考虑HTTP2.0的协议解析决定采用二进制格式，实现方便且健壮。
>2. **多路复用**（MultiPlexing），即连接共享。一个request对应一个id，这样一个连接上可以有多个request，多个请求可同时在一个连接上**并行**执行。某个请求任务耗时严重，不会影响到其它连接的正常执行，接收方可以根据request的 id将request再归属到各自不同的服务端请求里面。
>3. **header压缩**，如上文中所言，对前面提到过HTTP1.x的header带有大量信息，而且每次都要重复发送，HTTP2.0使用encoder来减少需要传输的header大小，通讯双方各自cache一份header fields表，既避免了重复header的传输，又减小了需要传输的大小。



## **HTTP3.0  与 HTTP2.0 区别**

>- HTTP/3 使用 stream 进一步扩展了 HTTP/2 的多路复用。在 HTTP/3 模式下，一般传输多少个文件就会产生对应数量的 stream。当这些文件中的其中一个发生丢包时，你只需要重传丢包文件的对应 stream 即可。
>- HTTP/3 不再是基于 TCP 建立的，而是通过 UDP 建立，在用户空间保证传输的可靠性，相比 TCP，UDP 之上的 QUIC 协议提高了连接建立的速度，降低了延迟。
>- 通过引入 Connection ID，使得 HTTP/3 支持连接迁移以及 NAT 的重绑定。
>- HTTP/3 含有一个包括验证、加密、数据及负载的 built-in 的 TLS 安全机制。
>- 拥塞控制。TCP 是在内核区实现的，而 HTTP/3 将拥塞控制移出了内核，通过用户空间来实现。这样做的好处就是不再需要等待内核更新可以实现很方便的进行快速迭代。
>- 头部压缩。HTTP/2 使用的 HPACK，HTTP/3 更换成了兼容 HPACK 的 QPACK 压缩方案。QPACK 优化了对乱序发送的支持，也优化了压缩率



## **HTTP2.0的多路复用和HTTP1.X中的长连接区别？**

>- HTTP/1.0:         一次请求-响应，建立一个连接，用完关闭；每一个请求都要建立一个连接；
>- HTTP/1.1:         Pipeling管道解决方式为，**若干个请求排队串行化单线程处理**，后面的请求等待前面请求的返回才能获得执行机会，一旦有某请求超时等，后续请求只能被阻塞，毫无办法，也就是人们常说的线头阻塞；
>- HTTP/2            多个请求可同时在一个连接上**并行**执行。某个请求任务耗时严重，不会影响到其它连接的正常执



## 长连接/短连接区别以及应用场景

>**短连接**:
>连接->传输数据->关闭连接。
>比如HTTP是无状态的的短链接，浏览器和服务器每进行一次HTTP操作，就建立一次连接，但任务结束就中断连接：
>具体就是 `浏览器client发起并建立TCP连接 -> client发送HttpRequest报文 -> server接收到报文->server handle并发送HttpResponse报文给前端,发送完毕之后立即调用socket.close方法->client接收response报文->client最终会收到server端断开TCP连接的信号->client 端断开TCP连接，具体就是调用close方法`。也可以这样说：短连接是指SOCKET连接后，发送接收完数据后马上断开连接。因为连接后接收了数据就断开了，所以每次数据接受处理不会有联系。 这也是HTTP协议无状态的原因之一。
>
>**长连接**
>连接->传输数据->保持连接 -> 传输数据-> ...........->直到一方关闭连接，多是客户端关闭连接。长连接指建立SOCKET连接后不管是否使用都保持连接，但安全性较差。
>
>---
>
>**场景：**
>
>**长连接**: 
>
>适用于操作频繁/点对点通讯等连接数不太多的情况，如：一些游戏/即时通讯场景应该使用长连接； 最经典的案例是：数据库使用的连接（如果使用短连接会造成Socket错误）。每个TCP连接都需要三步握手（时间）,每个操作必须先连接->再操作处理速度会降低，而且每个操作用完不断开,当次处理时直接用通道发送数据包，不在建立TCP连接。
>
>**短连接: **
>
>短连接适用于交互频繁的场景，例如电子商务网站，银行APP。适用于Web【wapWeb/H5等】的http服务，长连接对于服务端来说会耗费一定资源。对于电子商务Web的访问量可能是千万级别甚至亿万级别的。如果使用长连接的方式 当一万个用户访问时会占用一万个连接，假设服务器站点（IIS等）的通信吞吐量只有1千个，那么另外九千人就彻底挂啦，所以并发量大且用户不需要频繁的交互式操作时 用短连接为上策
>
>------
>
>**版本开启方式：**
>
>1. http 1.0默认使用的是短连接。当client请求Server会建立一次连接，用完中断连接。client访问的html中有js文件/图像文件/css等，那么client会建立一个http会话。
>
>2. http 1.1以后默认使用的是长连接。主要解决保持连接的特性。如果使用长连接的http协议，则在请求响应头加入代码：
>
>**Connection:keep-alive**：那么client与Server之间用于传输http数据的TCP连接不会关闭 以便于再次访问时会使用这一次建立的连接。但要记住Keep-Aive不是永久的保持连接，这个时间可以在服务器软件中进行设定（Apache）

## HTTP请求方法和HTTP响应状态码

>| 方法                                                         | 说明                                                         |
>| ------------------------------------------------------------ | ------------------------------------------------------------ |
>| GET                                                          | 请求一个指定资源的表示形式. 使用GET的请求应该只被用于获取数据.  URL最大长度是2048个字节 |
>| HEAD                                                         | 请求一个与GET请求的响应相同的响应，但没有响应体.HEAD方法作用：1、在不获取资源的情况下获取资源信息(类型、大小等)。2、通过状态码查看资源是否存在。3、通过查看首部，测试资源是否被修改了。 |
> | POST                                                         | 向指定资源提交数据，请求服务器对数据进行相应处理，如：表单数据提交、文件上传等，请求数据会被包含在请求体中。 |
>| PUT                                                          | 向服务器写入资源，请求服务器创建一个新的目标资源，或者替换原先的目标资源。 |
>| OPTIONS                                                      | 该请求返回服务器对指定资源支持哪些 HTTP 请求方法。           |
>| DELETE                                                       | 用于删除指定的资源。和PUT一样，服务器可能会不支持。          |
> | TRACE                                                        | 回显服务器收到的请求，主要用于测试或诊断。                   |
>| CONNECT                                                      | HTTP/1.1协议中预留给能够将连接改为管道方式的代理服务器。     |
>| [PATCH](https://developer.mozilla.org/zh-CN/docs/Web/HTTP/Methods/PATCH) | 用于对资源进行部分修改。由于PATCH不是标准的HTTP方法，所以不能保证客户端和服务端都已经实现。 |
>
> -----
>
>1. `100-199` 信息，服务器收到请求，需要请求者继续执行操作。
>2. `200-299` 表示请求成功，操作被成功接收并处理。
>3. `300-399` 重定向，需要进一步的操作以完成请求。
>4. `400-499` 表示浏览器方面出错。
>5. `500-599` 表示服务器方面出错。
>
>| 状态代码 | 状态信息                        | 含义                                                         |
>| -------- | ------------------------------- | ------------------------------------------------------------ |
>| 100      | Continue                        | 初始的请求已经接受，客户应当继续发送请求的其余部分。（HTTP 1.1新） |
>| 101      | Switching Protocols             | 服务器将遵从客户的请求转换到另外一种协议（HTTP 1.1新）       |
>| 200      | OK                              | 一切正常，对GET和POST请求的应答文档跟在后面。                |
>| 201      | Created                         | 服务器已经创建了文档，Location头给出了它的URL。              |
>| 202      | Accepted                        | 已经接受请求，但处理尚未完成。                               |
>| 203      | Non-Authoritative Information   | 文档已经正常地返回，但一些应答头可能不正确，因为使用的是文档的拷贝（HTTP 1.1新）。 |
>| 204      | No Content                      | 没有新文档，浏览器应该继续显示原来的文档。如果用户定期地刷新页面，而Servlet可以确定用户文档足够新，这个状态代码是很有用的。 |
>| 205      | Reset Content                   | 没有新的内容，但浏览器应该重置它所显示的内容。用来强制浏览器清除表单输入内容（HTTP 1.1新）。 |
>| 206      | Partial Content                 | 客户发送了一个带有Range头的GET请求，服务器完成了它（HTTP 1.1新）。 |
>| 300      | Multiple Choices                | 客户请求的文档可以在多个位置找到，这些位置已经在返回的文档内列出。如果服务器要提出优先选择，则应该在Location应答头指明。 |
>| 301      | Moved Permanently               | 永久重定向： 客户请求的文档在其他地方，新的URL在Location头中给出，浏览器应该自动地访问新的URL。 |
>| 302      | Found            临时跳转       | 类似于301，但新的URL应该被视为临时性的替代，而不是永久性的。注意，在HTTP1.0中对应的状态信息是“Moved Temporatily”。出现该状态代码时，浏览器能够自动访问新的URL，因此它是一个很有用的状态代码。注意这个状态代码有时候可以和301替换使用。例如，如果浏览器错误地请求http://host/~user（缺少了后面的斜杠），有的服务器 返回301，有的则返回302。严格地说，我们只能假定只有当原来的请求是GET时浏览器才会自动重定向。请参见307。 |
>| 303      | See Other                       | 类似于301/302，不同之处在于，如果原来的请求是POST，Location头指定的重定向目标文档应该通过GET提取（HTTP 1.1新）。 |
>| 304      | Not Modified                    | 客户端有缓冲的文档并发出了一个条件性的请求（一般是提供If-Modified-Since头表示客户只想比指定日期更新的文档）。服务器告 诉客户，原来缓冲的文档还可以继续使用。 |
>| 305      | Use Proxy                       | 客户请求的文档应该通过Location头所指明的代理服务器提取（HTTP 1.1新）。 |
>| 307      | Temporary Redirect              | 和302 （Found）相同。许多浏览器会错误地响应302应答进行重定向，即使原来的请求是POST，即使它实际上只能在POST请求的应答是303时才能重定 向。由于这个原因，HTTP 1.1新增了307，以便更加清除地区分几个状态代码：当出现303应答时，浏览器可以跟随重定向的GET和POST请求；如果是307应答，则浏览器只 能跟随对GET请求的重定向。（HTTP 1.1新） |
>| 400      | Bad Request                     | 请求出现语法错误。                                           |
>| 401      | Unauthorized                    | 客户试图未经授权访问受密码保护的页面。应答中会包含一个WWW-Authenticate头，浏览器据此显示用户名字/密码对话框，然后在填 写合适的Authorization头后再次发出请求。 |
>| 403      | Forbidden                       | 资源不可用。服务器理解客户的请求，但拒绝处理它。通常由于服务器上文件或目录的权限设置导致。 |
>| 404      | Not Found                       | 无法找到指定位置的资源。这也是一个常用的应答。               |
>| 405      | Method Not Allowed              | 请求方法（GET、POST、HEAD、DELETE、PUT、TRACE等）对指定的资源不适用。（HTTP 1.1新） |
>| 406      | Not Acceptable                  | 指定的资源已经找到，但它的MIME类型和客户在Accpet头中所指定的不兼容（HTTP 1.1新）。 |
>| 407      | Proxy Authentication Required   | 类似于401，表示客户必须先经过代理服务器的授权。（HTTP 1.1新） |
>| 408      | Request Timeout                 | 在服务器许可的等待时间内，客户一直没有发出任何请求。客户可以在以后重复同一请求。（HTTP 1.1新） |
>| 409      | Conflict                        | 通常和PUT请求有关。由于请求和资源的当前状态相冲突，因此请求不能成功。（HTTP 1.1新） |
>| 410      | Gone                            | 所请求的文档已经不再可用，而且服务器不知道应该重定向到哪一个地址。它和404的不同在于，返回407表示文档永久地离开了指定的位置，而 404表示由于未知的原因文档不可用。（HTTP 1.1新） |
>| 411      | Length Required                 | 服务器不能处理请求，除非客户发送一个Content-Length头。（HTTP 1.1新） |
>| 412      | Precondition Failed             | 请求头中指定的一些前提条件失败（HTTP 1.1新）。               |
>| 413      | Request Entity Too Large        | 目标文档的大小超过服务器当前愿意处理的大小。如果服务器认为自己能够稍后再处理该请求，则应该提供一个Retry-After头（HTTP 1.1新）。 |
>| 414      | Request URI Too Long            | URI太长（HTTP 1.1新）。                                      |
>| 416      | Requested Range Not Satisfiable | 服务器不能满足客户在请求中指定的Range头。（HTTP 1.1新）      |
>| 500      | Internal Server Error           | 服务器遇到了意料不到的情况，不能完成客户的请求。             |
>| 501      | Not Implemented                 | 服务器不支持实现请求所需要的功能。例如，客户发出了一个服务器不支持的PUT请求。 |
>| 502      | Bad Gateway                     | 服务器作为网关或者代理时，为了完成请求访问下一个服务器，但该服务器返回了非法的应答。 |
>| 503      | Service Unavailable             | 服务器当前不能处理客户端的请求，一段时间后可能恢复正常。：服务器由于维护或者负载过重未能应答。例如，Servlet可能在数据库连接池已满的情况下返回503。服务器返回503时可以提供一个 Retry-After头。 |
>| 504      | Gateway Timeout                 | 由作为代理或网关的服务器使用，表示不能及时地从远程服务器获得应答。（HTTP 1.1新） |
>| 505      | HTTP Version Not Supported      | 服务器不支持请求中所指明的HTTP版本。（HTTP 1.1新）           |
>
>200 OK - 客户端请求成功
>
>- 301 - 资源（网页等）被永久转移到其它URL   永久重定向
>- 302 - 临时跳转                                                    临时重定向 需要两次跳转
>- 400 Bad Request - 客户端请求有语法错误，不能被服务器所理解
>- 401 Unauthorized - 请求未经授权，这个状态代码必须和WWW-Authenticate报头域一起使用
>- 404 - 请求资源不存在，可能是输入了错误的URL
>- 500 - 服务器内部发生了不可预期的错误
>- 503 Server Unavailable - 服务器当前不能处理客户端的请求，一段时间后可能恢复正常。



## HTTP请求头、请求行、请求体

https://blog.csdn.net/u010256388/article/details/68491509

>### request  请求
>
><img src="http://dl.iteye.com/upload/attachment/0069/3451/412b4451-2738-3ebc-b1f6-a0cc13b9697b.jpg" alt="img" style="zoom:60%;" />
>
>**请求行**
>
>1. 请求方法：`get` `post delete put head ` 
>2. 请求URL
>3. HTTP协议及版本号   `1.0 1.1 2.0`
>
>**请求头：**
>
>1. **Accept** ：通过“Accept”报文头属性告诉服务端 客户端接受什么类型的响应 常见返回形式有`text xml json sql image`
>2. **Cookie** :  服务端来识别客户端是隶属于一个Session。
>3. **Cache-Control** ： 客户端要对服务端响应的内容要缓存多长时间。  `no cache` 服务端将对应请求返回的响应内容不要在客户端缓存
>4. **Host**:请求服务器域名 以及 服务器监听的TCP端口号。
>5. **Connection**： 客户端和服务单是否保持连接 `keep-Alive`
>6. **Accept-Encoding**: 接收端接受的内容编码格式 
>7. **user-Agent**: 用户代理信息  浏览器发起请求还是我们自己自定义的  例如使用`python`进行网站爬取
>
>**请求体**
>
>1. 就是 类似于` json  text sql  `   跟 `Aceept`类型在这里差不多呢
>
>------
>
>##### Response  响应
>
><img src="http://dl.iteye.com/upload/attachment/0069/3492/bddb00b6-a3e1-3112-a4f4-4b3cb8687c70.jpg" alt="img" style="zoom:80%;" />
>
>**响应行**
>
>1. 报文协议及版本号 例如`HTTP1.1`
>2. 状态码 以及 状态码描述    状态由五个状态来描述
>
>```xml
>1xx 消息，一般是告诉客户端，请求已经收到了，正在处理，别急...
>2xx 处理成功，一般表示：请求收悉、我明白你要的、请求已受理、已经处理完成等信息.
>3xx 重定向到其它地方。它让客户端再发起一个请求以完成整个处理。
>4xx 处理发生错误，责任在客户端，如客户端的请求一个不存在的资源，客户端未被授权，禁止访问等。
>5xx 处理发生错误，责任在服务端，如服务端抛出异常，路由出错，HTTP版本不支持等。
>```
>
>
>
>**响应头：**
>
>1. **Cache-Control** ：  响应输出到客户端后，服务端通过该报文头属告诉客户端如何控制响应内容的缓存，如果在缓存时间范围内，客户再次访问该资源，直接从客户端的缓存中返回内容给客户，不要再从服务端获取。 （当然，这个功能是靠客户端实现的，服务端只是通过这个属性提示客户端“应该这么做”，做不做，还是决定于客户端，如果是自己宣称支持HTTP的客户端，则就应该这样实现） 。
>2. **ETag** : 一个代表响应服务端资源版本的报文头属性，如果某个服务端资源发生变化了，这个ETag就会相应发生变化。它是Cache-Control的有益补充，可以让客户端更智能地处理什么时候要从服务端取资源，什么时候可以直接从缓存中返回响应。 
>3. **Set-Cookie** ： 服务端可以设置客户端的Cookie，可以设置用户ID、最大生命周期、以及版本号 ,`Set-Cookie: UserID=JohnDoe; Max-Age=3600; Version=1`
>4. **Location**:    需要将页面重新定向至的地址。一般在响应码为3xx的响应中才会有意义。`Location: /index.html`
>5. **Access-Control-Allow-Methods,**
>     **Access-Control-Allow-Headers**    对某些特定的资源 方法和头部的限制
>
>
>
>**响应体**
>
>响应体里边的内容就是发给客户端想要接受类型的内容  例如 `json text sql xml`等一些格式。



## **Keep-Alive模式**？

>HTTP协议采用请求-应答模式，有普通的非KeepAlive模式，也有KeepAlive模式。
>
>非`KeepAlive`模式时，每个请求/应答客户和服务器都要新建一个连接，完成 之后立即断开连接（HTTP协议为无连接的协议）；当使用Keep-Alive模式（又称持久连接、连接重用）时，Keep-Alive功能使客户端到服 务器端的连接持续有效，当出现对服务器的后继请求时，Keep-Alive功能避免了建立或者重新建立连接。



## GET和POST区别 Get长度限制是多少

>1.Get，它用于获取信息，它只是获取、查询数据，也就是说它不会修改服务器上的数据，从这点来讲，它是数据安全的，而稍后会提到的Post它是可以修改数据的，所以这也是两者差别之一了。
>
>2.Post，它是可以向服务器发送修改请求，从而修改服务器的，比方说，我们要在论坛上回贴、在博客上评论，这就要用到Post了，当然它也是可以仅仅获取数据的。
>
>3.Delete 删除数据。可以通过Get/Post来实现。用的不多。
>
>4.Put，增加、放置数据，可以通过Get/Post来实现。用的不多。
>
>-----------------------------------------------------------------------------------------------------------------------
>
>GET是从服务器上获取数据，POST是向服务器传送数据。
>
>1、**GET参数通过url传递，POST放在request body中，GET提交的数据会在地址栏中显示出来，而POST提交，地址栏不会改变。**
>
>2、GET请求在url中传递的参数是有长度限制的，而POST没有。**使用GET方法，则最多限制为2,048个字符**
>
>4、GET请求会被浏览器主动cache，而POST不会，除非手动设置。
>
>5、GET请求只能进行url编码，而POST支持多种编码方式。
>
>6、关于安全性。无论是GET还是POST都不够安全，因为HTTP本身是明文协议。每个HTTP请求和返回的每个byte都会在网络上明文传播，不管是url，header还是body。
>
>为了避免传输中数据被窃取，必须做从客户端到服务器的端端加密。可以使用HTTPS协议  



## ==Session 和 Cookie区别==

>Cookie 主要用于以下三个方面：
>
>- 会话状态管理（如用户登录状态、购物车、游戏分数或其它需要记录的信息）
>- 个性化设置（如用户自定义设置、主题等）
>- 浏览器行为跟踪（如跟踪分析用户行为等）
>
>**什么是 Session**
>
>Session 代表着服务器和客户端一次会话的过程。Session 对象存储特定用户会话所需的属性及配置信息。
>
>## 第二层楼  Cookie 和 Session 有什么不同？
>
>Cookie 和 Session 有什么不同？
>
>客户端需要有一个机制来告诉服务端，本次操作用户是否登录，是哪个用户在执行的操作，那这套机制的实现就需要 Cookie 和 Session 的配合。服务端利用session来识别是哪个客户端呢 在这里哈呢
>
>- 作用范围不同，Cookie 保存在客户端（浏览器），Session 保存在服务器端。
>- 存取方式的不同，Cookie 只能保存 ASCII，Session 可以存任意数据类型，
>- 隐私策略不同，Cookie 存储在客户端，比较容易遭到不法获取，早期有人将用户的登录名和密码存储在 Cookie 中导致信息被窃取；Session 存储在服务端，安全性相对 Cookie 要好一些。
>- **存储大小不同， 单个 Cookie 保存的数据不能超过 4K，Session 可存储数据远高于 Cookie。**
>
>前两层楼内容，绝大部分同学都可以准确回答
>
>## 第三层楼  Cookie 和 Session，他们有什么关联？
>
>客户端需要有一个机制来告诉服务端，本次操作用户是否登录，是哪个用户在执行的操作，那这套机制的实现就需要 Cookie 和 Session 的配合。
>
>Session: 由于HTTP协议是无状态的协议，所以服务端需要记录用户的状态时，就需要用某种机制来识具体的用户，这个机制就是Session.典型的场景比如购物车，当你点击下单按钮时，由于HTTP协议无状态，所以并不知道是哪个用户操作的，所以服务端要为特定的用户创建了特定的Session，用用于标识这个用户，并且跟踪用户，这样才知道购物车里面有几本书。这个Session是保存在服务端的，有一个唯一标识。在服务端保存Session的方法很多，内存、数据库、文件都有。
>
>Cookie:  思考一下服务端如何识别特定的客户？这个时候Cookie就登场了。每次HTTP请求的时候，客户端都会发送相应的Cookie信息到服务端。实际上大多数的应用都是用 Cookie 来实现Session跟踪的，第一次创建Session的时候，服务端会在HTTP协议中告诉客户端，需要在 Cookie 里面记录一个Session ID，以后每次请求把这个会话ID发送到服务器，我就知道你是谁了。有人问，如果客户端的浏览器禁用了 Cookie 怎么办？一般这种情况下，会使用一种叫做URL重写的技术来进行会话跟踪，即每次HTTP交互，URL后面都会被附加上一个诸如 sid=xxxxx 这样的参数，服务端据此来识别用户。
>
>## 第四层楼  禁止了 Cookie
>
>既然服务端是根据 Cookie 中的信息判断用户是否登录，那么如果浏览器中禁止了 Cookie，如何保障整个机制的正常运转。
>
>第一种方案，每次请求中都携带一个 SessionID 的参数，也可以 Post 的方式提交，也可以在请求的地址后面拼接 `xxx?SessionID=123456...`。
>
>第二种方案，Token 机制。Token 机制多用于 App 客户端和服务器交互的模式，也可以用于 Web 端做用户状态管理。
>
>Token 的意思是“令牌”，是服务端生成的一串字符串，作为客户端进行请求的一个标识。Token 机制和 Cookie 和 Session 的使用机制比较类似。
>
>当用户第一次登录后，服务器根据提交的用户信息生成一个 Token，响应时将 Token 返回给客户端，以后客户端只需带上这个 Token 前来请求数据即可，无需再次登录验证。
>
>## 第五层楼  如何考虑分布式 Session 问题？
>
>如何考虑分布式 Session 问题？
>
>在互联网公司为了可以支撑更大的流量，后端往往需要多台服务器共同来支撑前端用户请求，那如果用户在 A 服务器登录了，第二次请求跑到服务 B 就会出现登录失效问题。
>
>分布式 Session 一般会有以下几种解决方案：
>
>- Nginx ip_hash 策略，服务端使用 Nginx 代理，每个请求按访问 IP 的 hash 分配，这样来自同一 IP 固定访问一个后台服务器，避免了在服务器 A 创建 Session，第二次分发到服务器 B 的现象。
>- Session 复制，任何一个服务器上的 Session 发生改变（增删改），该节点会把这个 Session 的所有内容序列化，然后广播给所有其它节点。
>- 共享 Session，服务端无状态话，将用户的 Session 等信息使用缓存中间件来统一管理，保障分发到每一个服务器的响应结果都一致。
>
>建议采用第三种方案。
>
>## 第六层楼
>
>如何解决跨域请求？Jsonp 跨域的原理是什么？
>
>说起跨域请求，必须要了解浏览器的同源策略，同源策略/SOP（Same origin policy）是一种约定，由 Netscape 公司 1995年引入浏览器，它是浏览器最核心也最基本的安全功能，如果缺少了同源策略，浏览器很容易受到 XSS、CSFR 等攻击。所谓同源是指"协议+域名+端口"三者相同，即便两个不同的域名指向同一个 ip 地址，也非同源。
>
>解决跨域请求的常用方法是：
>
>- 通过代理来避免，比如使用 Nginx 在后端转发请求，避免了前端出现跨域的问题。



## 怎么保持HTTP状态

>`Cookie`本质是上客户端的东西，客户端不能自己创建`Cookie`吗？客户端当然可以自己创建`Cookie`！！只不过在用户进行认证的流程中，标识用户身份的`cookie`是服务器下发的
>
>利用Cookie来保持http的状态是现在很常见的解决方案，其中的一个原因是：在浏览器中没有跨域的情况下，浏览器会在http请求中自动携带cookie，非常方便。在非浏览器环境中，可能需要写代码来保证每次都携带对应的cookie。
>
>服务端在接收到`http`请求，解析对应的`cookie`即可得到需要保持的状态标识。说到服务端，不少人提到了`session`会保持http状态，这是不是又不太好了，首先session本质上是一个抽象的概念，其次我们平时所说的用户信息等session是属于服务端的kv数据，不同的客户端可以识别不同的session本质上也是通过cookie机制来实现，我认为那些说session可以保持http状态的说法是不明确的。
>
>-----
>
>在浏览器中，受限于每个浏览器的功能，浏览器发送一个http请求，自动携带的只有规定的那些header和body数据，而多数header只能携带协议规定的那些固定值，这也是浏览器中要想保持http状态方案少的原因之一。body一般用在post的http请求中，所以它的应用场景是有限的。但是header里边存在“Authorization”字段  Authorization”这个header用于用户认证





## HTTP 与 Socket区别

>1. **http 为短连接：**客户端发送请求都需要服务器端回送响应.请求结束后，主动释放链接，因此为短连接.  但是Http1.1支持长连接了
>
>2. Socket 是对 TCP/IP 协议的封装，Socket 只是个接口不是协议，通过 Socket 我们才能使用 TCP/IP 协议，除了 TCP，也可以使用 UDP 协议来传递数据。
>
>3. **Socket为长连接：**通常情况下Socket 连接就是 TCP 连接，因此 Socket 连接一旦建立,通讯双方开始互发数据内容，直到双方断开连接。
>
>-----
>
>很多情况下，都是需要服务器端向客户端主动推送数据，保持客户端与服务端的实时同步。
>
>若双方是 Socket 连接，可以由服务器直接向客户端发送数据。
>
>若双方是 HTTP 连接，则服务器需要等客户端发送请求后，才能将数据回传给客户端。
>
>------
>
>**socket套接字 常用函数进行总结：**
>
>TCP_Server
>
>socket()  ：  我们使用系统调用socket()来获得文件描述符：
>
>bind()   		绑定IP地址和端口
>
>listen()		 监听连接
>
>accept()	   接受连接  accept为每个连接建立新的套接字，并从监听队列中移除这个连接。
>
>![img](https://img-blog.csdn.net/20170723201012737?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbmV2ZXJiZWZhdA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)





# ==HTTPS==

## HTTP与HTTPS的区别

>**TPC/IP协议是传输层协议，主要解决数据如何在网络中传输，而HTTP是应用层协议，主要解决如何包装数据 ** **一般http中存在如下问题：**
>
>-------
>
>- http是超文本传输协议，信息是明文传输。https则是具有安全性的ssl加密传输协议
>- HTTP协议运行在TCP之上，所有传输的内容都是明文，HTTPS运行在SSL/TLS之上，SSL/TLS运行在TCP之上，所有传输的内容都经过加密的。
>- 申请SSL证书需要钱，功能越强大的证书费用越高。
>- http和https使用的是完全不同的连接方式，用的端口也不一样，前者是80，后者是443。
>- SSL涉及到的安全算法会消耗 CPU 资源，对服务器资源消耗较大。



## HTTPS的实现原理 SSL握手

>**HTTPS 在内容传输的加密上使用的是对称加密，非对称加密只作用在证书验证阶段。**
>
>**HTTPS的整体过程分为证书验证和数据传输阶段，具体的交互过程如下：**
>
>![img](https://pic4.zhimg.com/80/v2-1ea0209a526f3527a713736fe7609fcf_720w.jpg)
>
>**① 证书验证阶段：  非对称加密**
>
>- 1）浏览器发起 HTTPS 请求；
>- 2）服务端返回 HTTPS 证书；
>- 3）客户端验证证书是否合法，如果不合法则提示告警。
>
>**② 数据传输阶段：  对称加密**
>
>- 客户端当证书验证合法后，在本地生成随机数；通过证书中的公钥对随机数进行加密传输到服务端；
>- 服务端接收后通过私钥解密得到随机数
>- 客户端和服务器均使用 client random，server random 和 私钥解密 premaster secret，并通过相同的算法生成相同的共享密钥 **KEY**。
>- **客户端就绪：**客户端发送经过共享密钥 **KEY**加密过的"finished"信号。
>- **服务器就绪：**服务器发送经过共享密钥 **KEY**加密过的"finished"信号。
>- **达成安全通信：**握手完成，双方使用对称加密进行安全通信。

## 如何得到认证机构的公钥

>证书颁发机构：认证中心CA: 需要引入权威的第三方机构CA。简单的说，PKI就是浏览器和CA，CA是整个安全机制的重要保障，我们平时用的证书就是由CA机构颁发，其实就是**用CA的私钥给用户的证书签名**，然后在证书的签名字段中填充这个签名值，浏览器在验证这个证书的时候就是使用CA的公钥进行验签。



## 认证机构更新了 或者证书到期

>更新ssl证书，一般就是在SSL证书过期之前进行证书安装，SSL证书并不是永久的，它是有保护期限的，一般是在1-2年左右，具体的和证书的类型，以及证书的品牌是有关系的，大家将新证书重新导入到服务器进行安装，或者是去证书颁发机构续费，那么SSL证书也可以更新好了。



## SSL算法

>1. ***对称加密：\***有流式、分组两种，加密和解密都是使用的同一个密钥。
>
>   例如：DES、AES-GCM、ChaCha20-Poly1305 等。
>
>2. ***非对称加密：\***加密使用的密钥和解密使用的密钥是不相同的，分别称为：公钥、私钥，公钥和算法都是公开的，私钥是保密的。非对称加密算法性能较低，但是安全性超强，由于其加密特性，非对称加密算法能加密的数据长度也是有限的。
>
>   例如：RSA、DSA、ECDSA、 DH、ECDHE 等。
>
>**采用DH算法后，Premaster secret不需要传递，双方只要交换各自的参数，就可以算出这个随机数。**
>
>缺点： 计算量大呗 在这里哈呢



## 对称加密和非对称加密

>**对称加密：**指的就是加、解密使用的同是一串密钥，所以被称做对称加密。对称加密只有一个密钥作为私钥。 
>常见的对称加密算法：DES，AES等。
>
>**非对称加密：**指的是加、解密使用不同的密钥，一把作为公开的公钥，另一把作为私钥。公钥加密的信息，只有私钥才能解密。反之，私钥加密的信息，只有公钥才能解密。 
>
>--------
>
>
>
>**对称加密优缺点：**对称加密相比非对称加密算法来说，加解密的效率要高得多、加密速度快。但是缺陷在于对于密钥的管理和分发上比较困难，不是非常安全，密钥管理负担很重。
>
>**非对称加密优缺点：**安全性更高，公钥是公开的，密钥是自己保存的，不需要将私钥给别人。缺点：加密和解密花费时间长、速度慢，只适合对少量数据进行加密。

## 为什么数据传输是用对称加密？

>**首先：**非对称加密的加解密效率是非常低的，而 http 的应用场景中通常端与端之间存在大量的交互，非对称加密的效率是无法接受的。
>
>**另外：**在 HTTPS 的场景中只有服务端保存了私钥，一对公私钥只能实现单向的加解密，所以HTTPS 中内容传输加密采取的是对称加密，而不是非对称加密。

## 浏览器如何验证证书的合法性？

>浏览器发起 HTTPS 请求时，服务器会返回网站的 SSL 证书，浏览器需要对证书做以下验证：
>
>1. 验证域名、有效期等信息是否正确：证书上都有包含这些信息，比较容易完成验证；
>2. 判断证书来源是否合法：每份签发证书都可以根据验证链查找到对应的根证书，操作系统、浏览器会在本地存储权威机构的根证书，利用本地根证书可以对对应机构签发证书完成来源验证
>3. 判断证书是否被篡改：需要与 CA 服务器进行校验；
>4. 判断证书是否已吊销：通过CRL（Certificate Revocation List 证书注销列表）和 OCSP（Online Certificate Status Protocol 在线证书状态协议）实现，其中 OCSP 可用于第3步中以减少与 CA 服务器的交互，提高验证效率。
>
>以上任意一步都满足的情况下浏览器才认为证书是合法的。

## 本地随机数被窃取怎么办？

>证书验证是采用非对称加密实现，但是传输过程是采用对称加密，而其中对称加密算法中重要的随机数是由本地生成并且存储于本地的，HTTPS 如何保证随机数不会被窃取？
>
>其实 HTTPS 并不包含对随机数的安全保证，HTTPS 保证的只是传输过程安全，而随机数存储于本地，本地的安全属于另一安全范畴，应对的措施有安装杀毒软件、反木马、浏览器升级修复漏洞等。



# ==DNS==

## DNS的作用和原理

>`DNS（Domain Name System，`域名系统），是因特网上作为域名和`IP`地址相互映射的一个分布式数据库，能够使用户更方便的访问互联网，而不用去记住能够被机器直接读取的`IP`数串。通过主机名，最终得到该主机名对应的`IP`地址的过程叫做域名解析（或主机名解析）。
>
>
>
>##### DNS负载均衡
>
>当一个网站有足够多的用户的时候，假如每次请求的资源都位于同一台机器上面，那么这台机器随时可能会蹦掉。处理办法就是用DNS负载均衡技术，它的原理是在DNS服务器中为同一个主机名配置多个IP地址,在应答DNS查询时,DNS服务器对每个查询将以DNS文件中主机记录的IP地址按顺序返回不同的解析结果,将客户端的访问引导到不同的机器上去,使得不同的客户端访问不同的服务器,从而达到负载均衡的目的｡例如可以根据每台机器的负载量，该机器离用户地理位置的距离等等。
>
>
>
>##### **DNS查询的两种方式：递归查询和迭代查**
>
>**1、递归解析**
>
>当局部DNS服务器自己不能回答客户机的DNS查询时，它就需要向其他DNS服务器进行查询。此时有两种方式。局部DNS服务器自己负责向其他DNS服务器进行查询，一般是先向该域名的根域服务器查询，再由根域名服务器一级级向下查询。最后得到的查询结果返回给局部DNS服务器，再由局部DNS服务器返回给客户端。
>
>![img](https://pic3.zhimg.com/80/v2-4415eab38ab3774f85663197b3559942_720w.jpg)
>
>**2、迭代解析**
>
>当局部DNS服务器自己不能回答客户机的DNS查询时，也可以通过迭代查询的方式进行解析，如图所示。局部DNS服务器不是自己向其他DNS服务器进行查询，而是把能解析该域名的其他DNS服务器的IP地址返回给客户端DNS程序，客户端DNS程序再继续向这些DNS服务器进行查询，直到得到查询结果为止。也就是说，迭代解析只是帮你找到相关的服务器而已，而不会帮你去查。比如说：[http://baidu.com](https://link.zhihu.com/?target=http%3A//baidu.com)的服务器ip地址在192.168.4.5这里，你自己去查吧，本人比较忙，只能帮你到这里了。
>
>![img](https://pic2.zhimg.com/80/v2-1d2ec667390081157834a69f5a17445d_720w.jpg)



## DNS为什么用UDP

>### DNS在进行区域传输的时候使用TCP协议，其它时候则使用UDP协议；
>
>DNS的规范规定了2种类型的DNS服务器，一个叫主DNS服务器，一个叫辅助DNS服务器。在一个区中主DNS服务器从自己本机的数据文件中读取该区的DNS数据信息，而辅助DNS服务器则从区的主DNS服务器中读取该区的DNS数据信息。当一个辅助DNS服务器启动时，它需要与主DNS服务器通信，并加载数据信息，这就叫做区传送（zone transfer）。
>
>区域传输是DNS的事务，对准确性要求比较高，而且会产生大于512字节的数据包，因此使用TCP协议。
>
>### 为什么既使用TCP又使用UDP？
>
>首先了解一下TCP与UDP传送字节的长度限制。
>
>*UDP报文的最大长度为512字节，而TCP则允许报文长度超过512字节*。当DNS查询超过512字节时，协议的TC标志出现删除标志，这时则使用TCP发送。通常传统的UDP报文一般不会大于512字节。
>
>### 区域传送时使用TCP，主要有一下两点考虑
>
>1. 辅域名服务器会定时（一般时3小时）向主域名服务器进行查询以便了解数据是否有变动。如有变动，则会执行一次区域传送，进行数据同步。区域传送将使用TCP而不是UDP，因为数据同步传送的数据量比一个请求和应答的数据量要多得多。
>2. TCP是一种可靠的连接，保证了数据的准确性。
>
>### 域名解析时使用UDP协议
>
>客户端向DNS服务器查询域名，一般返回的内容都不超过512字节，用UDP传输即可。不用经过TCP三次握手，这样DNS服务器负载更低，响应更快。虽然从理论上说，客户端也可以指定向DNS服务器查询的时候使用TCP，但事实上，很多DNS服务器进行配置的时候，仅支持UDP查询包。
>
>## 总结
>
>大多数情况下，DNS解析请求和响应都很小，使用UDP协议更加高效，虽然没有TCP可靠，但是速度快，消耗的系统资源更少，非常适合少量数据包的传输。
>一些DNS事务，比如区域传输或其他附加查询，可能会产生大于512字节的数据包，因此使用TCP更加可靠，使用TCP会减少丢包和重新发包的情况，因此更加可靠与高效。



# ==---------------------------------------------------------------传输层==



# ==TCP 三次握手==

>https://zhuanlan.zhihu.com/p/86426969   知乎特别好的博文

>`URG`：此标志表示`TCP`包的紧急指针域有效，用来保证`TCP`连接不被中断，并且督促中间层设备要尽快处理这些数据；
>
>`ACK `：此标志表示应答域有效, 只有`ACK=1`代表确认号字段有效，也规定连接建立后所有发送的报文的`ACK`必须为`1`,  确认号（确认号为`501`，即表明序号1-`500`的字节已成功收到，接下来期望收到从A发来的序号为`501`的字节）
>
>`PSH`：这个标志位表示`Push`操作。所谓`Push`操作就是指在数据包到达接收端以后，立即传送给应用程序，而不是在缓冲区中排队；
>
>`RST`：用来异常的关闭连接，A向B发起连接，但B之上并未监听相应的端口，这时B操作系统上的TCP处理程序会发RST包。
>
>`SYN(SYNchronization)` ： 在连接建立时用来同步序号。当`SYN=1`而`ACK=0`时，表明这是一个连接请求报文。对方若同意建立连接，则应在响应报文中,`SYN=1`和`ACK=1`. 因此, `SYN`置`1`就表示这是一个连接请求或连接接受报文。
>
>`FIN(finis)`即完，终结的意思， 用来释放一个连接。当 `FIN = 1` 时，表明此报文段的发送方的数据已经发送完毕，并要求释放连接。
>
>`seq`：同步序列编号`（Synchronize Sequence Numbers）。`
>
>---------
>
>![preview](https://pic3.zhimg.com/v2-2a54823bd63e16674874aa46a67c6c72_r.jpg)
>
>三次握手其实就是指建立一个`TCP`连接时，需要客户端和服务器总共发送`3`个包。进行三次握手的主要作用就是为了确认双方的接收能力和发送能力是否正常、指定自己的初始化序列号为后面的可靠性传送做准备。实质上其实就是连接服务器指定端口，建立`TCP`连接，并同步连接双方的序列号和确认号，交换`TCP`窗口大小信息。
>
>1. 第一次握手：建立连接时，客户端发送`SYN`（连接请求报文段到服务器) 并指明客户端的初始化序列号 `ISN`，并进入`SYN_SENT`状态，等待服务器确认。无应用层数据。首部的同步位`SYN=1`，初始序号`seq=x`，`SYN=1`的报文段不能携带数据，否则会造成SYN攻击。
>
>2. 第二次握手：服务器收到`syn`报文之后，会以自己的` SYN `报文置1作为应答并为该`TPC`连接分配缓存和变量，且也是指定了自己的初始化序列号 `ISN(s)`,同时会把客户端的` seq + 1 `作为`ACK `的值，表示自己已经收到了客户端的 `SYN`，允许连接。在确认报文段中`SYN=1，ACK=1，`确认号`ack=x+1，`初始序号`seq=y。`进入`SYN_RECV`状态
>
>3. 第三次握手：客户端收到服务器的`SYN+ACK`包，会发送一个 `ACK `报文，当然，也是一样把服务器的` seq+ 1 `作为 `ACK` 的值，表示已经收到了服务端的 `SYN `报文，此时客户端处于 `ESTABLISHED `状态。服务器收到` ACK `报文之后，也处于` ESTABLISHED `状态，此时，双方已建立起了连接。确认报文段`ACK=1`，确认号`ack=y+1`，序号`seq=x+1`（初始为`seq=x`，第二个报文段所以要`+1`），`ACK`报文段可以携带数据，不携带数据则不消耗序号。

## **为什么需要三次握手，两次不行吗？**

>第一次握手：客户端发送网络包，服务端收到了。这样服务端就能得出结论：客户端的发送能力、服务端的接收能力是正常的。
>
>第二次握手：服务端发包，客户端收到了。这样客户端就能得出结论：服务端的接收、发送能力，客户端的接收、发送能力是正常的。不过此时服务器并不能确认客户端的接收能力是否正常。
>
>第三次握手：客户端发包，服务端收到了。这样服务端就能得出结论：客户端的接收、发送能力正常，服务器自己的发送、接收能力也正常。
>
>因此，需要三次握手才能确认双方的接收与发送能力是否正常。
>
>试想如果是用两次握手，则会出现下面这种情况：
>
>-----
>
>
>
>**为了防止已失效的连接请求报文段突然又传送到了服务端，因而产生错误**
>
>如客户端发出连接请求，但因连接请求报文丢失而未收到确认，于是客户端再重传一次连接请求。后来收到了确认，建立了连接。数据传输完毕后，就释放了连接，客户端共发出了两个连接请求报文段，其中第一个丢失，第二个到达了服务端，但是第一个丢失的报文段只是在某些网络结点长时间滞留了，延误到连接释放以后的某个时间才到达服务端，此时服务端误认为客户端又发出一次新的连接请求，于是就向客户端发出确认报文段，同意建立连接，不采用三次握手，只要服务端发出确认，就建立新的连接了，此时客户端忽略服务端发来的确认，也不发送数据，则服务端一致等待客户端发送数据，浪费资源。



## 第二次握手传回ACK 为什么还要传回SYN

>`TCP`是可靠连接，双方都要确保发送的信息是可靠的、准确无误的。
>
>`SYN `是 `TCP/IP` 建立连接时使用的握手信号。在客户机和服务器之间建立正常的` TCP` 网络连接时，客户机首先发出一个 `SYN `消息，服务器使用 `SYN-ACK` 应答表示接收到了这个消息，回传了`SYN`只是证明服务器收到的确实是客户端发送的信号(连接请求报文段)，但是服务器到客户端之间的通道还需要`ACK`信号来保证信息的准确无误。最后客户机再以`ACK`消息响应。这样在客户机和服务器之间才能建立起可靠的TCP连接，数据才可以在客户机和服务器之间传递。



## 第三次握手ACK丢失怎么办

>当`Client`端收到`Server`的SYN+ACK应答后，其状态变为`ESTABLISHED可建立连接`，并发送ACK包给Server；
>
>1. 如果此时ACK在网络中丢失，那么Server端该TCP连接的状态为SYN_RECV，并且依次等待3秒、6秒、12秒后重新发送SYN+ACK包，以便Client重新发送ACK包。 
>2. Server重发SYN+ACK包的次数，可以设置，默认值为5。  如果重发指定次数后，仍然未收到ACK应答，那么一段时间后，Server自动关闭这个连接。
>3. 但是Client认为这个连接已经建立，如果Client端向Server写数据，Server端将以RST包(用于强制关闭tcp连接)响应，方能感知到Server的错误。



## **三次握手过程中可以携带数据吗？**

>其实第三次握手的时候，是可以携带数据的。但是，第一次、第二次握手不可以携带数据
>
>假如第一次握手可以携带数据的话，如果有人要恶意攻击服务器，那他每次都在第一次握手中的 `SYN `报文中放入大量的数据。因为攻击者根本就不理服务器的接收、发送能力是否正常，然后疯狂着重复发 `SYN `报文的话，这会让服务器花费很多时间、内存空间来接收这些报文。
>
>也就是说，第一次握手不可以放数据，其中一个简单的原因就是会让服务器更加容易受到攻击了。而对于第三次的话，此时客户端已经处于 `ESTABLISHED `状态。对于客户端来说，他已经建立起连接了，并且也已经知道服务器的接收、发送能力是正常的了，所以能携带数据也没啥毛病。



## **SYN攻击是什么？**

>#####  YN攻击是什么？**
>
>服务器端的资源分配是在二次握手时分配的，而客户端的资源是在完成三次握手时分配的，所以服务器容易受到`SYN`洪泛攻击。`SYN`攻击就是`Client`在短时间内伪造大量不存在的`IP`地址，并向`Server`不断地发送`SYN`包，`Server`则回复确认包，并等待`Client`确认，由于源地址不存在，因此`Server`需要不断重发直至超时，这些伪造的`SYN`包将长时间占用未连接队列，导致正常的`SYN`请求因为队列满而被丢弃，从而引起网络拥塞甚至系统瘫痪。`SYN `攻击是一种典型的 `DoS/DDoS `攻击。
>
>检测 `SYN `攻击非常的方便，当你在服务器上看到大量的半连接状态时，特别是源IP地址是随机的，基本上可以断定这是一次`SYN`攻击。
>
>常见的防御` SYN `攻击的方法有如下几种：
>
>- 缩短超时（`SYN Timeout`）时间
>- 增加最大半连接数
>- 过滤网关防护
>- `SYN cookies`技术

## **什么是半连接队列？**

>服务器第一次收到客户端的 `SYN` 之后，就会处于 `SYN_RCVD `状态，此时双方还没有完全建立其连接，服务器会把此种状态下请求连接放在一个队列里，我们把这种队列称之为半连接队列。
>
>当然还有一个全连接队列，就是已经完成三次握手，建立起连接的就会放在全连接队列中。如果队列满了就有可能会出现丢包现象。
>
>`SYN-ACK` 重传次数的问题：
>
>服务器发送完`SYN-ACK`包，如果未收到客户确认包，服务器进行首次重传，等待一段时间仍未收到客户确认包，进行第二次重传。如果重传次数超过系统规定的最大重传次数，系统将该连接信息从半连接队列中删除。
>
>每次重传等待的时间不一定相同，一般会是指数增长，例如间隔时间为` 1s，2s，4s，8s…`



# ==TCP 四次挥手== TCP是如何断开连接的



>建立一个连接需要三次握手，而终止一个连接要经过四次挥手。这由`TCP`的半关闭`（half-close`）造成的。所谓的半关闭，其实就是TCP提供了连接的一端在结束它的发送后还能接收来自另一端数据的能力。
>
>`TCP `的连接的拆除需要发送四个包，因此称为四次挥手`(Four-way handshake)`，客户端或服务器均可主动发起挥手动作。
>
>刚开始双方都处于` ESTABLISHED `状态，假如是客户端先发起关闭请求。四次挥手的过程如下：
>
>![preview](https://pic2.zhimg.com/v2-c7d4b5aca66560365593f57385ce9fa9_r.jpg)
>
>1. 第一次挥手：客户端发送一个` FIN` 报文，报文中会指定一个序列号。此时客户端处于` FIN_WAIT1` 状态。发出连接释放报文段（`FIN=1，序号seq=u`），并停止再发送数据，主动关闭`TCP`连接，进入`FIN_WAIT1`（终止等待1）状态，等待服务端的确认。
>
>2. 第二次挥手：服务端收到 `FIN `之后，会发送 `ACK `报文，且把客户端的序列号值 `+1 `作为 `ACK` 报文的序列号值，表明已经收到客户端的报文了，此时服务端处于 `CLOSE_WAIT` 状态。即服务端收到连接释放报文段后即发出确认报文段（`ACK=1`，确认号`ack=u+1`，序号`seq=v`），服务端进入`CLOSE_WAIT`（关闭等待）状态，此时的`TCP`处于半关闭状态，客户端到服务端的连接释放。客户端收到服务端的确认后，进入`FIN_WAIT2`（终止等待2）状态，等待服务端发出的连接释放报文段。
>
>3. 第三次挥手：服务端也想断开连接了，和客户端的第一次挥手一样，发给` FIN` 报文，且指定一个序列号。此时服务端处于 `LAST_ACK `的状态。即服务端没有要向客户端发出的数据，服务端发出连接释放报文段（`FIN=1，ACK=1，序号seq=w，确认号ack=u+1`），服务端进入`LAST_ACK`（最后确认）状态，等待客户端的确认。
>
>4. 第四次挥手：客户端收到 `FIN `之后，一样发送一个 `ACK `报文作为应答，且把服务端的序列号值 `+1 `作为自己 `ACK `报文的序列号值，此时客户端处于 `TIME_WAIT` 状态。需要过一阵子以确保服务端收到自己的 `ACK `报文之后才会进入` CLOSED `状态，服务端收到` ACK `报文之后，就处于关闭连接了，处于 `CLOSED `状态。即客户端收到服务端的连接释放报文段后，对此发出确认报文段（`ACK=1，seq=u+1，ack=w+1`），客户端进入`TIME_WAIT`（时间等待）状态。此时`TCP`未释放掉，需要经过时间等待计时器设置的时间`2MSL`后，客户端才进入`CLOSED`状态。



## **挥手为什么需要四次？**

>tcp是全双工通信，服务端和客服端都能发送和接收数据。
>
>tcp在断开连接时，需要服务端和客服端都确定对方将不再发送数据。
>
>第1次挥手: 	由客户端向服务端发起，服务端收到信息后就能确定客户端已经停止发送数据。
>
>第2次挥手:	 由服务端向客户端发起，客户端收到消息后就能确定服务端已经知道客户端不会再发送数据。
>
>第3次握手:	 由服务端向客户端发起，客户端收到消息后就能确定服务端已经停止发送数据。
>
>第4次挥手: 	由客户端向服务端发起，服务端收到信息后就能确定客户端已经知道服务端不会再发送数据。
>
>为什么不是3次挥手:
>
>在客服端第1次挥手时，服务端可能还在发送数据。 所以第2次挥手和第3次挥手不能合并。



## **TIME_WAIT等待2MSL的意义?**

>`MSL`是`Maximum Segment Lifetime`的英文缩写，可译为最长报文段寿命，它是任何报文在网络上存在的最长时间，超过这个时间报文将被丢弃。
>
>1. 为了保证客户端发送的最后一个`ACK`报文段能够到达服务器。因为这个`ACK`有可能丢失，从而导致处在`LAST-ACK`状态的服务器收不到对`FIN-ACK`的确认报文。服务器会超时重传这个`FIN-ACK`，接着客户端再重传一次确认，重新启动      时间等待计时器。最后客户端和服务器都能正常的关闭。假设客户端不等待`2MSL`，而是在发送完`ACK`之后直接释放关闭，一但这个`ACK`丢失的话，服务器就无法正常的进入关闭连接状态，浪费资源。
>
>2. 其次：客户端在发送完最后一个`ACK`报文段后，再经过`2MSL`，就可以使本连接持续的时间内所产生的所有报文段都从网络中消失，使下一个新的连接中不会出现这种旧的连接请求报文段。
>
>**为什么`TIME_WAIT`状态需要经过`2MSL`才能返回到`CLOSE`状态？**
>
>四个报文都发送完毕，就可以直接进入`CLOSE`状态了，但是可能网络是不可靠的，有可能最后一个`ACK`丢失。所以`TIME_WAIT`状态就是用来重发可能丢失的`ACK`报文。



## TIME_WAIT和CLOSE_WAIT状态区别

>**TIME_WAIT** 
>
>TIME_WAIT 是主动关闭链接时形成的，等待2MSL时间，约4分钟。主要是防止最后一个ACK丢失。  由于TIME_WAIT 的时间会非常长，因此server端应尽量减少主动关闭连接
>
>**CLOSE_WAIT**
>CLOSE_WAIT是被动关闭连接是形成的。根据TCP状态机，服务器端收到客户端发送的FIN，则按照TCP实现发送ACK，因此进入CLOSE_WAIT状态。但如果服务器端不执行close()，就不能由CLOSE_WAIT迁移到LAST_ACK，则系统中会存在很多CLOSE_WAIT状态的连接。此时，可能是系统忙于处理读、写操作，而未将已收到FIN的连接，进行close。此时，recv/read已收到FIN的连接socket，会返回0。
>
>**为什么需要 TIME_WAIT 状态？**
>假设最终的ACK丢失，server将重发FIN，client必须维护TCP状态信息以便可以重发最终的ACK，否则会发送RST，结果server认为发生错误。TCP实现必须可靠地终止连接的两个方向(全双工关闭)，client必须进入 TIME_WAIT 状态，因为client可能面 临重发最终ACK的情形。
>
>**为什么 TIME_WAIT 状态需要保持 2MSL 这么长的时间？**
>如果 TIME_WAIT 状态保持时间不足够长(比如小于2MSL)，第一个连接就正常终止了。第二个拥有相同相关五元组的连接出现，而第一个连接的重复报文到达，干扰了第二个连接。TCP实现必须防止某个连接的重复报文在连接终止后出现，所以让TIME_WAIT状态保持时间足够长(2MSL)，连接相应方向上的TCP报文要么完全响应完毕，要么被 丢弃。建立第二个连接的时候，不会混淆。
>
> **TIME_WAIT 和\**CLOSE_WAIT\**状态socket过多**
>
>如果服务器出了异常，百分之八九十都是下面两种情况：
>
>1.服务器保持了大量TIME_WAIT状态
>
>2.服务器保持了大量CLOSE_WAIT状态，简单来说CLOSE_WAIT数目过大是由于被动关闭连接处理不当导致的。
>
>





## 过多TIME_WAIT状态会导致什么问题 高并发场景

>```
>TIME_WAIT状态存在的理由：
>1）可靠地实现TCP全双工连接的终止
>   在进行关闭连接四次挥手协议时，最后的ACK是由主动关闭端发出的，如果这个最终的ACK丢失，服务器将重发最终的FIN，
>因此客户端必须维护状态信息允许它重发最终的ACK。如果不维持这个状态信息，那么客户端将响应RST分节，服务器将此分节解释成一个错误（在java中会抛出connection reset的SocketException)。
>因而，要实现TCP全双工连接的正常终止，必须处理终止序列四个分节中任何一个分节的丢失情况，主动关闭的客户端必须维持状态信息进入TIME_WAIT状态。
>```
>
>1.  **高并发可以让服务器在短时间范围内同时占用大量端口**，而端口有个0~65535的范围，并不是很多，刨除系统和其他服务要用的，剩下的就更少了。
>
>2. 在这个场景中，**短连接表示“业务处理+传输数据的时间 远远小于 TIMEWAIT超时的时间”的连接**。
>
>   这里有个相对长短的概念，比如取一个web页面，1秒钟的http短连接处理完业务，在关闭连接之后，这个业务用过的端口会停留在TIMEWAIT状态几分钟，而这几分钟，其他HTTP请求来临的时候是无法占用此端口的(占着茅坑不拉翔)。 服务器资源严重浪费。（说个题外话，从这个意义出发来考虑服务器性能调优的话，长连接业务的服务就不需要考虑TIMEWAIT状态。同时，假如你对服务器业务场景非常熟悉，你会发现，在实际业务场景中，一般**长连接对应的业务的并发量并不会很高**。
>
>3.  综合这两个方面，持续的到达一定量的高并发短连接，会使服务器因端口资源不足而拒绝为一部分客户服务。
>
>-------
>
>```
>存在即是合理的，既然TCP协议能盛行四十多年，就证明他的设计合理性。所以我们尽可能的使用其原本功能。
>依靠TIME_WAIT状态来保证我的服务器程序健壮，服务功能正常。
>那是不是就不要性能了呢？并不是。如果服务器上跑的短连接业务量到了我真的必须处理这个TIMEWAIT状态过多的问题的时候
>```
>
>解决方案：我的原则是尽量处理，而不是跟`TIMEWAIT`干上，非先除之而后快。如果尽量处理了，还是解决不了问题，仍然拒绝服务部分请求，那我会采取负载均衡来抗这些高并发的短请求。持续十万并发的短连接请求，两台机器，每台5万个，应该够用了吧。一般的业务量以及国内大部分网站其实并不需要关注这个问题，一句话，达不到时才需要关注这个问题的访问量。





# ==TCP==



## TCP报文格式      或者是首部

>TCP报文是TCP层传输的数据单元，也叫报文段。
>
>![图片加载中](https://img-blog.csdn.net/20170227111849763?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvTWFyeTE5OTIwNDEw/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)
>
>**1、端口号：用来标识同一台计算机的不同的应用进程。**
>
>1）源端口：源端口和IP地址的作用是标识报文的返回地址。
>
>2）目的端口：端口指明接收方计算机上的应用程序接口。
>
>TCP报头中的源端口号和目的端口号同IP数据报中的源IP与目的IP唯一确定一条TCP连接。
>
>**2、序号和确认号：**
>
>是TCP可靠传输的关键部分。序号是本报文段发送的数据组的第一个字节的序号。在TCP传送的流中，每一个字节一个序号。e.g.一个报文段的序号为300，此报文段数据部分共有100字节，则下一个报文段的序号为400。所以序号确保了TCP传输的有序性。确认号，即ACK，指明下一个期待收到的字节序号，表明该序号之前的所有数据已经正确无误的收到。确认号只有当ACK标志为1时才有效。比如建立连接时，SYN报文的ACK标志位为0。
>
>----
>
>3. **控制位：URG  ACK  PSH  RST  SYN  FIN，共6个，每一个标志位表示一个控制功能。**
>
>1. URG：紧急指针标志，为1时表示紧急指针有效，为0则忽略紧急指针。
>2. ACK：确认序号标志，为1时表示确认号有效，为0表示报文中不含确认信息，忽略确认号字段。
>3. PUSH：push标志，    为1表示是带有push标志的数据，指示接收方在接收到该报文段以后，应尽快将这个报文段交给应用程序，而不是在缓冲区排队。
>4. RST：重置连接标志，用于重置由于主机崩溃或其他原因而出现错误的连接。或者用于拒绝非法的报文段和拒绝连接请求。
>5. SYN：同步序号，用于建立连接过程，在连接请求中，SYN=1和ACK=0表示该数据段没有使用捎带的确认域，而连接应答捎带一个确认，即SYN=1和ACK=1。
>6. FIN：finish标志，用于释放连接，为1时表示发送方已经没有数据发送了，即关闭本方数据流。
>
>-----
>
>4. **窗口：**
>
>   滑动窗口大小，用来告知发送端接受端的缓存大小，以此控制发送端发送数据的速率，从而达到流量控制。窗口大小时一个16bit字段，因而窗口大小最大为65535。

## TCP流量控制(端到端)

>**流量控制产生的原因：**如果发送者发送数据过快，接收者来不及接收，那么就会有分组丢失。为了避免分组丢失，控制发送者的发送速度，使得接收者来得及接收，这就是流量控制。流量控制根本目的是防止分组丢失，它是构成TCP可靠性的一方面。
>
>-----
>
>**解决办法：**
>
>流量控制，是利用灵活可变的滑动窗口控制流量。
>
>**具体是消息接收方会发送流量控制报文，通知发送方窗口大小，发送方发送的数据大小不能超过窗口大小。**



## **TCP流量控制产生的死锁？**

>**什么情况会造成流量控制的死锁？**
>
>当发送者收到了一个窗口为0的应答，发送者便停止发送，等待接收者的下一个应答。但是如果这个窗口不为0的应答在传输过程丢失，发送者一直等待下去，而接收者以为发送者已经收到该应答，等待接收新数据，这样双方就相互等待，从而产生死锁。
>
>----
>
>**如何避免？**
>
>为了避免流量控制引发的死锁，TCP使用了持续计时器。每当发送者收到一个0窗口的应答后就启动该计时器。时间一到便主动发送报文询问接收者的窗口大小。若接收者仍然返回0窗口，则重置该计时器继续等待，若窗口不为0，则表示应答报文丢失了，此时重置发送窗口后开始发送，这样避免了死锁的产生。

## TCP拥塞控制(区域网络)

>拥塞控制：拥塞控制是作用于网络的，它是防止过多的数据注入到网络中，避免出现网络负载过大的情况；常用的方法就是：
>
>（ 1 ）慢开始、拥塞避免两者结合使用（ 2 ）快重传、快恢复。
>
>------
>
>**慢开始：**发送方维持一个叫做拥塞窗口cwnd（congestion window）的状态变量。拥塞窗口的大小取决于网络的拥塞程度，并且动态地在变化。发送方让自己的发送窗口等于拥塞窗口，另外考虑到接受方的接收能力，发送窗口可能小于拥塞窗口。
>
>1. 慢开始算法的思路就是，不要一开始就发送大量的数据，先探测一下网络的拥塞程度，也就是说由小到大逐渐增加拥塞窗口的大小。
>
>2. 刚开始建立连接的时候，发送放拥塞窗口大小为1，然后逐步增加窗口的大小，如每次加倍。1.2.4.8
>
>-------
>
>**拥塞避免算法：**
>
>当慢开始算法达到门限值得时候，开始使用拥塞避免算法，这个时候拥塞窗口缓慢增长+1. 当拥塞窗口达到一定值的时候网络出现超时，会重新更新门限值是拥塞窗口的一般。之后拥塞窗口置为1，（这样做的目的就是要迅速减少主机发送到网络中的分组数，使得发生拥塞的路由器有足够时间把队列中积压的分组处理完毕。）再次慢开始算法进行试探。达到门限值再次进行拥塞避免算法。
>
>![preview](https://pic3.zhimg.com/v2-f7db63b1f00cbd8170e1435616e06216_r.jpg)
>
>**（1）拥塞窗口cwnd初始化为1个报文段，慢开始门限初始值为16**
>**（2）执行慢开始算法，指数规律增长到第4轮，即cwnd=16=ssthresh，改为执行拥塞避免算法，拥塞窗口按线性规律增长**
>**（3）假定cwnd=24时，网络出现超时（拥塞），则更新后的ssthresh=12，cwnd重新设置为1，并执行慢开始算法。当cwnd=12=ssthresh时，改为执行拥塞避免算法。**

--------

>避免拥塞控制 还有快重传、快回复 两种算法：
>
>**快重传：**
>
>刚开始阶段跟慢开始和拥塞避免算法是一样的。 当网络出现超时的时候，快重传要求接收方在收到一个失序的报文段后就立即发出重复确认，发送方只要一连收到三个重复确认就应当立即重传对方尚未收到的报文段，而不必继续等待设置的重传计时器时间到期。
>
>**快恢复算法：**
>
>当发送方连续收到三个重复确认时，就执行“乘法减小”算法，把ssthresh门限减半（为了预防网络发生拥塞）。但是接下来不是把拥塞窗口置1，不执行慢开始算法，而是跟门限值一样，而是执行拥塞避免算法。
>
>为什么不使用慢开始算法：考虑到如果网络出现拥塞的话就不会收到好几个重复的确认，所以发送方现在认为网络可能没有出现拥塞。所以此时不执行慢开始算法，而是将cwnd设置为ssthresh减半后的值，然后执行拥塞避免算法，使cwnd缓慢增大。 TCP Reno版本是目前使用最广泛的版本。
>
><img src="https://pic4.zhimg.com/v2-5f4034bc11c3a48a1d1a115f9ee0259b_r.jpg" alt="preview" style="zoom:67%;" />
>
>在采用快恢复算法时，慢开始算法只是在TCP连接建立时和网络出现超时时才使用







## TCP如何保证可靠传输

>　TCP提供一种面向连接的、可靠的字节流服务。
>
>　　对于可靠性，TCP通过以下方式进行保证：
>
>- **数据包校验**：目的是检测数据在传输过程中的任何变化，若校验出包有错，则丢弃报文段并且不给出响应，这时TCP发送数据端超时后会重发数据；
>- **对失序数据包重排序**：既然TCP报文段作为IP数据报来传输，而IP数据报的到达可能会失序，因此TCP报文段的到达也可能会失序。TCP将对失序数据进行重新排序，然后才交给应用层；
>- **丢弃重复数据**：对于重复数据，能够丢弃重复数据；
>- **应答机制**：当TCP收到发自TCP连接另一端的数据，它将发送一个确认。这个确认不是立即发送，通常将推迟几分之一秒；
>- **超时重发**：当TCP发出一个段后，它启动一个定时器，等待目的端确认收到这个报文段。如果不能及时收到一个确认，将重发这个报文段；
>- **流量控制**：TCP连接的每一方都有固定大小的缓冲空间。TCP的接收端只允许另一端发送接收端缓冲区所能接纳的数据，这可以防止较快主机致使较慢主机的缓冲区溢出，这就是流量控制。TCP使用的流量控制协议是可变大小的滑动窗口协议



## TCP粘包

>1. **什么是TCP粘包问题？**
>    TCP粘包就是指发送方发送的若干包数据到达接收方时粘成了一包，从接收缓冲区来看，后一包数据的头紧接着前一包数据的尾，出现粘包的原因是多方面的，可能是来自发送方，也可能是来自接收方。
>
>----
>
>2. **造成TCP粘包的原因**
>
> (1)发送方原因:
>
>TCP默认使用Nagle算法（主要作用：减少网络中报文段的数量），而Nagle算法主要做两件事：只有上一个分组得到确认，才会发送下一个分组
>收集多个小分组，在一个确认到来时一起发送. Nagle算法造成了发送方可能会出现粘包问题
>
>（2）接收方原因
>
>TCP接收到数据包时，并不会马上交到应用层进行处理，或者说应用层并不会立即处理。实际上，TCP将接收到的数据包保存在接收缓存里，然后应用程序主动从缓存读取收到的分组。这样一来，如果TCP接收数据包到缓存的速度大于应用程序从缓存中读取数据包的速度，多个包就会被缓存，应用程序就有可能读取到多个首尾相接粘到一起的包。
>
>----
>
>3.**什么时候需要处理粘包现象？**
>
>1. 如果发送方发送的多组数据本来就是同一块数据的不同部分，比如说一个文件被分成多个部分发送，这时当然不需要处理粘包现象
>2. 如果多个分组毫不相干，甚至是并列关系，那么这个时候就一定要处理粘包现象了
>
>------
>
>**4. 处理粘包现象？**
>
>发送方处理办法： 对于发送方造成的粘包问题，可以通过关闭Nagle算法来解决，使用TCP_NODELAY选项来关闭算法。
>
>
>
>（2）接收方
>
>接收方没有办法来处理粘包现象，只能将问题交给应用层来处理。
>
>（2）应用层
>
>应用层的解决办法简单可行，不仅能解决接收方的粘包问题，还可以解决发送方的粘包问题。
>
>解决办法：循环处理，应用程序从接收缓存中读取分组时，读完一条数据，就应该循环读取下一条数据，直到所有数据都被处理完成,有两种处理办法。
>
>- 格式化数据：每条数据有固定的格式（开始符，结束符），这种方法简单易行，但是选择开始符和结束符时一定要确保每条数据的内部不包含开始符和结束符。
>- 发送长度：发送每条数据时，将数据的长度一并发送，例如规定数据的前4位是数据的长度，应用层在处理时可以根据长度来判断每个分组的开始和结束位置。
>
>---
>
>**5.UDP会不会产生粘包问题呢？**
>TCP为了保证可靠传输并减少额外的开销（每次发包都要验证），采用了基于流的传输，基于流的传输不认为消息是一条一条的，是无保护消息边界的（保护消息边界：指传输协议把数据当做一条独立的消息在网上传输，接收端一次只能接受一条独立的消息）。
>
>UDP则是面向消息传输的，是有保护消息边界的，接收方一次只接受一条独立的信息，所以不存在粘包问题。



# ==UDP==

## UDP协议特点

>UDP协议全称是用户数据报协议，在网络中它与TCP协议一样用于处理数据包，是一种无连接的协议。传输数据之前源端和终端不建立连接,在发送端，UDP传送数据的速度仅仅是受应用程序生成数据的速度、 计算机的能力和传输带宽的限制； 在接收端，UDP把每个消息段放在队列中，应用程序每次从队列中读一个消息段。
>
>1. 由于传输数据不建立连接，因此也就不需要维护连接状态，包括收发状态等， 因此一台服务机可同时向多个客户机传输相同的消息。
>2. 吞吐量不受拥挤控制算法的调节，只受应用软件生成数据的速率、传输带宽、 源端和终端主机性能的限制。
>3. UDP是面向报文的。发送方的UDP对应用程序交下来的报文， 在添加首部后就向下交付给IP层。既不拆分，也不合并，而是保留这些报文的边界， 因此，应用程序需要选择合适的报文大小。
>4. 头部开销小，UDP信息包的标题很短，只有8个字节，相对于TCP的20个字节信息包的额外开销很小。
>5. UDP 不止支持一对一的传输方式，同样支持一对多，多对多，多对一的方式，也就是说 UDP 提供了单播，多播，广播的功能。



## TCP协议特点

>- 面向连接
>
>  面向连接，是指发送数据之前必须在两端建立连接。建立连接的方法是“三次握手”，这样能建立可靠的连接。建立连接，是为数据的可靠传输打下了基础。
>
>- 仅支持单播传输
>
>每条TCP传输连接只能有两个端点，只能进行点对点的数据传输，不支持多播和广播传输方式。
>
>- 面向字节流
>
>TCP不像UDP一样那样一个个报文独立地传输，而是在不保留报文边界的情况下以字节流方式进行传输。
>
>- 可靠传输
>
>  对于可靠传输，判断丢包，误码靠的是TCP的段编号以及确认号。TCP为了保证报文传输的可靠，就给每个包一个序号，同时序号也保证了传送到接收端实体的包的按序接收。然后接收端实体对已成功收到的字节发回一个相应的确认(ACK)；如果发送端实体在合理的往返时延(RTT)内未收到确认，那么对应的数据（假设丢失了）将会被重传。
>
>- 提供拥塞控制
>
>当网络出现拥塞的时候，TCP能够减小向网络注入数据的速率和数量，缓解拥塞
>
>- TCP提供全双工通信
>
>TCP允许通信双方的应用程序在任何时候都能发送数据，因为TCP连接的两端都设有缓存，用来临时存放双向通信的数据。当然，TCP可以立即发送一个数据段，也可以缓存一段时间以便一次发送更多的数据段（最大的数据段大小取决于MSS）



## ==TCP 和 UDP区别==

>#### 1. 对比
>
>|              | UDP                                        | TCP                                    |
>| :----------- | :----------------------------------------- | :------------------------------------- |
>| 是否连接     | 无连接                                     | 面向连接                               |
>| 是否可靠     | 不可靠传输，不使用流量控制和拥塞控制       | 可靠传输，使用流量控制和拥塞控制       |
>| 连接对象个数 | 支持一对一，一对多，多对一和多对多交互通信 | 只能是一对一通信                       |
>| 传输方式     | 面向报文                                   | 面向字节流                             |
>| 首部开销     | 首部开销小，仅8字节                        | 首部最小20字节，最大60字节             |
>| 适用场景     | 适用于实时应用（IP电话、视频会议、直播等） | 适用于要求可靠传输的应用，例如文件传输 |
>
>#### 2. 总结
>
>- TCP向上层提供面向连接的可靠服务 ，UDP向上层提供无连接不可靠服务。
>- 虽然 UDP 并没有 TCP 传输来的准确，但是也能在很多实时性要求高的地方有所作为
>- 对数据准确性要求高，速度可以相对较慢的，可以选用TCP
>
>------
>
>UDP首部 和 TCP首部区别   TCP首部就是看TCP报文格式
>
>![img](https://img-blog.csdn.net/20180710205000980?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2Rhbmd6aGFuZ2ppbmc5Nw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)source port: 源端口号，占16位，2个字节
>
>1. dest port: 目的端口号，占16位，2个字节
>     length: 此字段标记了整个数据报（UDP的首部+UDP数据）的最大长度
>     checksum: 检验和，此字段用处是用来检查收到地数据的对错的
>     ps：如果校验和出错，就会直接丢弃
>     Application data: 数据部分（如果有的话)

## 如何保证UDP可靠传输

>**模仿TCP的可靠传输：确认机制、重传机制、滑动窗口哦 **
>
>UDP不属于连接协议，具有资源消耗少，处理速度快的优点，所以通常音频，视频和普通数据在传送时，使用UDP较多，因为即使丢失少量的包，也不会对接受结果产生较大的影响。
>
>传输层无法保证数据的可靠传输，只能通过应用层来实现了。实现的方式可以参照tcp可靠性传输的方式，只是实现不在传输层，实现转移到了应用层。
>
>最简单的方式是在应用层模仿传输层TCP的可靠性传输。下面不考虑拥塞处理，可靠UDP的简单设计。
>
>- 1、添加seq/ack机制，确保数据发送到对端
>- 2、添加发送和接收缓冲区，主要是用户超时重传。
>- 3、添加超时重传机制。
>
>详细说明：送端发送数据时，生成一个随机seq=x，然后每一片按照数据大小分配seq。数据到达接收端后接收端放入缓存，并发送一个ack=x的包，表示对方已经收到了数据。发送端收到了ack包后，删除缓冲区对应的数据。时间到后，定时任务检查是否需要重传数据。

## UDP丢包

>**收包率低/丢包率高的原因分析**
>
>- （1） 缓存太小，不能及时接收数据。
>
>  连续多个UDP包超过了UDP接收缓冲区大小 ，比如：
>
>  1. 如：UDP包过大
>  2. 如：UDP发包速率过快，突发大数据流量超过了缓冲区上限
>
>- （2）recvfrom()接收到数据之后处理速度太慢
>
>  如果数据接收和处理是连续进行的，那么可能由于数据处理过慢，两次recvfrom调用的时间间隔里发过来的包丢失
>
>**对应的解决方法**
>
>- UDP包过大
>
>  解决方法：增加系统发送或接收缓冲区大小
>
>  ```cpp
>  int nBuf=32*1024;//设置为32K  
>  setsockopt(s,SOL_SOCKET,SO_RCVBUF,(const char*)&nBuf,sizeof(int));
>  setsockopt(s,SOL_SOCKET,SO_SNDBUF,(const char*)&nBuf,sizeof(int));
>  123
>  ```
>
>- 发包速率过快
>
>  解决方法:增加应答机制，处理完一个包后，在继续发包
>
>- recvfrom()接收到数据之后处理速度太慢
>
>  服务器程序启动之出，开辟两个线程，一个线程专门用于接收数据包，并存放在应用层的缓存区；另外一个线程用于专门处理和响应数据包请求，避免因为处理数据造成数据丢包。其本质上还是增大了缓冲区大小，只是将系统缓冲区转移到了自己的缓冲区。
>
>- 最复杂的方式
>
>  在应用层实现丢包**重发机制**和**超时机制**，确保数据包不丢失。



# ==---------------------------------------------------------------网络层IP==

# ==IP&&MAC==

>IP处在互连网络层。负责提供基本的数据封包传送功能，让每一块数据包都能够到达目的主机含义：指分组数据发送到最终目标地址的功能。
>
>1. 路由控制
>     含义：指分组数据发送到最终目标地址的功能。
>     路由控制表：所有主机都维护着一张路由控制表（Routing Table），记录了IP数据在下一步应该发给哪个路由器。IP包根据这个路由表在各个数据链路上传输。
>2. 数据链路的抽象化
>     IP对不同数据链路进行了抽象。那么不同数据链路之间最大的区别是：他们各自的最大传输单位（MTU，Maximum Transmission Unit）不同。
>     IP会进行分片处理（IP Fragmentation），将较大的IP包分成多个较小的IP包。
>     从网络层上看，可以忽略数据包在各个数据链路上的MTU，只需要按照原地址发送的长度接收数据包。
>3. IPv6把IP地址由32位增加到128位

## ARP地址解析协议

>**地址解析协议：**
>
>作用是在以太网环境中，**数据的传输所依懒的是MAC地址而非IP地址，而将已知IP地址转换为MAC地址的工作是由ARP协议来完成的**。在任何时候，一台主机有IP数据报文发送给另一台主机，它都要知道接收方的逻辑（IP）地址。但是IP地址必须封装成帧才能通过物理网络这就意味着发送方必须有接收方的物理（MAC）地址，因此需要完成逻辑地址到物理地址的映射。而ARP协议可以接收来自IP协议的逻辑地址，将其映射为相应的物理地址，然后把物理地址递交给数据链路层。
>
>-----
>
>**ARP**请求：
>
>任何时候，当主机需要找出这个网络中的另一个主机的物理地址时，它就可以发送一个ARP请求报文，这个报文包好了发送方的MAC地址和IP地址以及接收方的IP地址。因为发送方不知道接收方的物理地址，所以这个查询分组会在网络层中进行广播， ARP响应局域网中的每一台主机都会接受并处理这个ARP请求报文，然后进行验证，查看接收方的IP地址是不是自己的地址
>
>----
>
>**多播和组播的却别**
>
>网络和网络之间通过通过路由器连接（网状结构的）路由器在不同网段之间发送数据，不负责丢包重传，
>网络层：叫数据包---内含IP地址，，网络层只关心，接收到数据包之后选择路径，发到目标地址
>
>---
>
>**同一网段的ARP地址解析流程：**
>
>假如主机A向主机B进行通信：现假设主机A和B在同一个网段（均位于192.168.1.0/24网段） ，主机A要向主机B发送信息。主机A和主机B的IP地址和MAC地址均在图中已有标识，此时主机A已知道主机B的IP地址，要想获得主机B的MAC地址，棘突的地址解析流程如下。
>
>1. 主机A首先查看自己的ARP表（它是一个IP地址与MAC地址的映射表），确定其中是否包含有主机B的IP地址和对应的MAC地址。如果找到了对应的MAC地址，则主机A直接利用ARP表中的MAC地址对IP数据包进行帧封装，并将数据包发送给主机B。
>2. 如果主机A在ARP表中找不到对应的MAC地址，则先缓存该数据报文，然后以广播方式（目的MAC地址为广播地址——FFFFFF，任意网段的节点都可以接受到）发送一个ARP请求报文。ARP请求报文中的发送端（源）IP地址和发送端MAC地址分别为主机A的IP地址（192.168.1.1）和MAC地址（0002-6779-0F4C），目的IP地址和目的MAC地址为主机B的IP地址（192.168.1.2）和全0的MAC地址。因为ARP请求报文是以广播方式发送，所以该网段上的所有主机都可以接收到该请求包，但只有其IP地址与目的IP地址一致的主机B才会对该请求进行处理。
>3. 主机B将ARP请求报文中的发送端（即主机A）的IP地址和MAC地址存入自己的ARP表中。然后以单播方式向主机A发送一个ARP相应报文，应答报文中就包含了自己的MAC地址，也就是原来在请求报文中要请求的目的MAC地址。
>4. 主机A收到来自主机B的ARP响应报文之后，将主机B的MAC地址加入到自己的ARP表中以用于后续报文的转发，同时将原来缓存的IP数据包再次修改（在目的MAC地址字段填上主机B的MAC地址）后发送出去。
>
>---
>
>**不同网段的ARP地址解析流程：**
>
>1. 如果主机A不知道网关的MAC地址（也就是主机A的ARP表中没有网关对应的MAC地址表项），则主机A先在本网段中发出一个ARP请求广播，ARP请求报文中的目的IP地址为网关的IP地址，代表其目的就是想获得网关的MAC地址。如果主机A已经知道网关的MAC地址，则略过此步。
>2. 网关收到ARP广播包后同样会向主机A发回一个ARP应答包。当主机A收到的应答包中获得网关的MAC地址后，在主机A向主机B发送的原报文的目的MAC地址字段填上网关的MAC地址后发给网关。
>3. 如果网关的ARP表中已有主机B对应的MAC地址，则网关直接将在来自主机A的报文中的目的MAC地址字段填上主机B的MAC地址后转发给B。
>4. 如果网关ARP表中没有主机B的MAC地址，网关会再次向主机B所在的网段发送ARP广播请求，此时目的IP地址为主机B的IP地址，当网关从收到来自主机B的应答报文中获得主机B的MAC地址后，就可以将主机A发来的报文重新再目的MAC地址字段填上主机B的MAC地址后发送给B。



## IP地址和MAC地址缺一不可？

>**答案是肯定的**，我们来具体分析：
>
>​    在网络传输的过程中，第一次将信息从A端发往B端时，首先在A端需要将信息从应用层开始到物理层进行逐层封装，到达B端后再从物理层到应用层进行逐层分用解包，最后拿到信息。  信息在进行封装时，到网络层的时候只知道对方的IP地址，却不知道对方的MAC地址，我们知道数据链路层使用的是以太网协议，发送以太网帧，而以太网帧里需要源MAC地址和目的MAC地址，所以此时到不了数据链路层，无法发送帧，这个时候我们就需要通过ARP协议（以目标IP地址为线索，用来定位下一个应该接收数据分包的网络设备对应的MAC地址）来获取对端的MAC地址。这也是为什么需要ARP协议的原因。
>
>- 了解了上述过程，那么有人会问：**既然知道了对端的IP地址，为什么不直接使用IP地址来发送信息呢**？
>
>​       IP地址确实可以识别一台主机，但是在局域网中，IP地址都是动态分配的，当你下一次发送消息时，说不定就不是这个IP地址了（分配到了新的IP地址），但是MAC地址就不存在这个问题，其序列号存在于网卡中，全球唯一，一出厂网卡上就收录了MAC地址（48位的序列号）。
>
>- 那么此时又会有人问：**光使用MAC地址不就够了，还需要IP地址干嘛？**
>
>  如果用MAC地址方式寻址时，由于 MAC地址种类繁杂，到处分布在世界上海量的主机上，网关接收到你要访问的MAC后，该往那里转发呢? IP是按地域有序分布的，通过IP地址可以将地址进行解析，就好比你找某个地方，通过IP地址你可以找到大致的方位，如中国陕西，再通过MAC地址，找到具体地方的街道门牌号。所以用IP寻址能高效率的转发到目的地。



## MAC地址唯一么

>长度是48比特 6 字节 ，由16进制的数字组成，分为前24位和后24位。 出厂设置的时候唯一 。 
>
>但是用户可以进行修改：
>
>**当mac地址冲突时：**
>
>- 同一个局域网里如果有两个phy芯片的mac地址一样，系统会报出mac地址冲突的异常，此时两个phy都不能正常传输数据解决方法就是把冲突的地址改成不一样就行。
>
>- 不同局域网中的mac地址即使一样也不会造成冲突，毕竟网关不一样了。 类似于就是两个小区， IP就是小区，mac就是楼房号。



## 解决IPV4地址短缺---NAT

>NAT全称网络地址转换（Network Address Translation），也叫做网络掩蔽、IP掩蔽
>
>**它有什么用？**
>
>1. IP地址不足；这种通过使用少量的公有IP地址代表较多的私有IP地址的方式，将有助于减缓可用的IP地址空间的枯竭。
>2. 隐藏内部网络；NAT不仅能解决了lP地址不足的问题，而且还能够有效地避免来自网络外部的攻击，隐藏并保护网络内部的计算机。NAT 之内的 PC 联机到 Internet 上面时，他所显示的 IP 是 NAT 主机的公共 IP，所以Client端的PC当然就具有一定程度的安全了，外界在进行 portscan（端口扫描） 的时候，就侦测不到源Client 端的 PC 。
>3. 能够处理地址重复情况，避免了地址的重新编号，增加了编址的灵活性。
>4. 可以使多个使用TCP负载特性的服务器之间实现基本的数据包负载均衡。
>
>-----
>
>它的种类和特点
>**NAT的实现方式有三种，即静态转换Static Nat、动态转换DynamicNat和端口多路复用OverLoad。**
>
>1. 静态NAT：将内部私有IP和公网IP地址一对一绑定，这样公网IP地址不会变，主要用于一些需要实现外部网络对内部网络的指定设备（如服务器）的访问
>
>2. 动态NAT：保留多个公网IP，采用动态的方式分配给内部网络的设备（设备接入随机分配公网IP，设备断开就将公网IP放回池中，因此每次使用的公网IP会一直变化）
>
>3. 端口NAT（PAT）：将中小型网络隐藏在一个公网IP后面，修改外出数据包的源端口并进行端口转换（端口地址转换，Port Address Translation），端口多路复用（Port Multiplexing）。这样内部的设备都是用同一个IP接入互联网





# ==----------------------------------------------------------------------------web==

# 输入 URL 到页面展示到底发生了什么

>**1、客户端连接到Web服务器 建立TCP连接**
>
>首先客户端浏览器通过DNS协议解析到例如www.baidu,com对应的IP地址  通过这个IP地址找到客户端和服务器的路径，客户端向服务器段发起HTTP会话，与Web服务器的HTTP端口（默认为80）建立一个TCP套接字连接
>
>###### 2、发送HTTP请求
>
>通过TCP套接字，客户端向Web服务器发送一个文本的请求报文，一个请求报文由请求行、请求头部、空行和请求数据4部分组成。
>
>###### 3、服务器接受请求并返回HTTP响应
>
>Web服务器解析请求，定位请求资源。服务器将资源复本写到TCP套接字，由客户端读取。一个响应由状态行、响应头部、空行和响应数据4部分组成。
>
>###### 4、释放连接[TCP连接](https://www.jianshu.com/p/ef892323e68f)
>
>若connection 模式为close，则服务器主动关闭[TCP连接](https://www.jianshu.com/p/ef892323e68f)，客户端被动关闭连接，释放[TCP连接](https://www.jianshu.com/p/ef892323e68f);若connection 模式为keepalive，则该连接会保持一段时间，默认时间是两小时，在该时间内可以继续接收请求;
>
>###### 5、客户端浏览器解析HTML内容
>
>客户端浏览器首先解析状态行，查看表明请求是否成功的状态代码。然后解析每一个响应头，响应头告知以下为若干字节的HTML文档和文档的字符集。客户端浏览器读取响应数据，根据HTML的语法对其进行解析，并在浏览器窗口中显示。









# Tcp通信中服务器处理客户端意外断开！



>主要就是答心跳机制：
>
>客户端并没有正常关闭socket，双方并未按照协议上的四次挥手去断开连接，一般的处理办法都是利用保活机制。而保活机制分又可以让底层实现也可自己实现。
>
>一、双方拟定心跳（自实现）
>
>一般由客户端发送心跳包，服务端并不回应心跳，只是定时轮询判断一下与上次的时间间隔是否超时（超时时间自己设定）。服务器并不主动发送是不想增添服务器的通信量，减少压力。
>
>但这会出现三种情况：
>
>1. 客户端由于某种网络延迟等原因很久后才发送心跳（它并没有断），这时服务器若利用自身设定的超时判断其已经断开，而后去关闭socket。若客户端有重连机制，则客户端会重新连接。若不确定这种方式是否关闭了原本正常的客户端，则在ShutDown的时候一定要选择send,表示关闭发送通道，服务器还可以接收一下，万一客户端正在发送比较重要的数据呢，是不？
>
>
>
>2. 客户端很久没传心跳，确实是自身断掉了。在其重启之前，服务端已经判断出其超时，并主动close，则四次挥手成功交互。
>
>
>
>3. 客户端很久没传心跳，确实是自身断掉了。在其重启之前，服务端的轮询还未判断出其超时，在未主动close的时候该客户端已经重新连接。
>
>这时候若客户端断开的时候发送了FIN包，则服务端将会处于CLOSE_WAIT状态；
>
>这时候若客户端断开的时候未发送FIN包，则服务端处还是显示ESTABLISHED状态；
>
>而新连接上来的客户端（也就是刚才断掉的重新连上来了）在服务端肯定是ESTABLISHED;这时候就有个问题，若利用轮询还未检测出上条旧连接已经超时（这很正常，timer总有个间隔吧），而在这时，客户端又重复的上演情况3，那么服务端将会出现大量的假的ESTABLISHED连接和CLOSE_WAIT连接。
>
>最终结果就是新的其他客户端无法连接上来，但是利用netstat还是能看到一条连接已经建立，并显示ESTABLISHED，但始终无法进入程序代码。个人最初感觉导致这种情况是因为假的ESTABLISHED连接和CLOSE_WAIT连接会占用较大的系统资源，程序无法再次创建连接（因为每次我发现这个问题的时候我只连了10个左右客户端却已经有40多条无效连接）。而最近几天测试却发现有一次程序内只连接了2，3个设备，但是有8条左右的虚连接，此时已经连接不了新客户端了。这时候我就觉得我想错了，不可能这几条连接就占用了大量连接把，如果说几十条还有可能。但是能肯定的是，这个问题的产生绝对是设备在不停的重启，而服务器这边又是简单的轮询，并不能及时处理，暂时还未能解决。
>
>
>
>二、利用KeepAlive
>
>在S和C建立连接后，若双方均不发送数据只保持连接，默认是两个小时，系统会自动启动保活机制向peer发送包，看对方是否回应ack，若可以收到则继续保持，否则无效。



# 服务器如何给多个客户端分配端口号

>在一台计算机上，使用socket通信时，不同进程区分网络通信的连接依靠三个参数：通信所用协议、地址IP、端口号。
>
>1. 现在使用多路IO复用epoll等，配置好点的服务器可以支持数十万个并发连接，端口号为16位，最多才2^16-1，且加上一些常用的端口号不能使用，可用的端口号都没那么多。
>2. 现在服务器大多使用防火墙，防火墙只对特定端口开放。如果accept随机分配端口号，会不能通过防火墙。
>
>TCP/IP协议中，IP协议是端到端的协议，它只是负责把把数据发送到端，交付给上层而已。运输层TCP、UDP加上了端口号，目的是区分不同的应用。其中TCP还实现了流量控制、可靠传输等，而UDP只是应该是没有对IP层数据进行处理了。
>
>问题：： 在以往的知识中，我知道一个应用程序只能使用一个端口号，如果accept返回的句柄还是使用listen的端口号，那么怎么实现通信呢？如果建立多个连接，应用程序怎么区收到的信息来自哪个客户端呢？
>
>答案：： 现在才理解到accept返回的句柄建立的连接包括四部分：源IP、源端口号、目的IP、目的端口号。这样在一个应用程序中，就算和多个客户端建立连接，在收到数据后，应用程序通过目的IP和目的端口号也能区分是哪一条连接。



# 两台电脑连接起来ping不通

>假设A代表电脑1，B代表电脑2
>
>情形1：A能ping通B，而B却ping不通A
>
>情形2：A和B都无法ping通
>
>情形1解决：关闭A的防火墙
>
>情形2解决：检查是否在同一局域网内，防火墙是否关闭



# ping用到哪些协议

>**1）**假设有两个主机，主机A（192.168.0.1）和主机B（192.168.0.2），现在我们要监测主机A和主机B之间网络是否可达，那么我们在主机A上输入命令：ping 192.168.0.2；
>
>**2）**此时，ping命令会在主机A上构建一个 ICMP的请求数据包（数据包里的内容后面再详述），然后 ICMP协议会将这个数据包以及目标IP（192.168.0.2）等信息一同交给IP层协议；
>
>**3）**IP层协议得到这些信息后，将源地址（即本机IP）、目标地址（即目标IP：192.168.0.2）、再加上一些其它的控制信息，构建成一个IP数据包；
>
>**4）**IP数据包构建完成后，还不够，还需要加上MAC地址，因此，还需要通过ARP映射表找出目标IP所对应的MAC地址。当拿到了目标主机的MAC地址和本机MAC后，一并交给数据链路层，组装成一个数据帧，依据以太网的介质访问规则，将它们传送出出去；
>
>**5）**当主机B收到这个数据帧之后，会首先检查它的目标MAC地址是不是本机，如果是就接收下来处理，接收之后会检查这个数据帧，将数据帧中的IP数据包取出来，交给本机的IP层协议，然后IP层协议检查完之后，再将ICMP数据包取出来交给ICMP协议处理，当这一步也处理完成之后，就会构建一个ICMP应答数据包，回发给主机A；
>
>**6）**在一定的时间内，如果主机A收到了应答包，则说明它与主机B之间网络可达，如果没有收到，则说明网络不可达。除了监测是否可达以外，还可以利用应答时间和发起时间之间的差值，计算出数据包的延迟耗时。
>
>通过ping的流程可以发现，ICMP协议是这个过程的基础，是非常重要的，下面的章节会把ICMP协议再详细解释一下，请继续往下读。

## 静态资源和动态资源的访问是如何处理

>要遵循一条原则——一个服务只做一件事。要做动态请求就专做动态请求，要做静态请求就专做静态请求，这样才能提高性能。
>我们要做的，就是当用户访问静态资源时，让Nginx将静态资源返回给用户；当用户访问动态资源时，将访问转到Tomcat应用服务器上，Tomcat将数据返回给Nginx，Nginx再返回给用户。这样来实现动静分离。





## 负载均衡调度算法

> 负载均衡是为了解决并发情况下，多个请求访问，把请求通过提前约定好的规则转发给各个server。其中有好几个种经典的算法。在用java代码编写这几种算法之前，先来了解一下负载均衡这个概念。
>
> 
>
>**1.概念**
>
>  负载，从字面意思可以分析，是指后端server可以承受的压力。这个一方面是服务器的性能，另一方面就是代码的质量了。
>
>  均衡，就是说把服务部署在多态server，如何调度这些资源。根据服务器性能不同，进行一个权衡。
>
>  当web访问量增加，服务器性能不同，更好的去利用服务器，我们需要负载均衡算法。
>
> 
>
>**2.几种负载均衡算法简介**
>[![img](https://img2020.cnblogs.com/blog/885859/202004/885859-20200409221427147-1092568434.png)](https://img2020.cnblogs.com/blog/885859/202004/885859-20200409221427147-1092568434.png)
>
>主要的负载均衡算法是图中这些，在代码实现之前，我们先简单回顾一下他们的概念。
>
> 
>
>**轮询法：**
>
>  轮询算法按顺序把每个新的连接请求分配给下一个服务器，最终把所有请求平分给所有的服务器。
>
>  优点：绝对公平
>
>  缺点：无法根据服务器性能去分配，无法合理利用服务器资源。
>
> 
>
>**加权轮询法：**
>
>  该算法中，每个机器接受的连接数量是按权重比例分配的。这是对普通轮询算法的改进，比如你可以设定：第三台机器的处理能力是第一台机器的两倍，那么负载均衡器会把两倍的连接数量分配给第3台机器。加权轮询分为：简单的轮询、平滑的轮询。
>
>   什么是平滑的轮询，就是把每个不同的服务，平均分布。在Nginx源码中，实现了一种叫做平滑的加权轮询（smooth weighted round-robin balancing）的算法，它生成的序列更加均匀。5个请求现在分散开来，不再是连续的。
>
> 
>
>**随机法：**
>
>  负载均衡方法随机的把负载分配到各个可用的服务器上，通过随机数生成算法选取一个服务器。毕竟随机，，有效性受到了质疑。
>
> 
>
>**加权随机法：**
>
>   获取带有权重的随机数字，随机这种东西，不能看绝对，只能看相对。
>
> 
>
>**IP_Hash算法：**
>
>  hash(object)%N算法，通过一种散列算法把请求分配到不同的服务器上。
>
>
>
>## 轮循(Round Robin)
>
>这种方法会将收到的请求循环分配到服务器集群中的每台机器，即有效服务器。如果使用这种方式，所有的标记进入虚拟服务的服务器应该有相近的资源容量以及负载形同的应用程序。如果所有的服务器有相同或者相近的性能那么选择这种方式会使服务器负载形同。基于这个前提，轮循调度是一个简单而有效的分配请求的方式。然而对于服务器不同的情况，选择这种方式就意味着能力比较弱的服务器也会在下一轮循环中接受轮循，即使这个服务器已经不能再处理当前这个请求了。这可能导致能力较弱的服务器超载。
>
>## 加权轮循(Weighted Round Robin)
>
>这种算法解决了简单轮循调度算法的缺点：传入的请求按顺序被分配到集群中服务器，但是会考虑提前为每台服务器分配的权重。管理员只是简单的通过服务器的处理能力来定义各台服务器的权重。例如，能力最强的服务器A给的权重是100，同时能力最低的服务器给的权重是50。这意味着在服务器B接收到第一个请求之前前，服务器A会连续的接受到2个请求，以此类推。
>
>## 最少连接数(Least Connection)
>
>以上两种方法都没有考虑的是系统不能识别在给定的时间里保持了多少连接。因此可能发生，服务器B服务器收到的连接比服务器A少但是它已经超载，因为服务器B上的用户打开连接持续的时间更长。这就是说连接数即服务器的负载是累加的。这种潜在的问题可以通过"最少连接数"算法来避免：传入的请求是根据每台服务器当前所打开的连接数来分配的。即活跃连接数最少的服务器会自动接收下一个传入的请求。接本上和简单轮询的原则相同：所有拥有虚拟服务的服务器资源容量应该相近。值得注意的是，在流量率低的配置环境中，各服务器的流量并不是相同的，会优先考虑第一台服务器。这是因为，如果所有的服务器是相同的，那么第一个服务器优先，直到第一台服务器有连续的活跃流量，否则总是会优先选择第一台服务器。
>
>## 最少连接数慢启动时间(Least Connection Slow Start Time)
>
>对最少连接数和带权重的最小连接数调度方法来说，当一个服务器刚加入线上环境是，可以为其配置一个时间段，在这段时间内连接数是有限制的而且是缓慢增加的。这为服务器提供了一个'过渡时间'以保证这个服务器不会因为刚启动后因为分配的连接数过多而超载。这个值在L7配置界面设置。
>
>## 加权最少连接(Weighted Least Connection)
>
>如果服务器的资源容量各不相同，那么"加权最少连接"方法更合适：由管理员根据服务器情况定制的权重所决定的活跃连接数一般提供了一种对服务器非常平衡的利用，因为他它借鉴了最少连接和权重两者的优势。通常，这是一个非常公平的分配方式，因为它使用了连接数和服务器权重比例;集群中比例最低的服务器自动接收下一个请求。但是请注意，在低流量情况中使用这种方法时，请参考"最小连接数"方法中的注意事项。



## 如何保证web安全

>https://zhuanlan.zhihu.com/p/24864406
>
>1. HTTPS 
>2. 输入验证
>3. 输出编码



## 如何加速web静态资源的访问

>CDN的概念大致可以理解为在每个地区建立一个网络中心，这样做的好处就是用户访问速度加快了，不然的话就比如说你只有一台服务器在北京，而我要在全国各地去访问，这样的话信道传输速度肯定会变慢，所以为了使内容传输的更快、更稳定。可以通过在网络各处放置节点服务器的方法来加快资源访问。
>
>![img](https://img-blog.csdn.net/20180412121857939)
>
>-----
>
>设置缓存 可以向浏览器等Web客户端提供文档  可以放置数据文件 让全世界下载
>
>----
>
>压缩： 降低网络传输连接的字节数了呗 



# ==--------------------------------------------------------------------操作系统==

# **操作系统**

>**操作系统**（英语：**O**perating **S**ystem，缩写：**OS**）是一组主管并控制[计算机](https://zh.wikipedia.org/wiki/电子计算机)操作、运用和运行[硬件](https://zh.wikipedia.org/wiki/硬件)、[软件](https://zh.wikipedia.org/wiki/软件)[资源](https://zh.wikipedia.org/wiki/資源_(計算機科學))和提供公共[服务](https://zh.wikipedia.org/wiki/守护进程)来组织用户交互的相互关联的[系统软件](https://zh.wikipedia.org/wiki/系统软件)[程序](https://zh.wikipedia.org/wiki/程序)，同时也是计算机系统的内核与基石。操作系统需要处理如管理与配置[内存](https://zh.wikipedia.org/wiki/内存)、决定系统资源供需的优先次序、控制输入与输出设备、操作[网络](https://zh.wikipedia.org/wiki/计算机网络)与管理[文件系统](https://zh.wikipedia.org/wiki/文件系统)等基本事务。操作系统也提供一个让用户与系统交互的操作界面。
>
>**操作系统是控制其他程序运行，管理系统资源并为用户提供操作界面的系统软件的集合**

## 操作系统主要有哪些功能

>主要包括5个方面的管理功能：进程与处理机管理、作业管理、存储管理、设备管理、文件管理。
>
>1、微处理器管理功能
>
>在大型操作系统中．可存在多个微处理器，并同时可管理多个作业。怎样选出其中一个作业进入主存储器难备运行，怎样为这个作业分配微处理器等等，都由微处理器管理模块负责。微处理器管理模块，要对系统中各个微处理器的状态进行登记，还要登记各个作业对微处理器的要求。管理模块还要用一个优化算法实现最佳调度规则。把所有的微处理器分配给各个用户作业使用。最终日的是提高微处理器的利用率。这就是操作系统的微处理器管理功能。
>
>2、内存管理功能
>
>内存储器的管理，主要由内存管理模块来完成。内存管理模块对内存的管理分三步。首先为各个用户作业分配内存空间；其次是保护已占内存空间的作业不被破坏；最后，是结合硬件实现信息的物理地址至逻辑地址的变换。
>
>3、外部设备管理功能
>
>设备管理模块的任务是当用户要求某种设备时，应马亡分配给用户所要求的设备，并技用户要求驱动外部设备以供用户应用。并且对外部设备的中断请求，设备管理模块要给以响应并处理。这就是操作系统的外部设备管理功能。
>
>4、文件管理功能
>
>操作系统对文件的管理主要是通过文件管理模块来实现的。文件管理模块管理的范围包括文件目录、文件组织、文件操作和文件保护。
>
>5、进程管理功能
>
>进程管理也称作业管理，用户交给计算机处理的工作称为作业。作业管理是由进程管理模块来控制的，进程管理模块对作业执行的全过程进行管理和控制。



## 中断

>中断的分类
>
>![img](https://upload-images.jianshu.io/upload_images/18464438-13f85d45ddec0f0a.png?imageMogr2/auto-orient/strip|imageView2/2/format/webp)



## 堆与栈区别

>堆与栈实际上是操作系统对进程占用的内存空间的两种管理方式，主要有如下几种区别：
>
>1. 管理方式不同。栈由操作系统自动分配释放，无需我们手动控制；堆的申请和释放工作由程序员控制，容易产生内存泄漏；
>
>2. 空间大小不同。每个进程拥有的栈的大小要远远小于堆的大小。理论上，程序员可申请的堆大小为虚拟内存的大小，进程栈的大小 64bits 的 Windows 默认 1MB，64bits 的 Linux 默认 10MB；
>
>4. 分配方式不同。堆都是动态分配的，没有静态分配的堆。栈有2种分配方式：静态分配和动态分配。静态分配是由操作系统完成的，比如局部变量的分配。动态分配由alloca函数进行分配，但是栈的动态分配和堆是不同的，他的动态分配是由操作系统进行释放，无需我们手工实现。
>
>5. 分配效率不同。栈由操作系统自动分配，会在硬件层级对栈提供支持：分配专门的寄存器存放栈的地址，压栈出栈都有专门的指令执行，这就决定了栈的效率比较高。堆则是由C/C++提供的库函数或运算符来完成申请与管理，实现机制较为复杂，频繁的内存申请容易产生内存碎片。显然，堆的效率比栈要低得多。
>
>6. **存放内容不**同。**栈存放的内容，函数返回地址、相关参数、局部变量和寄存器内容等**。当主函数调用另外一个函数的时候，要对当前函数执行断点进行保存，需要使用栈来实现，首先入栈的是主函数下一条语句的地址，即扩展指针寄存器的内容（EIP），然后是当前栈帧的底部地址，即扩展基址指针寄存器内容（EBP），再然后是被调函数的实参等，一般情况下是按照从右向左的顺序入栈，之后是被调函数的局部变量，注意静态变量是存放在数据段或者BSS段，是不入栈的。出栈的顺序正好相反，最终栈顶指向主函数下一条语句的地址，主程序又从该地址开始执行。**堆，一般情况堆顶使用一个字节的空间来存放堆的大小，而堆中具体存放内容是由程序员来填充的。**



## OS是怎么做到原子操作的

>　　如果没有硬件提供的原子操作，只有软件上层是不可能设计出原子操作的。因为中断便是CPU在当前执行的硬件指令的指令周期的最后一个时钟周期去检测中断引脚，因此无论如何，当前的硬件指令可以执行到底。
>
>1. **中断保护**：相比另一种方式更加简便，但是只适用于单核环境。因为如果是多核环境，需要发出信号使其它CPU也禁止中断，这将不再是原子操作，而且也让多核心在一定程度上失去了各个核心的独立性（多核的初衷就是让它们可以独立执行）。就算我们这样做了，也将付出极大的代价（保证各个核心的中断是一个原子操作），因此不提倡使用。
>
>2. **test&set原语**：实现相对复杂，且在多核环境下也可以工作。因为即使是多核，各个核心也在使用共享内存，而该指令针对的就是内存单元。在多核环境下，test&set原语会结合**总线锁**来保证同一时间只有一个核心可以访问共享内存，从而保证该原语在多核环境下的原子性。



## 大端 、 小端模式

>**画图更直观理解一下：**
>
>![img](https://pic2.zhimg.com/80/v2-1a4d887d6b9a2701a50114e2eac530f5_1440w.jpg)
>
>![img](https://pic3.zhimg.com/80/v2-28edbce28d58b8c4f4ff02fc1730929e_1440w.jpg)
>
>**总结下：**
>
>- 大端小端是不同的字节顺序存储方式，统称为**字节序**；
>- **大端模式**，是指数据的高字节位 保存在 内存的低地址中，而数据的低字节位 保存在 内存的高地址中。这样的存储模式有点儿类似于把数据当作字符串顺序处理：地址由小向大增加，而数据从高位往低位放。和我们”从左到右“阅读习惯一致。
>- **小端模式**，是指数据的高字节位 保存在 内存的高地址中，而数据的低字节位 保存在 内存的低地址中。这种存储模式将地址的高低和数据位权有效地结合起来，高地址部分权值高，低地址部分权值低，和我们的逻辑方法一致。
>
>**分享一个私人口语化记忆小技巧：**
>
>- **大端模式**：“【低位】字节却硬要存在【高】地址中“。---“低对高(或高对低)，门不当户不对，真令人头大”，记作大端模式。
>- **小端模式**：“【低位】字节正好存在【低】地址中”。--- “低对低，门当户对，你侬我侬”，记作小端模式。
>
>
>
>**3> 为什么要学习理解“大端”“小端”**
>
>**大端模式：**
>
>基于其存储特点，符号位在所表示的数据的内存的第一个字节中，便于快速判断数据的正负和大小（CPU做数值运算时从内存中依顺序依次从低位地址到高位地址取数据进行运算，大端就会最先拿到数据的(高字节的)符号位）。
>
>**小端模式：**
>
>基于其存储特点，内存的低地址处存放低字节，所以在强制转换数据时不需要调整字节的内容（比如，把int---4字节强制转换成short---2字节，就可以直接把int数据存储的前两个字节给short就行，因为其前两个字节刚好就是最低的两个字节，符合转换逻辑；另外CPU做数值运算时从内存中依顺序依次从低位地址到高位地址取数据进行运算，开始只管取值，最后刷新最高位地址的符号位就行，这样的运算方式会更高效一些）。
>
>因为两种模式各有优点，存在“你有我无，你无我有”的特点，所以造就了不同的硬件厂商基于不同的效率(角度)考虑，有了不同的硬件设计支持，最终形成了计算机各个相关领域目前并没有采用统一的字节序，没有统一标准的现状。
>
>
>
>**“大端“ ”小端” 各有优点，同吃鸡蛋方式一样，世人都有各自选择的权利。**





# ==---------------------------------------------------------------------------进程==

## 进程、线程的区别和联系

>1. 根本区别：进程是操作系统资源分配的基本单位，而线程是处理器任务调度和执行的基本单位
>2. 资源开销：每个进程都有独立的代码和数据空间（程序上下文），程序之间的切换会有较大的开销；线程可以看做轻量级的进程，同一类线程共享代码和数据空间，每个线程都有自己独立的运行栈和程序计数器（PC），线程之间切换的开销小。
>3. 包含关系：进程至少包含一个线程；线程是进程的一部分，所以线程也被称为轻量级进程。
>4. 内存分配：同一进程的线程共享本进程的地址空间和资源，而进程之间的地址空间和资源是相互独立的
>5. 影响关系：一个进程崩溃后，不会对其他进程产生影响（在保护模式下），但是一个线程崩溃整个进程都死掉。所以多进程要比多线程健壮。



## ==进程之间通信方式==

>https://blog.csdn.net/ludan_xia/article/details/105653707   通信方式及其特点呗 在这里哈呢
>
>##### 1. 管道
>
>管道是一种半双工的通信方式，数据只能单向流动。只能把第一个命令的输出作为第二个命令的输入，如果进程之间想要互相通信的话，那么需要创建两个管道。管道的通知机制类似于缓存，就像一个进程把数据放在某个缓存区域，然后等着另外一个进程去拿.
>
>优点：比较简单，能够保证我们的数据已经真的被其他进程拿走了。
>
>缺点：通信方式效率低下，不适合频繁通信的进程。两个进程需要通信完事才能继续通信。  匿名管道通信 高级管道通信
>
>实现原理：
>
>管道是由内核管理的一个缓冲区，相当于我们放入内存中的一个纸条。管道的一端连接一个进程的输出。这个进程会向管道中放入信息。
>管道的另一端连接一个进程的输入，这个进程取出被放入管道的信息。一个缓冲区不需要很大一般为4K大小，它被设计成为环形的数据结构，以便管道可以被循环利用。当管道中没有信息的话，从管道中读取的进程会等待，直到另一端的进程放入信息。当管道被放满信息的时候，尝试放入信息的进程会等待，直到另一端的进程取出信息。当两个进程都终结的时候，管道也自动消失。
>
>**在Linux中，管道的实现并没有使用专门的数据结构，而是借助了文件系统的file结构和VFS的索引节点inode。通过将两个file结构指向同一个临时的 VFS 索引节点，而这个 VFS 索引节点又指向一个物理页面而实现的。**
>具体文件系统的索引节点是存放在磁盘上的，是一种静态结构，要使用它，必须调入内存，填写VFS的索引节点，因此，也称VFS索引节点是动态节点。
>
>----------
>
>-----------
>
>##### 2、消息队列
>
>管道通信属于一股脑的输入，不能把进程的数据放在某个内存之后就马上让进程返回呢？无需等待其他进程来取就返回呢？
>
>消息队列是由消息的链表组成，存放在内核中并由消息队列标识符标识。相比于管道只能承载无格式字节流以及缓冲区大小受限等缺点，消息队列克服了信号传递信息少。消息队列在发送数据的时候，按照一个个独立单元(消息体)进行发送，其中每个消息体规定大小块，同时发送方和接收方约定好消息类型。这种通信方式也类似于缓存吧。
>
>优点：消息队列允许不同进程以消息队列的形式发送给任意的进程。
>
>缺点：如果发送到消息队列的数据太大，需要拷贝消息的时间也就越多
>
>-------------
>
>------------
>
>##### 3、共享内存
>
>共享内存这个通信方式就可以很好着解决消息拷贝所消耗的时间了。
>
>系统加载一个进程的时候，分配给进程的内存并不是实际物理内存，而是虚拟内存空间。那么我们可以让两个进程各自拿出一块虚拟地址空间来，然后映射到相同的物理内存中，这样，两个进程虽然有着独立的虚拟内存空间，但有一部分却是映射到相同的物理内存，这就完成了内存共享机制了。
>
>优点：解决了信息拷贝所消耗的时间
>
>缺点：可能存在多进程内存竞争，类似于线程安全的问题。
>
>-----------
>
>--------
>
>##### 4、信号量
>
>共享内存最大的问题是什么？就是多进程竞争内存的问题，就像类似于我们平时说的线程安全问题。信号量就是解决了我们进程竞争冲突的问题。
>
>信号量是一个计数器，可以用来控制多个进程对共享资源的访问。它作为一种锁机制，防止某进程正在访问共享资源时，其他进程也访问该资源。因此，主要作为进程间内不同线程之间的同步手段。
>
>信号量定义了两种操作，`p`操作和`v`操作，`p`操作为申请资源，会将数值减去`M`，表示这部分被他使用了，其他进程暂时不能用。`v`操作是归还资源操作，告知归还了资源可以用这部分。例如信号量的初始值是 `1`，然后 `a `进程来访问内存`1`的时候，我们就把信号量的值设为 `0`，然后进程`b `也要来访问内存`1`的时候，看到信号量的值为 `0 `就知道已经有进程在访问内存`1`了，这个时候进程 `b` 就会访问不了内存`1`。所以说，信号量也是进程之间的一种通信方式。
>
>优点：解决多进程竞争内存的问题。
>
>----------
>
>--------
>
>##### 5、Socket
>
>1. 上面我们说的共享内存、管道、信号量、消息队列，他们都是多个进程在一台主机之间的通信
>
>2. 跨网络中的进程通信 只能就是用`Socket`套接字
>
>`Socket `是对 `TCP/IP `协议的封装，`Socket `只是个接口不是协议，通过` Socket` 我们才能使用 `TCP/IP `协议，除了` TCP`，也可以使用 `UDP `协议来传递数据,具体看我们编程选择的类型。 使用起来 绑定、监听、连接服务器、相互发送数据、断开连接。
>
>例如我们平时通过浏览器发起一个` http `请求，然后服务器给你返回对应的数据，这种就是采用` Socket` 的通信方式了。因为`Socket`底层封装的是`TCP`或者是`UDP` 在这里通信用的比较多。
>
>----------
>
>-------
>
>##### 总结
>
>当问到具体用过那个的时候： 可以答消息队列、以及`socket` 两种







## Linux有名管道和无名管道区别

>一个进程在管道的尾部写入数据，另一个进程从管道的头部读出数据。管道包括**无名管道**和**有名管道**两种，**前者只能用于父进程和子进程间的通信，后者可用于运行于同一系统中的任意两个进程间的通信。**
>
>**管道通信特点：**
>
>1. 管道通讯是**单向**的，有**固定的读端和写端**。
>2.  数据被进程从管道读出后，在管道中该数据就不存在了。
>3.  当进程去读取空管道的时候，进程会阻塞。
>4.  当进程往满管道写入数据时，进程会阻塞。
>5.  管道容量为**64KB**（#define PIPE_BUFFERS 16 include/linux/pipe_fs_i.h）



## 进程之间同步方式 互斥量、信号量

>1、**临界区**（Critical Section）:通过对多线程的串行化来访问公共资源或一段代码，速度快，适合控制数据访问。
>
>优点：保证在某一时刻只有一个线程能访问数据的简便办法
>
>缺点：虽然临界区同步速度很快，但却只能用来同步本进程内的线程，而不可用来同步多个进程中的线程。
>
> 
>
>2、**互斥量**（Mutex）:为协调共同对一个共享资源的单独访问而设计的。
>
>互斥量跟临界区很相似，比临界区复杂，互斥对象只有一个，只有拥有互斥对象的线程才具有访问资源的权限。
>
>优点：使用互斥不仅仅能够在同一应用程序不同线程中实现资源的安全共享，而且可以在不同应用程序的线程之间实现对资源的安全共享。
>
>缺点：①互斥量是可以命名的，也就是说它可以跨越进程使用，所以创建互斥量需要的资源更多，所以如果只为了在进程内部是用的话使用临界区会带来速度上的优势并能够减少资源占用量。因为互斥量是跨进程的互斥量一旦被创建，就可以通过名字打开它。
>
>②通过互斥量可以指定资源被独占的方式使用，但如果有下面一种情况通过互斥量就无法处理，比如现在一位用户购买了一份三个并发访问许可的数据库系统，可以根据用户购买的访问许可数量来决定有多少个线程/进程能同时进行数据库操作，这时候如果利用互斥量就没有办法完成这个要求，信号量对象可以说是一种资源计数器。
>
> 
>
>3、**信号量**（Semaphore）:为控制一个具有有限数量用户资源而设计。它允许多个线程在同一时刻访问同一资源，但是需要限制在同一时刻访问此资源的最大线程数目。互斥量是信号量的一种特殊情况，当信号量的最大资源数=1就是互斥量了。
>
>优点：适用于对Socket（套接字）程序中线程的同步。（例如，网络上的HTTP服务器要对同一时间内访问同一页面的用户数加以限制，只有不大于设定的最大用户数目的线程能够进行访问，而其他的访问企图则被挂起，只有在有用户退出对此页面的访问后才有可能进入。）
>
>缺点：①信号量机制必须有公共内存，不能用于分布式操作系统，这是它最大的弱点；
>
>②信号量机制功能强大，但使用时对信号量的操作分散， 而且难以控制，读写和维护都很困难，加重了程序员的编码负担；
>
>③核心操作P-V分散在各用户程序的代码中，不易控制和管理，一旦错误，后果严重，且不易发现和纠正。
>
> 
>
>4、**事件**（Event）: 用来通知线程有一些事件已发生，从而启动后继任务的开始。
>
>优点：事件对象通过通知操作的方式来保持线程的同步，并且可以实现不同进程中的线程同步操作。
>
>缺点：
>
>---
>
>**总结：**
>
>**①临界区不是内核对象，只能用于进程内部的线程同步，是用户方式的同步。互斥、信号量是内核对象可以用于不同进程之间的线程同步（跨进程同步）。**
>**②互斥其实是信号量的一种特殊形式。互斥可以保证在某一时刻只有一个线程可以拥有临界资源。信号量可以保证在某一时刻有指定数目的线程可以拥有临界资源。**





## 操作系统如何管理进程

>1. 首先对于一个进程，它在被执行前其实是一个.exe可执行程序。这个程序是被放在磁盘上的，当它要被执行的时候，它先被加载到内存当中，然后再放入到寄存器中，最后再让cpu执行该程序，这个时候一个静态的程序就变成了进程。
>
>2. 操作系统通过一个双向链表把进程连起来。但是，对于进程其实它是一个抽象的概念，操作系统通过PCB来描述进程，于是这个双向链表连接的其实是PCB进程管理块，它就是一个结构体，用来描述进程。



## 进程在内存中如何分配的

>1. 每个进程运行的时候，都会拿到4G的虚拟内存，其中3G是交给用户的，1G是交给内核的，而PCB进程管理块就是存储在这1G的内核系统空间中。
>
>2. 每个进程都有各自的有用户空间（0-3G），这个空间对系统中的其他进程是不可见的。
>3. 最高的1GB内核空间则为所有进程以及内核所共享。
>4. 至于为什么需要这个1G的内核空间，是因为进程需要调用一些系统调用，来交给内核跑，程序的一部分逻辑可能是要交给内核去跑的，所以一部分虚拟地址必须要留给内核使用。
>
><img src="https://img-blog.csdn.net/20180829231421923?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2x2eWliaW44OTA=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img" style="zoom:50%;" />
>
>其实每个进程的PCB都是存在所有进程共享的内核空间的中，操作系统要管理进程，也就是在内核空间中管理的，在内核空间中通过链表管理所有进程的PCB，如果有一个进程要被创建，实际上多分配了这么一个4G的虚拟内存，并在共享的内核空间中的双向链表中加入了自己的PCB。



## PCB  进程被分配了哪些资源

>**让你设计一个进程的数据结构，应该包括哪些数据字段以及分别对应的数据类型，为什么？** 

>1. 标识相关：pid  进程标识符用于唯一的表示一个进程
>2. 文件相关：进程需要记录打开的文件信息，于是需要文件描述符表
>3. 内存相关：内存指针，指向进程的虚拟地址空间（用户空间）信息
>4. 优先级相关：进程相对于其他进程的调度优先级
>5. 上下文信息相关：CPU的所有寄存器中的值、进程的状态以及堆栈上的内容，当内核需要切换到另一个进程时，需要保存当前进程的所有状态，即保存当前进程的进程上下文，以便再次执行该进程时，能够恢复切换时的状态，继续执行。
>6. 状态相关：进程当前的状态，说明该进程处于什么状态
>7. 信号相关：进程的信号处理函数，以及记录当前进程是否还有待处理的信号
>8. I/O相关：记录进程与各种I/O设备之间的交互
>
>
>![img](https://img-blog.csdnimg.cn/20210428124031119.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0NjY5NzE1,size_16,color_FFFFFF,t_70)



## 进程的地址空间是啥，它是属于进程的资源吗？

>地址空间存放的内容：
>
>| 名称   | 存储内容                                  |
>| ------ | ----------------------------------------- |
>| 栈     | 局部变量、函数参数、返回地址等            |
>| 堆     | 动态分配的内存                            |
>| BSS段  | 未初始化或初值为0的全局变量和静态局部变量 |
>| 数据段 | 已初始化且初值非0的全局变量和静态局部变量 |
>| 代码段 | 可执行代码、字符串字面值、只读变量        |
>

## 进程状态转换图

> ![img](https://img-blog.csdnimg.cn/20210428160022871.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0NjY5NzE1,size_16,color_FFFFFF,t_70)![点击并拖拽以移动](data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==)
>
> 1）创建状态：进程正在被创建（为进程分配内存空间，并创建、初始化PCB）
>
> 2）就绪状态：进程被加入到就绪队列中等待CPU调度运行（进程获得除了CPU处理机以外的所有资源，只需得到CPU调度）
>
> 3）运行状态：进程正在被运行（单核处理机每一时刻最多有1个进程处于运行态，双核则有两个）
>
> 4）等待阻塞状态：进程因为某种原因，比如等待I/O，等待设备，而暂时不能运行。（如等待操作系统分配打印机、等待读磁盘操作结果）
>
> 5）终止状态：进程运行完毕（进程从操作系统中撤销，操作系统回收进程拥有的资源，撤销PCB）



# ==----------------------------------------------------------------------线程和协程==

>1. 进程是操作系统资源分配的基本单位，而线程是处理器任务调度和执行的基本单位
>2. 线程的出现是为了降低上下文切换的消耗，提高系统的并发性，并突破一个进程只能干一样事的缺陷，使得进程内并发成为可能
>3. 协程通过在线程中实现调度，避免了陷入内核级别的上下文切换造成的性能损失，进而突破了线程在IO上的性能瓶颈。 这里边GO就是具体实现



## 线程之间通信方式

>机制：包括互斥锁、条件变量、读写锁
>
>1. 互斥锁提供了以排他方式防止数据结构被并发修改的方法。 
>2. 读写锁允许多个线程同时读共享数据，而对写操作是互斥的。 
>3. 条件变量可以以原子的方式阻塞进程，直到某个特定条件为真为止。对条件的测试是在互斥锁的保护下进行的。条件变量始终与互斥锁一起使用。
>
>-----
>
>在Java中线程之间通信方式
>
>1. wait/notify 等待   搭配  `synchronized`
>2. Volatile 内存共享
>3. Lock接口下的 `lock` 和 `unlock`
>4. 信号量机制  Semaphore `acquire` 和 `relase`
>5. `unsafe`下的一个类LockSupport 也可以实现
>
>-----
>
>临界区：通过多线程的串行化来访问公共资源或一段代码，速度快，适合控制数据访问；
>
>互斥量Synchronized/Lock：采用互斥对象机制，只有拥有互斥对象的线程才有访问公共资源的权限。因为互斥对象只有一个，所以可以保证公共资源不会被多个线程同时访问
>
>信号量Semphare：为控制具有有限数量的用户资源而设计的，它允许多个线程在同一时刻去访问同一个资源，但一般需要限制同一时刻访问此资源的最大线程数目。
>
>事件(信号)，Wait/Notify：通过通知操作的方式来保持多线程同步，还可以方便的实现多线程优先级的比较操作



## 详解信号量和互斥锁之间的区别 字节二面

>二 **信号量**
>
> 信号量我们都很熟悉，为了防止出现因多个程序同时访问一个共享资源而引发的一系列问题，我们需要一种方法，它可以通过生成并使用令牌来授权，在任一时刻只能有一个执行线程访问 代码的临界区域。临界区域是指执行数据更新的代码需要独占式地执行。而信号量就可以提供这样的一种访问机制，让一个临界区同一时间只有一个线程在访问它， 也就是说信号量是用来调协进程对共享资源的访问的。它可用于线程也可用于进程，但是通常我们不用信号量来做线程间的同步，而是用互斥量,基本上对于共享内存的同步都用信号量来做。
>
> 具体使用:linux进程间通信-信号量（semaphore）   
>
>三 ：**互斥量**
>
>互斥量即我们常用的mutex，通常我们拿它来做线程间的同步，很少用来对进程间做同步，其实它也是可以做到的。
>
>不过我们需要把互斥量放在多进程访问的共享内存中，并且设置调用pthread_rwlockattr_setpshared linuxAPI设置PTHREAD_PROCESS_SHARED属性这样互斥量才能对多进程可见。
>
>---
>
>在进程同步上互斥量性能还是比价好的呗



## 协程      协程的目的

>**协程是一种用户态的轻量级线程，**协程的调度完全由**用户控制**。协程拥有自己的寄存器上下文和栈。通常创建协程时，会从进程的堆中分配一段内存作为协程的栈。线程的栈有 8MB，而协程栈的大小通常只有几十 KB。

>1. 一是节省 CPU，避免系统内核级的线程频繁切换，造成的 CPU 资源浪费。好钢用在刀刃上。而协程是用户态的线程，用户可以自行控制协程的创建于销毁，极大程度避免了系统级线程上下文切换造成的资源浪费。
>
>2. 二是节约内存，在 64 位的 Linux 中，一个线程需要分配 8MB 栈内存和 64MB 堆内存，系统内存的制约导致我们无法开启更多线程实现高并发。而在协程编程模式下，可以轻松有十几万协程，这是线程无法比拟的。
>
>3. 三是稳定性，前面提到线程之间通过内存来共享数据，这也导致了一个问题，任何一个线程出错时，进程中的所有线程都会跟着一起崩溃。
>
>4. 异步性：   使用协程在开发程序之中，可以很方便的将一些耗时的 IO 操作异步化，例如写文件、耗时 IO 请求等。



## 进程、线程、协程 上下文切换

>**上下文切换：用户空间的应用程序，通过系统调用，进入内核空间**
>
>**进程上下文切换**
>
>1. 用户级上下文: 正文、数据、用户堆栈以及共享存储区；
>2. 寄存器上下文: 通用寄存器、程序寄存器(IP)、处理器状态寄存器(EFLAGS)、栈指针(ESP)；
>3. 系统级上下文: 进程控制块task_struct、内存管理信息(mm_struct、vm_area_struct、pgd、pte)、内核栈
>
>------
>
>**线程上下文切换**
>
>线程在切换的过程中需要保存当前线程Id、线程状态、堆栈、寄存器状态等信息。其中寄存器主要包括SP PC EAX等寄存器，其主要功能如下：
>
>1. SP:堆栈指针，指向当前栈的栈顶地址
>
>2. PC:程序计数器，存储下一条将要执行的指令
>
>3. EAX:累加寄存器，用于加法乘法的缺省寄存器
>
>----
>
>**协程上下文切换**
>
>每个协程有独立的栈，而栈留了变量的值，也保留了函数的调用关系、参数和返回值，CPU 中的栈寄存器 SP 指向了当前协程的栈，而指令寄存器 IP 保存着下一条要执行的指令地址。因此，从协程 1 切换到协程 2 时，首先要把 SP、IP 寄存器的值为线程 1 保存下来，再从内存中找出协程 2 上一次切换前保存好的寄存器值，写入 CPU 的寄存器，这样就完成了协程切换。

## 多进程好还是多线程好

>这个就是不能一概而论， 而应该是分开进行讨论呗 ：
>
>在数据共享、同步当中： 多进程和多线程就是各有优劣呗
>
>在内存、CPU上：      	   线程占用的就是比较少
>
>在创建和销毁上“			   线程占优势
>
>在可靠性： 						进程占优势
>
>分布式下： 					    进程占优势

>![img](https://img-blog.csdnimg.cn/20190613152236666.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpZ3VwZW5nNzkyOQ==,size_16,color_FFFFFF,t_70)

## 

# ==--------------------------------------------------------虚拟内存 与 内存管理==

## 传统内存存储管理方式缺点

>传统内存管理有连续分配和非连续分配：
>
>## 连续分配
>
>- 概念：连续分配为用户分配一个连续的内存空间，比如某个作业需要100mb的内存空间，就为这个作业在内存中划分一个100mb的内存空间。
>
>##### 单一连续分配
>
>- 分配方法：将内存去划分为系统区域用户区，系统区为操作系统使用，剩下的用户区给**一个进程或作业**使用。
>- 特点：操作简单、没有外部碎片，适合单道处理系统。但是会有大量的内部碎片浪费资源，存储效率低。
>
>##### 固定分区分配
>
>- 分配方法：(1)分区大小相等：将内存的用户区分成大小相等的区域，每个进程只能申请一块区域；(2)分区大小不等：将内存的用户区分成大小不等的区域，分配原则是多个较小的区域、适量中等大小区域、少量的最大分区。每个进程根据大小只能申请一块区域。
>- 特点：固定分区分配虽然没有外部碎片，但是会造成大量的内部碎片。分区大小相等缺乏灵活性，大的进程可能放不进去；分区大小不等可能会造成大量的内部碎片，利用率极低。
>
>##### 动态分区分配
>
>- 分配方法：不会先划分内存区域，当进程进入内存的时候才会根据进程大小动态的为其建立分区，使分区大小刚好适合进程的需要。
>- 特点：在开始是很好的，进程一次按照顺序存入内存，但是运行久了以后随着进程的消亡，会出现很多成段的内存空间，时间越来越长就会导致很多不可利用的外部碎片，降低内存的利用率。这时需要分配算法来解决
>
>### 分配算法
>
>- **首次适应算法**：进程进入内存之后从头开始查找第一个适合自己大小的分区。空间分区就是按照地址递增的顺序排列。算法开销小，回收后放到原位置就好。综合看这个算法性能最好。
>- **最佳适应算法**：将分区从从小到大排列(容量递增)，找到最适合自己的分区，这样会有更大的分区被保留下来，满足别的进程需要。但是算法开销大，每次进程消亡产生新的区域后要重新排序。并且当使用多次后会产生很多外部碎片。
>- **最坏适应算法**：将分区从从大到小排列(容量递减)，进程每次都找最大的区域来使用。可以减少难以利用的外部碎片。但是大分区很快就被用完了，大的进程可能会有饥饿的现象。算法开销也比较大。
>- **邻近适应算法**：空间分区按照地址递增的顺序进行排列，是由首次适应演变而来，进程每次寻找空间，从上一次查找的地址以后开始查找(不同于首次适应，首次适应每次从开头查找)。算法开销小，大的分区也会很快被用完。
>- 注意：以上进程使用空间不会产生内部碎片，当进程大小为60mb的进程找到了一块100mb的空间，他只会使用60mb，剩下的40mb会给别的进程。
>
>----
>
>## 非连续分配
>
>可以将一个进程分散的装入内存分区。根据分区的大小是否固定可以分成分页存储管理(固定)与分段存储管理(不固定)，为了避免两者的缺点，还可以二者混用成段页式存储管理。再根据进程运行作业时是否将作业的的全部代码装入内存，又分为基本分页存储管理(全部装入内存)和请求分页存储管理(非一次全装入内存)。
>
>##### 基本分页存储管理
>
>主存空间划分为大小相等的块，块相对较小，作为主存的基本单元。每个进程也以块为单位划分，进程执行时，以块为单位申请内存空间
>
>##### 基本分段式存储管理
>
>分页存储是从计算机的角度设计的，目的是为了提高内存的利用率，提升计算机的性能。分段存储的提出是考虑到程序员和用户，以满足方编程、数据共享、信息保护、动态增长、动态链接的需要。
>
>##### 段页式管理方式
>
>- 背景：由于分段与分页各有利弊，页式存储提高内存利用率，段式存储反应程序逻辑结构有利于共享数据，所以可以结合二者来组成新的内存管理方式。
>- 概念：首先将进程根据逻辑结构划分成若干个逻辑段，每个段都有自己的段号，然后将这些段划分成若干个大小固定的页。这样对内存空间的管理依然和分页式管理相似，将内存分成和页面大小相同的存储块，对内存分配一存储块为单位。
>- 逻辑地址结构：逻辑地址结构由段号S(决定每个进程的段数)、页号P(决定每段的页数)、页内偏移量W(页面的大小和内存块的大小)组成。



## 分页与分段的区别

>1、页是信息的物理单位，分页的主要目的是为了实现离散分配，提高内存的利用率。段是信息的逻辑单位，分段的主要目的是更好地满足用户需求，一个段通常包含一组数语一个逻辑板块的信息。分段是用户可见的，用户编程时需要显示的给出段名。
>
>2、页的大小是固定的，系统绝决定；段的大小是不固定的，取决于系统程序
>
>3、分页的用户地址空间是一维的，程序员只需要给出一个记忆符就可以表示一个地址 分段存储管理的地址空间是二维的，程序员需要在标识一个地址的时候，既要给出段名，也要给出段内地址
>
>4、分段比分页更容易实现信息的共享和保护 注意：不能修改的代码称为纯代码(可重入代码)，这样的代码段不是临界资源，可以共享。可修改的代码是不可以共享的（比如由很多变量的代码段） 比如：生产者进程的一个进程段，是用来判断该缓冲区此时是否可以访问，这个时候消费者进程的段表项也可以指向这里 为什么分业管理不方便实现代码共享？ 因为将生产者进程分段，由于页面的空间有限，一段可能被装入多个空间，一个空间也可能有多个代码段被装进来，所以适合共享，达不到安全的效果
>
>5、访问一个逻辑地址需要几次访问内存？ 单级页表：1.查内存中的页表——2.访问目标内存单元 分段：1.查询内存中的段——2.访问目标内存单元 分段与分页系统相似，分段系统也可以引入快表机构，将近期访问过的段表放到快表中，这样可以少一次访问，加快地址变换速度
>
>6、分页：内存空间利用率高，不会产生外部碎片，只有少量的内部碎片；不方便按照逻辑模块实现信息的共享与保护 分段：方便按照逻辑模块实现信息的共享与保护；如果段太长，为其分配很大的存储空间很不方便，容易产生外部碎片.





## 虚拟内存的定义和特征

>**什么是虚拟内存：  基于局部性原理**： 
>
>1. 在程序装入时，可以将程序中很快会用到的部分装入内存，暂时用不到的部分留在外存，就可以让程序开始执行。
>2. 在程序执行过程中，当所访问的信息不在内存时，由操作系统将所需要的部分调入内存,然后继续执行程序。
>3.  若内存空间不够，由操作系统负责将内存中暂时不用到的信息换出到外存，从而腾出空间存放将要调入内存的信息。
>4. 在操作系统的管理下，用户看起来似乎有一个比实际内存大得多的内存。 实际上物理内存大小不变，只是在逻辑上进行了扩充。这就是虚拟内存技术
>
>**访问速度：** 寄存器---> 高速缓存 ---> 内存----> 外存（如磁盘、磁带）
>
>虚拟内存的**最大容量**是由计算机的地址结构（cpu寻址范围）确定的 `32位计算机   2^32==4GB`
>
>虚拟内存的**实际容量**=min(内存和外村容量之和，cpu寻址范围)
>
>-----
>
>**虚拟内存的特征：**
>
>1. 多次性： 允许将作业分成多次调入内存
>2. 对换性：无需将作业一直驻留在内存中，可以允许在作业过程中，将作业换入、换出。
>3. 虚拟性： 从逻辑上扩充了内存的容量，使用户看到的内存容量，远大于实际的容量。



## 虚拟内存的实现

>以往非连续内存的分配实现：
>
>1. 基于分页存储管理
>2. 基于分页存储管理
>3. 基于段页式存储管理
>
>虚拟内存技术，允许一个作业分多次调入到内存当中，如果采用连续分配方式，会不方便实现。因此，虚拟内存的实现需要在离散分配的内存管理方式基础上。
>
>在非连续内存管理上增加**两个额外**的功能：
>
>1. 当访问信息不在内存时，操作系统要提供请求调页或者请求调段的功能；
>2. 当内存空间不够时，操作系统要提供页面置换或者段置换的功能；
>
>----
>
>虚拟内存的实现：
>
>1. 请求分页存储管理   对比基本分页存储管理
>
>2. 请求分段存储管理
>
>3. 请求段页式存储管理
>
>   ### ==请求分页存储管理==

><img src="https://upload-images.jianshu.io/upload_images/18464438-38bfd5975b67da2c.png?imageMogr2/auto-orient/strip|imageView2/2/w/1018/format/webp" alt="img" style="zoom:80%;" />
>
>首先请求分页管理机制 需要跟 基本分页存储管理 主要区别就是：
>
>1. **程序执行过程中，允许一个作业分多次调入到内存当中，而不是一次性全部注入。所访问的信息不在内存时，操作系统负责将所需信息由外存调入内存，然后开始继续执行程序。**
>2. **若内存空间不够时，操作系统要实现页面置换功能，将内存中暂时不用到的信息换出到外存。**
>3. 为了实现 请求调页，操作系统需要知道每个页面是否已经调入到内存，如果还没有调入，那么也需要知道该页面在外存中存放的位置。
>4. 当内存空间不够时，操作系统要实现页面置换功能，并记录页面是否呗修改过。有的页面没有被修改过，就不用再浪费时间写回外存，有的页面修改过，就需要将外存中的旧数据进行覆盖。
>
>**页表机制**
>
><img src="https://upload-images.jianshu.io/upload_images/18464438-19f8228a0e7e3a08.png?imageMogr2/auto-orient/strip|imageView2/2/w/954/format/webp" alt="img" style="zoom:80%;" />
>
>---
>
>**缺页终端机构**
>
>1. 在访问操作系统中，每当要访问的页面不在内存时，便产生一个缺页中断，然后由操作系统的缺页中断处理程序来处理中断。此时缺页的进程阻塞，放入到阻塞队列，调页完成后再将其唤醒，放回就绪队列。
>2. 如果内存中有空闲块，则为进程分配一个空闲块，将所缺页面装入该块，并修改页面中相应的页表项。
>3. 如果内存中没有空闲块，则由页面置换算法选择一个页面进行淘汰，并判断该页面在内存期间被修改过，则要将其写回外存。未修改过的页面不用写回外存。
>
>-----
>
>**地址变换机构==请求分页式管理实现逻辑**
>
>1. 根据逻辑地址得到页号和页内偏移量。
>2.  对页号进行越界判断。
>3. 访问页表判断页是否在内存中，如果在继续执行程序。
>4. 如果访问的页不在内存中，产生缺页中断请求，需要请求调页。
>5. 判断内存空间是否已满，未满，将缺页调入内存，修改页表。如果已满，从内存中选择一个页面置换，如果页面发生修改，需要将修改写回外存，然后再将缺页调入内存，并修改页表。
>6. 页面调入后，阻塞的进程会处于就绪态等待处理机调度。



## 虚拟内存的作用

>1. 缓存。将内存视为一个存储在磁盘上的地址空间的高速缓存，在主存中只保存活动区域，并根据需要在磁盘和主存之间来回传送数据。
>2. 简化链接。独立地址空间允许每个进程的内存映像使用相同的基本格式。
>3. 简化内存分配。当运行在用户进程的程序要求额外的堆空间时，操作系统分配k个连续的虚拟内存页面，并且将它们映射到物理内存中任意位置的k个任意的物理页面。由于页表的存在，操作系统没必要分配k个连续的物理页面，页面可随机地分散在物理内存中。
>4. 内存保护。保护了每个进程的地址空间不被其他进程破坏。操作系统会控制进程对内存系统的访问，例如：
>             1）不允许一个用户进程修改它的只读代码段；
>           2）不允许用户进程读或修改任何内核中的代码和数据结构；
>           3）不允许用户进程读或写其他进程的私有内存；
>           4） 不允许用户进程修改任何其他进程共享的虚拟页表。





## 虚拟内存、物理内存、共享内存？

>物理内存就是主机的内存！虚拟内存就是硬盘上一块区域用于暂存与物理内存经常交换的数据(类似内存功能）！加内存的话当然会降低内存占用率
>
>共享内存有什么缺点？它怎么保证同步操作？https://blog.csdn.net/D_Guco/article/details/87905246
>
>**虚拟内存**：
>
>虚拟内存是一种实现在计算机软硬件之间的内存管理技术，它将程序使用到的内存地址(虚拟地址)映射到计算机内存中的物理地址，虚拟内存使得应用程序从繁琐的管理内存空间任务中解放出来，提高了内存隔离带来的安全性，虚拟内存地址通常是连续的地址空间，由操作系统的内存管理模块控制，在触发缺页中断时利用分页技术将实际的物理内存分配给虚拟内存，而且64位机器虚拟内存的空间大小远超出实际物理内存的大小，使得进程可以使用比物理内存大小更多的内存空间。
>
>**进程得到的这4G虚拟 32位操作系统 内存是一个连续的地址空间（这也只是进程认为），而实际上，它通常是被分隔成多个物理内存碎片，还有一部分存储在外部磁盘存储器上，在需要时进行数据交换。**
>
>**进程开始要访问一个地址，它可能会经历下面的过程：**
>
>1. 每次我要访问地址空间上的某一个地址，都需要把地址翻译为实际物理内存地址
>2. 所有进程共享这整一块物理内存，每个进程只把自己目前需要的虚拟地址空间映射到物理内存上
>3. 进程需要知道哪些地址空间上的数据在物理内存上，哪些不在（可能这部分存储在磁盘上），还有在物理内存上的哪里，这就需要通过页表来记
>4. 页表的每一个表项分两部分，第一部分记录此页是否在物理内存上，第二部分记录物理内存页的地址（如果在的话）
>5. 当进程访问某个虚拟地址的时候，就会先去看页表，如果发现对应的数据不在物理内存上，就会发生缺页异常
>6. 缺页异常的处理过程，操作系统立即阻塞该进程，并将硬盘里对应的页换入内存，然后使该进程就绪，如果内存已经满了，没有空地方了，那就找一个页覆盖，至于具体覆盖的哪个页，就需要看操作系统的页面置换算法是怎么设计的了。
>   
>
>---
>
>**物理内存：**
>
>**物理内存（Physical Memory）**是相对虚拟内存而言的，是指通过插在主板内存槽上的物理内存条而获得的内存空间。物理内存，即 **RAM（Random Access Memory，随机存取存储器）**，也叫主存（内存），是与 CPU 直接进行数据交互的内部存储器。它可以随时读写（刷新时除外），而且速度很快，主要作用是在计算机运行时为操作系统和各种程序提供临时储存。看计算机配置的时候，主要看的就是这个物理内存。
>
>-----
>
>**共享内存：**
>
>进程在运行过程中，会加载许多操作系统的动态库，比如 libc.so、libld.so等。这些库对于每个进程而言都是公用的，它们在内存中实际只会加载一份，这部分称为共享内存。如上图中的A4和B3部分即为共享内存，实际都映射到同一块物理内存。
>
>注意，进程占用的共享内存也是计算到驻留内存中的。









# ==---------------------------------------------------------------用户态和内核态==

## 用户态和内核态 区别

>1. 内核态：cpu可以访问内存的所有数据，包括外围设备，例如硬盘，网卡，cpu也可以将自己从一个程序切换到另一个程序。
>
>2. 用户态：**只能受限的访问内存**，且不允许访问外围设备，占用cpu的能力被剥夺，cpu资源可以被其他程序获取。



## 为什么要区别用户态和内核态

>为了安全性。在CPU的所有指令中，有一些指令是非常危险的，如果错用，将导致整个系统崩溃。**比如：清内存、设置时钟等**。如果所有的程序都能使用这些指令将会导致整个系统崩溃。分了内核态和用户态后，当用户需要操作这些指令时候，内核为其提供了API，可以通过系统调用陷入内核，让内核去执行这些操作。





## 用户态转换到内核态的三种方式

>**1）用户态切换到内核态的3种方式**
>1、系统调用
>
>这是用户进程主动要求切换到内核态的一种方式，用户进程通过系统调用申请操作系统提供的服务程序完成工作。而系统调用的机制其核心还是使用了操作系统为用户特别开放的一个中断来实现，例如Linux的ine 80h中断。
>
>2、异常
>
>当CPU在执行运行在用户态的程序时，发现了某些事件不可知的异常，这时会触发由当前运行进程切换到处理器。异常的内核相关程序中，也就到了内核态，比如缺页异常。
>
>3、外围设备的中断
>
>当外围设备完成用户请求的操作之后，会向CPU发出相应的中断信号，这时CPU会暂停执行下一条将要执行的指令，转而去执行中断信号的处理程序，如果先执行的指令是用户态下的程序，那么这个转换的过程自然也就发生了有用户态到内核态的切换。比如硬盘读写操作完成，系统会切换到硬盘读写的中断处理程序中执行后续操作等。
>
>-----
>
>**2）切换操作 原理**
>
>从出发方式看，可以在认为存在前述3种不同的类型，但是从最终实际完成由用户态到内核态的切换操作上来说，涉及的关键步骤是完全一样的，没有任何区别，都相当于执行了一个中断响应的过程，因为系统调用实际上最终是中断机制实现的，而异常和中断处理机制基本上是一样的，用户态切换到内核态的步骤主要包括：
>
>1、从当前进程的描述符中提取其内核栈的ss0及esp0信息。
>
>2、使用ss0和esp0指向的内核栈将当前进程的cs，eip，eflags，ss，esp信息保存起来，这个过程也完成了由用户栈找到内核栈的切换过程，同时保存了被暂停执行的程序的下一条指令。
>
>3、将先前由中断向量检索得到的中断处理程序的cs，eip信息装入相应的寄存器，开始执行中断处理程序，这时就转到了内核态的程序执行了。





# ==-----------------------------------------------------------------------------死锁==

>所谓死锁，是指多个进程在运行过程中因争夺资源而造成的一种僵局，当进程处于这种僵持状态时，若无外力作用，它们都将无法再向前推进。

## 产生死锁的原因？

>当进程需要以独占的方式访问资源时，可能会发生死锁（Deadlock）。死锁是指两个或以上进程因竞争临界资源而造成的一种僵局，即一个进程等待一个已经被占用且永不释放的资源。若无外力作用，这些进程都无法向前推进。
>
>1. **产生死锁的根本原因**是系统能够提供的资源个数比要求该资源的进程数要少。
>
>2. **产生死锁的基本原因**：进程推进顺序不合理。
>
>-----
>
>1. 资源竞争不合理：
>
>A有纸，B有笔
>
>A：你不给我笔，我就写不了作业
>
>B：你不给我纸，我就写不了作业
>
>彼此僵持不下……
>
>2. 多个程序同时运行时，进程推进顺序不合理。
>
>例子：
>
>A要前进2步，到桌子前，再后退2步。
>
>但如果执行顺序不合理：A先后退，就永远到不了桌子前，触发不了后续动作，就会死锁。

## 死锁产生的4个必要条件？

>产生死锁的必要条件：
>
>- 互斥条件
>
>  涉及的资源是非共享的，即一次只能有一个进程使用。如果有另一个进程申请该资源，那么申请进程必须等待，直到该资源被释放。
>
>- 不剥夺条件（非抢占）
>
>  进程所获得的资源在未使用完毕之前，不能被其他进程强行夺走，即只能由获得该资源的进程自行释放。
>
>- 请求与保持条件    占有并等待（部分分配）
>
>  在等待新资源的同时，进程继续占用已分配到的资源。
>
>- 循环等待    
>
>  存在一种进程首尾相接的循环链，链中每个进程都在等待下一个进程所持有的资源，造成这组进程处于永远等待状态。
>
>**注意：** 这四个条件是死锁的必要条件，只要系统发生死锁，这些条件必然成立。反之，上述条件只要有一个不满足，就不会发生死锁。所以要避免发生死锁，只需要破坏其必要条件。

## 解决死锁的基本方法

>##### 预防死锁
>
>1. **资源一次性分配：一次性分配所有资源，这样就不会再有请求了：（破坏请求条件）**
>2. **只要有一个资源得不到分配，也不给这个进程分配其他的资源：（破坏保持条件）**
>3. **可剥夺资源：即当某进程获得了部分资源，但得不到其它资源，则释放已占有的资源（破坏不可剥夺条件）**
>4. **资源有序分配法：系统给每类资源赋予一个编号，每一个进程按编号递增的顺序请求资源，释放则相反（破坏环路等待条件）**
>5. **超时放弃   不会永远等待下去。**
>
>当使用synchronized关键词提供的内置锁时，只要线程没有获得锁，那么就会永远等待下去，然而Lock接口提供了boolean tryLock(long time, TimeUnit unit) throws InterruptedException方法，该方法可以按照固定时长等待锁，因此线程可以在获取锁超时以后，主动释放之前已经获得的所有的锁。通过这种方式，也可以很有效地避免死锁。 

>- **避免死锁**
>
>1. 如果进程请求的资源会导致死锁，系统就拒绝启动该进程；
>
>2. 如果对一个资源的分配会导致下一步的死锁，系统就拒绝本次分配；
>
>显然要避免死锁，系统必须事先知道所拥有的资源数量及其属性。
>
>一个著名的避免死锁的算法是银行家算法。
>
>**所谓银行家算法，是指分配资源之前先确定资源分配是否会造成系统死锁。如果会死锁，则不分配，只有确认不会死锁后才进行分配。**
>
>银行家算法，需要按如下原则判断是否分配资源：
>
>- 新进程进入系统时，它必须说明对各类资源的最大需求量，这一数量不能超过系统的资源总数。只有满足这一条件系统才接纳该进程。
>
>- 当进程申请一组资源时，该算法需要检查进程对各类资源的最大需求量，如果系统现存的各类资源的数量可以满足此时的资源最大需求量时，就分配资源；否则进程必须等待，直到其他进程释放足够的资源为止。
>- 进程需要在一定时间内无条件地归还它所申请的全部资源。
>
>-----
>
>##### 检测死锁
>
>首先为每个进程和每个资源指定一个唯一的号码；
>然后建立资源分配表和进程等待表。
>
>----
>
>##### 解除死锁
>
>解除死锁的方法 包括资源剥夺法、进程撤销法、进程回退法、系统重启法等：
>
>- 资源剥夺法
>
>剥夺陷入死锁的进程所占用的资源，但并不撤销此进程，再将这些资源分配给需要的进程，直至死锁解除。
>
>- 进程撤销法
>
>一次性撤销陷入死锁的所有进程，回收所有占用的资源，等死锁解除后，再重新运行进程。逐个撤销陷入死锁的进程，依次回收其资源并重新分配，直至死锁解除。可以优先撤销优先级低、预计剩余执行时间最长、CPU消耗时间少的进程。
>
>- 进程回退法
>
>让所有的进程回退到系统保存的检查点，这种方法要求系统建立并保存检查点、建立回退机制。
>
>- 系统重启法
>
>  结束所有进程并重启系统。这种方法很简单，但损失很大，先前的工作可能都浪费了。



## 哲学家进餐

>有五个哲学家，他们的生活方式是交替地进行思考和进餐。他们共用一张圆桌，分别坐在五张椅子上。
>
>在圆桌上有五个碗和五支筷子，平时一个哲学家进行思考，饥饿时便试图取用其左、右最靠近他的筷子，只有在他拿到两支筷子时才能进餐。进餐完毕，放下筷子又继续思考。
>
>此算法可以保证不会有相邻的两位哲学家同时进餐。
>
>若五位哲学家同时饥饿而各自拿起了左边的筷子，这使五个信号量 chopstick 均为 0，当他们试图去拿起右边的筷子时，都将因无筷子而无限期地等待下去，即可能会引起死锁。
>
>**哲学家进餐问题的改进解法**
>**方法一：至多只允许四位哲学家同时去拿左筷子，最终能保证至少有一位哲学家能进餐，并在用完后释放两只筷子供他人使用。**
>**方法二：仅当哲学家的左右手筷子都拿起时才允许进餐。**
>**方法三：规定奇数号哲学家先拿左筷子再拿右筷子，而偶数号哲学家相反。**
>
>1. 方法一
>
>   至多只允许四位哲学家同时去拿左筷子，最终能保证至少有一位哲学家能进餐，并在用完后释放两只筷子供他人使用。
>
>设置一个初值为 4 的信号量 r，只允许 4 个哲学家同时去拿左筷子，这样就能保证至少有一个哲学家可以就餐，不会出现饿死和死锁的现象。
>
>原理：至多只允许四个哲学家同时进餐，以保证至少有一个哲学家能够进餐，最终总会释放出他所使用过的两支筷子，从而可使更多的哲学家进餐。
>
>
>
>2. 方法二
>仅当哲学家的左右手筷子都拿起时才允许进餐。
>
>解法 1：利用 AND 型信号量机制实现。
>
>原理：多个临界资源，要么全部分配，要么一个都不分配，因此不会出现死锁的情形。但是会保持饥饿状态。
>
>
>
>解法 2：利用信号量的保护机制实现。
>
>原理：通过互斥信号量 mutex 对 eat() 之前取左侧和右侧筷子的操作进行保护，可以防止死锁的出现。
>
>
>
>3. 方法三
>     规定奇数号哲学家先拿左筷子再拿右筷子，而偶数号哲学家相反。
>
>原理：按照下图，将是 2,3 号哲学家竞争 3 号筷子，4,5 号哲学家竞争 5 号筷子。1 号哲学家不需要竞争。最后总会有一个哲学家能获得两支筷子而进餐。
>
>
>
>原文链接：https://blog.csdn.net/qq_28602957/article/details/53538329



## 银行家算法

>一句话：
>
>银行家算法的实质就是**要设法保证系统动态分配资源后不进入不安全状态，以避免可能产生的死锁。**即没当进程提出资源请求且系统的资源能够满足该请求时，系统将判断满足此次资源请求后系统状态是否安全，如果判断结果为安全，则给该进程分配资源，否则不分配资源，申请资源的进程将阻塞。
>
>**银行家算法的执行有个前提条件，即要求进程预先提出自己的最大资源请求，并假设系统拥有固定的资源总量。**
>
>那么此时会有一个问题，如何判断系统是否处于安全状态？算法流程将用下面一张图来表示。
>
>一张图
>
>![img](https://img-blog.csdn.net/20180508204335770?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMzNDE0Mjcx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
>
>首先是银行家算法中的进程：
>
>1. 包含进程Pi的需求资源数量（也是最大需求资源数量，MAX）
>2. 已分配给该进程的资源A（Allocation）
>3. 还需要的资源数量N（Need=M-A）
>
>Available为空闲资源数量，即资源池（注意：资源池的剩余资源数量+已分配给所有进程的资源数量=系统中的资源总量）
>
>假设资源P1申请资源，银行家算法先试探的分配给它（当然先要看看当前资源池中的资源数量够不够），若申请的资源数量小于等于Available，然后接着判断分配给P1后剩余的资源，能不能使进程队列的某个进程执行完毕，若没有进程可执行完毕，则系统处于不安全状态（即此时没有一个进程能够完成并释放资源，随时间推移，系统终将处于死锁状态）。
>
>若有进程可执行完毕，则假设回收已分配给它的资源（剩余资源数量增加），把这个进程标记为可完成，并继续判断队列中的其它进程，若所有进程都可执行完毕，则系统处于安全状态，并根据可完成进程的分配顺序生成安全序列（如{P0，P3，P2，P1}表示将申请后的剩余资源Work先分配给P0–>回收（Work+已分配给P0的A0=Work）–>分配给P3–>回收（Work+A3=Work）–>分配给P2–>······满足所有进程）。



# ==--------------------------------------------------------------------------调度算法==

>https://www.cnblogs.com/xiaolincoding/p/13631224.html

>![本文提纲](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%B0%83%E5%BA%A6%E7%AE%97%E6%B3%95/%E8%B0%83%E5%BA%A6%E7%AE%97%E6%B3%95%E6%8F%90%E7%BA%B2.png)





## 进程调度算法

>进程调度算法也称 CPU 调度算法，毕竟进程是由 CPU 调度的。
>
>当 CPU 空闲时，操作系统就选择内存中的某个「就绪状态」的进程，并给其分配 CPU。
>
>什么时候会发生 CPU 调度呢？通常有以下情况：
>
>1. 当进程从运行状态转到等待状态；
>2. 当进程从运行状态转到就绪状态；
>3. 当进程从等待状态转到就绪状态；
>4. 当进程从运行状态转到终止状态；
>
>其中发生在 1 和 4 两种情况下的调度称为「非抢占式调度」，2 和 3 两种情况下发生的调度称为「抢占式调度」。
>
>非抢占式的意思就是，当进程正在运行时，它就会一直运行，直到该进程完成或发生某个事件而被阻塞时，才会把 CPU 让给其他进程。
>
>**而抢占式调度，顾名思义就是进程正在运行的时，可以被打断，使其把 CPU 让给其他进程。那抢占的原则一般有三种，分别是时间片原则、优先权原则、短作业优先原则。**
>
>------
>
>1. **先来先服务（\*First Come First Severd, FCFS\*）算法**
>
>非抢占式： **每次从就绪队列选择最先进入队列的进程，然后一直运行，直到进程退出或被阻塞，才会继续从队列中选择第一个进程接着运行。**这似乎很公平，但是当一个长作业先运行了，那么后面的短作业等待的时间就会很长，不利于短作业。
>
>---
>
>2. **最短作业优先（\*Shortest Job First, SJF\*）调度算法**
>
>它会**优先选择运行时间最短的进程来运行**，这有助于提高系统的吞吐量。这显然对长作业不利，很容易造成一种极端现象。
>
>----
>
>3. **高响应比优先 （\*Highest Response Ratio Next, HRRN\*）调度算法**
>
>主要是权衡了短作业和长作业。
>
>**每次进行进程调度时，先计算「响应比优先级」，然后把「响应比优先级」最高的进程投入运行**，「响应比优先级」的计算公式：
>
><img src="https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BF%9B%E7%A8%8B%E5%92%8C%E7%BA%BF%E7%A8%8B/26-%E5%93%8D%E5%BA%94%E6%AF%94%E5%85%AC%E5%BC%8F.jpg" alt="img" style="zoom:67%;" />
>
>从上面的公式，可以发现：
>
>- 如果两个进程的「等待时间」相同时，「要求的服务时间」越短，「响应比」就越高，这样短作业的进程容易被选中运行；
>- 如果两个进程「要求的服务时间」相同时，「等待时间」越长，「响应比」就越高，这就兼顾到了长作业进程，因为进程的响应比可以随时间等待的增加而提高，当其等待时间足够长时，其响应比便可以升到很高，从而获得运行的机会；
>
>-----
>
>4. **时间片轮转（\*Round Robin, RR\*）调度算法**。
>
><img src="https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BF%9B%E7%A8%8B%E5%92%8C%E7%BA%BF%E7%A8%8B/27-%E6%97%B6%E9%97%B4%E7%89%87%E8%BD%AE%E8%AF%A2.jpg" alt="RR 调度算法" style="zoom:67%;" />
>
>**每个进程被分配一个时间段，称为时间片（\*Quantum\*），即允许该进程在该时间段中运行。**
>
>- 如果时间片用完，进程还在运行，那么将会把此进程从 CPU 释放出来，并把 CPU 分配另外一个进程；并且回到队尾
>- 如果该进程在时间片结束前阻塞或结束，则 CPU 立即进行切换；
>
>另外，时间片的长度就是一个很关键的点：
>
>- 如果时间片设得太短会导致过多的进程上下文切换，降低了 CPU 效率；
>- 如果设得太长又可能引起对短作业进程的响应时间变长。将
>
>通常时间片设为 `20ms~50ms` 通常是一个比较合理的折中值。
>
>-------
>
>5. **最高优先级（\*Highest Priority First，HPF\*）调度算法**。
>
>它们希望调度是有优先级的，即希望调度程序能**从就绪队列中选择最高优先级的进程进行运行，这称为最高优先级（\*Highest Priority First，HPF\*）调度算法**。
>
>进程的优先级可以分为，静态优先级或动态优先级：
>
>- 静态优先级：创建进程时候， PCB已经确定了优先级了，然后整个运行时间优先级都不会变化；
>- 动态优先级：根据进程的动态变化调整优先级，比如如果进程运行时间增加，则降低其优先级，如果进程等待时间（就绪队列的等待时间）增加，则升高其优先级，也就是**随着时间的推移增加等待进程的优先级**。
>
>该算法也有两种处理优先级高的方法，非抢占式和抢占式：
>
>- 非抢占式：当就绪队列中出现优先级高的进程，运行完当前进程，再选择优先级高的进程。
>- 抢占式：当就绪队列中出现优先级高的进程，当前进程挂起，调度优先级高的进程运行。
>
>**但是依然有缺点，可能会导致低优先级的进程永远不会运行。**
>
>------
>
>6. **多级反馈队列（\*Multilevel Feedback Queue\*）调度算法**
>
>**多级反馈队列（\*Multilevel Feedback Queue\*）调度算法**是「时间片轮转算法」和「最高优先级算法」的综合和发展。
>
>顾名思义：
>
>- **「多级」表示有多个队列，每个队列优先级从高到低，同时优先级越高时间片越短。**
>- 「**反馈」表示如果有新的进程加入优先级高的队列时，立刻停止当前正在运行的进程，转而去运行优先级高的队列；**
>
><img src="https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BF%9B%E7%A8%8B%E5%92%8C%E7%BA%BF%E7%A8%8B/28-%E5%A4%9A%E7%BA%A7%E9%98%9F%E5%88%97.jpg" alt="多级反馈队列" style="zoom:67%;" />
>
>来看看，它是如何工作的：
>
>- 设置了多个队列，赋予每个队列不同的优先级，每个**队列优先级从高到低**，同时**优先级越高时间片越短**；
>- 新的进程会被放入到第一级队列的末尾，按先来先服务的原则排队等待被调度，如果在第一级队列规定的时间片没运行完成，则将其转入到第二级队列的末尾，以此类推，直至完成；
>- 当较高优先级的队列为空，才调度较低优先级的队列中的进程运行。如果进程运行时，有新进程进入较高优先级的队列，则停止当前运行的进程并将其移入到原队列末尾，接着让较高优先级的进程运行；
>
>可以发现，对于短作业可能可以在第一级队列很快被处理完。对于长作业，如果在第一级队列处理不完，可以移入下次队列等待被执行，虽然等待的时间变长了，但是运行时间也会更长了，所以该算法很好的**兼顾了长短作业，同时有较好的响应时间。**



## Linux进程调度算法

>Linux在进行进程调度的时候把进程分为两种：1.普通进程；2.实时进程
>
>实时进程的优先级永远比普通进程的优先级高，也就是说实时进程只要来了就可以抢占普通进程，而且还抓住处理器就不撒手，直到所有的实时进程都执行完毕，才会把处理器让出来给普通进程使用
>
>之前也说了，普通进程的调度采用的是完全公平调度（CFS）对应的是SCHED_NORMAL
>
>而实时进程采用的调度方法就比较简单粗暴了，Linux提供了两种实时调度策略：SCHED_FIFO和SCHED_RR。
>
>SCHED_FIFO：简单的先入先出的调度算法，不使用时间片，只可能被更高优先级的FIFO或者SCHED_RR抢占
>
>SCHED_RR：时间片轮转的方式，优先级比SCHED_FIFO还要高，可以抢占SCHED_FIFO
>
>实时进程的调度没有实时优先级这一说法，采用的是静态优先级，一开始定好优先级之后就不会改变了。
>
>
>------
>
>**CFS  完全公平调度算法：**
>
> 我们把主要进程分为两种：1.I/O消耗型进程；2.处理器消耗型进程       当然也有既是I/O消耗型也是处理器消耗型的进程
>
>I/O消耗型进程如字面意思一样，轮到它的时候会把大部分时间消耗在I/O请求和等待I/O上，真正使用CPU的时间很少，处理器消耗性进程会把大部分时间用在使用CPU进行计算之类的，如果给这两种继承分配的时间片长度相等，就会体现出不公平。
>
>同时我们想给处理器消耗型的进程多一些处理器时间，而给I/O消耗性进程少一些处理器时间，于是linux采取的不是简单的时间片调度算法，而是改进的优先级调度算法CFS
>
>就是说每个进程**真正使用cpu的时间**是一样的，包括I/O消耗型和处理器消耗型，以达到真正的公平，这就解释了刚才的问题，CPU使用比低的占用时间会不可避免的少于占用比高的进程，那我们只好让这个进程具有抢占能力，一就绪就可以抢占，这样子“**看起来**CPU使用比高了”（其实没变）“**看起来**CPU占用时间也和其他进程一样多了”（其实不多）
>
>
>
>- **Linux对普通进程采用的是完全公平调度算法（CFS）**
>- Linux的进程调度并未使用直接均分时间片的方式，而是对优先级进行了改进，**采用了两种不同的优先级范围，一种是nice值，范围是-20到+19，越大的nice值意味着更低的优先级，低nice值的进程会获得更多的处理器时间（按比例获得），第二种范围是实时优先级，其值是可配置的，默认情况下它的变化范围是从0到99，与nice值意义相反，越高的实时优先级数值意味着进程优先级越高，任何实时进程的优先级都高于普通进程**



## 页面置换算法    

>页面置换算法的功能是，**当出现缺页异常，需调入新页面而内存已满时，选择被置换的物理页面**，也就是说选择一个物理页面换出到磁盘，然后把需要访问的页面换入到物理页。
>
>那其算法目标则是，尽可能减少页面的换入换出的次数，常见的页面置换算法有如下几种：
>
>- 最佳页面置换算法（*OPT*）
>- 先进先出置换算法（*FIFO*）
>- 最近最久未使用的置换算法（*LRU*）
>- 时钟页面置换算法（*Lock*）
>- 最不常用置换算法（*LFU*）
>
>------

>**1. 最佳页面置换算法（*OPT*）**
>
>最佳页面置换算法基本思路是，**置换在「未来」最长时间不访问的页面**。
>
>所以，该算法实现需要计算内存中每个逻辑页面的「下一次」访问时间，然后比较，选择未来最长时间不访问的页面。

----

>**2.先进先出置换算法（*FIFO*）**
>
>预知页面在下一次访问前所需的等待时间，那我们可以**选择在内存驻留时间很长的页面进行中置换**，这个就是「先进先出置换」算法的思想。跟最佳页面置换算法比较起来，性能明显差了很多。
>
>实现：使用一个队列，新加入的页面放入队尾，每次淘汰队首的页面，即最先进入的数据，最先被淘汰。
>
>弊端：无法体现页面冷热信息

----

>**3.最近最久未使用的置换算法（*LRU*）**
>
>最近最久未使用（*LRU*）的置换算法的基本思路是，发生缺页时，**选择最长时间没有被访问的页面进行置换**，也就是说，该算法假设已经很久没有使用的页面很有可能在未来较长的一段时间内仍然不会被使用。
>
>这种算法近似最优置换算法，最优置换算法是通过「未来」的使用情况来推测要淘汰的页面，而 LRU 则是通过「历史」的使用情况来推测要淘汰的页面。
>
>**为了完全实现 LRU，需要在内存中维护一个所有页面的链表，最近最多使用的页面在表头，最近最少使用的页面在表尾。**
>
>**困难的是，在每次访问内存时都必须要更新「整个链表」。在链表中找到一个页面，删除它，然后把它移动到表头是一个非常费时的操作。性能开销比较大**

----

>4. **最不常用（*LFU*）算法**
>
>但是它的意思不是指这个算法不常用，而是**当发生缺页中断时，选择「访问次数」最少的那个页面，并将其淘汰**。 页表中有访问标志位。
>
>它的实现方式是，对每个页面设置一个「访问计数器」，每当一个页面被访问时，该页面的访问计数器就累加 1。在发生缺页中断时，淘汰计数器值最小的那个页面。
>
>缺点：
>
>1. 要增加一个计数器来实现，这个硬件成本是比较高的，另外如果要对这个计数器查找哪个页面访问次数最小，查找链表本身，如果链表长度很大，是非常耗时的，效率不高。
>2. LFU 算法只考虑了频率问题，没考虑时间的问题

-----

>**5.时钟页面置换算法（*Lock*）**
>
>它跟 LRU 近似，又是对 FIFO 的一种改进。
>
>该算法的思路是，把所有的页面都保存在一个类似钟面的「环形链表」中，一个表针指向最老的页面。
>
>当发生缺页中断时，算法首先检查表针指向的页面：



## 磁盘调度算法

>磁盘调度算法的目的很简单，就是为了提高磁盘的访问性能，一般是通过优化磁盘的访问请求顺序来做到的。都需要看磁头的初始位置是 53：
>
>1. **先来先服务**
>
>先来先服务（*First-Come，First-Served，FCFS*），顾名思义，先到来的请求，先被服务。
>
>缺点：比较简单粗暴，但是如果大量进程竞争使用磁盘，请求访问的磁道可能会很分散，那先来先服务算法在性能上就会显得很差，因为寻道时间过长。

------

>2. **最短寻道时间优先**
>
>最短寻道时间优先（*Shortest Seek First，SSF*）算法的工作方式是，优先选择从当前磁头位置所需寻道时间最短的请求
>
><img src="https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%B0%83%E5%BA%A6%E7%AE%97%E6%B3%95/%E7%A3%81%E7%9B%98%E8%B0%83%E5%BA%A6-%E6%9C%80%E7%9F%AD%E5%AF%BB%E9%81%93%E6%97%B6%E9%97%B4%E4%BC%98%E5%85%88.png" alt="最短寻道时间优先" style="zoom:30%;" />
>
>缺点：但这个算法可能存在某些请求的**饥饿**，距离较远的磁头位置可能产生饥饿，后续进程请求都小于当前这个最远磁头距离

------

>3. **扫描算法**
>
>最短寻道时间优先算法会产生饥饿的原因在于：磁头有可能再一个小区域内来回得移动。
>
>为了防止这个问题，可以规定：**磁头在一个方向上移动，访问所有未完成的请求，直到磁头到达该方向上的最后的磁道，才调换方向，这就是扫描算法**。
>
><img src="https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%B0%83%E5%BA%A6%E7%AE%97%E6%B3%95/%E7%A3%81%E7%9B%98%E8%B0%83%E5%BA%A6-%E6%89%AB%E6%8F%8F%E7%AE%97%E6%B3%95.png" alt="扫描算法" style="zoom:33%;" />
>
>扫描调度算法性能较好，不会产生饥饿现象，但是存在这样的问题，中间部分的磁道会比较占便宜，中间部分相比其他部分响应的频率会比较多，也就是说每个磁道的响应频率存在差异。

-----

>4. **循环扫描算法**
>
>扫描算法使得每个磁道响应的频率存在差异，那么要优化这个问题的话，可以总是按相同的方向进行扫描，使得每个磁道的响应频率基本一致。
>
>循环扫描规定：只有磁头朝某个特定方向移动时，才处理磁道访问请求，而返回时直接快速移动至最靠边缘的磁道，也就是复位磁头，这个过程是很快的，并且**返回中途不处理任何请求**，该算法的特点，就是**磁道只响应一个方向上的请求**。 循环扫描算法相比于扫描算法，对于各个位置磁道响应频率相对比较平均。
>
><img src="https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%B0%83%E5%BA%A6%E7%AE%97%E6%B3%95/%E7%A3%81%E7%9B%98%E8%B0%83%E5%BA%A6-C-SCAN%E7%AE%97%E6%B3%95.png" alt="循环扫描算法" style="zoom:33%;" />

-----

>5. #### LOOK 与 C-LOOK算法
>
>look与c-look是扫描算法和循环扫描算法的改进  。磁头在每个方向上仅仅移动到最远的请求位置，然后立即反向移动，而不需要移动到磁盘的最始端或最末端，**反向移动的途中会响应请求**。  重点就是不会移动到最开始端或者末端。







# ==深入剖析Linux IO原理和几种零拷贝机制的实现==

https://zhuanlan.zhihu.com/p/83398714    这篇文章非常

>用户态直接 I/O 使得应用进程或运行在用户态（user space）下的库函数直接访问硬件设备，数据直接跨过内核进行传输，内核在数据传输过程除了进行必要的虚拟存储配置工作之外，不参与任何其他工作，这种方式能够直接绕过内核，极大提高了性能。
>
>![img](https://pic1.zhimg.com/80/v2-4cb0f465ebeb7ff0f5e31e8d3f790c80_1440w.jpg)
>
>用户态直接 I/O 只能适用于不需要内核缓冲区处理的应用程序，这些应用程序通常在进程地址空间有自己的数据缓存机制，称为自缓存应用程序，如数据库管理系统就是一个代表。其次，这种零拷贝机制会直接操作磁盘 I/O，由于 CPU 和磁盘 I/O 之间的执行时间差距，会造成大量资源的浪费，解决方案是配合异步 I/O 使用。
>
>### 6.2. mmap + write
>
>一种零拷贝方式是使用 mmap + write 代替原来的 read + write 方式，减少了 1 次 CPU 拷贝操作。mmap 是 Linux 提供的一种内存映射文件方法，即将一个进程的地址空间中的一段虚拟地址映射到磁盘文件地址，mmap + write 的伪代码如下：
>
>```cpp
>tmp_buf = mmap(file_fd, len);
>write(socket_fd, tmp_buf, len);
>```
>
>使用 mmap 的目的是将内核中读缓冲区（read buffer）的地址与用户空间的缓冲区（user buffer）进行映射，从而实现内核缓冲区与应用程序内存的共享，省去了将数据从内核读缓冲区（read buffer）拷贝到用户缓冲区（user buffer）的过程，然而内核读缓冲区（read buffer）仍需将数据到内核写缓冲区（socket buffer），大致的流程如下图所示：
>
>![img](https://pic2.zhimg.com/80/v2-28463616753963ac9f189ce23a485e2d_1440w.jpg)
>
>基于 mmap + write 系统调用的零拷贝方式，整个拷贝过程会发生 4 次上下文切换，1 次 CPU 拷贝和 2 次 DMA 拷贝，用户程序读写数据的流程如下：
>
>1. 用户进程通过 mmap() 函数向内核（kernel）发起系统调用，上下文从用户态（user space）切换为内核态（kernel space）。
>2. 将用户进程的内核空间的读缓冲区（read buffer）与用户空间的缓存区（user buffer）进行内存地址映射。
>3. CPU利用DMA控制器将数据从主存或硬盘拷贝到内核空间（kernel space）的读缓冲区（read buffer）。
>4. 上下文从内核态（kernel space）切换回用户态（user space），mmap 系统调用执行返回。
>5. 用户进程通过 write() 函数向内核（kernel）发起系统调用，上下文从用户态（user space）切换为内核态（kernel space）。
>6. CPU将读缓冲区（read buffer）中的数据拷贝到的网络缓冲区（socket buffer）。
>7. CPU利用DMA控制器将数据从网络缓冲区（socket buffer）拷贝到网卡进行数据传输。
>8. 上下文从内核态（kernel space）切换回用户态（user space），write 系统调用执行返回。
>
>mmap 主要的用处是提高 I/O 性能，特别是针对大文件。对于小文件，内存映射文件反而会导致碎片空间的浪费，因为内存映射总是要对齐页边界，最小单位是 4 KB，一个 5 KB 的文件将会映射占用 8 KB 内存，也就会浪费 3 KB 内存。
>
>mmap 的拷贝虽然减少了 1 次拷贝，提升了效率，但也存在一些隐藏的问题。当 mmap 一个文件时，如果这个文件被另一个进程所截获，那么 write 系统调用会因为访问非法地址被 SIGBUS 信号终止，SIGBUS 默认会杀死进程并产生一个 coredump，服务器可能因此被终止。
>
>### 6.3. sendfile
>
>sendfile 系统调用在 Linux 内核版本 2.1 中被引入，目的是简化通过网络在两个通道之间进行的数据传输过程。sendfile 系统调用的引入，不仅减少了 CPU 拷贝的次数，还减少了上下文切换的次数，它的伪代码如下：
>
>```cpp
>sendfile(socket_fd, file_fd, len);
>```
>
>通过 sendfile 系统调用，数据可以直接在内核空间内部进行 I/O 传输，从而省去了数据在用户空间和内核空间之间的来回拷贝。与 mmap 内存映射方式不同的是， sendfile 调用中 I/O 数据对用户空间是完全不可见的。也就是说，这是一次完全意义上的数据传输过程。
>
>![img](https://pic3.zhimg.com/80/v2-48132735369375701f3d8ac1d6029c2a_1440w.jpg)
>
>基于 sendfile 系统调用的零拷贝方式，整个拷贝过程会发生 2 次上下文切换，1 次 CPU 拷贝和 2 次 DMA 拷贝，用户程序读写数据的流程如下：
>
>1. 用户进程通过 sendfile() 函数向内核（kernel）发起系统调用，上下文从用户态（user space）切换为内核态（kernel space）。
>2. CPU 利用 DMA 控制器将数据从主存或硬盘拷贝到内核空间（kernel space）的读缓冲区（read buffer）。
>3. CPU 将读缓冲区（read buffer）中的数据拷贝到的网络缓冲区（socket buffer）。
>4. CPU 利用 DMA 控制器将数据从网络缓冲区（socket buffer）拷贝到网卡进行数据传输。
>5. 上下文从内核态（kernel space）切换回用户态（user space），sendfile 系统调用执行返回。
>
>相比较于 mmap 内存映射的方式，sendfile 少了 2 次上下文切换，但是仍然有 1 次 CPU 拷贝操作。sendfile 存在的问题是用户程序不能对数据进行修改，而只是单纯地完成了一次数据传输过程。
>
>### 6.4. sendfile + DMA gather copy
>
>Linux 2.4 版本的内核对 sendfile 系统调用进行修改，为 DMA 拷贝引入了 gather 操作。它将内核空间（kernel space）的读缓冲区（read buffer）中对应的数据描述信息（内存地址、地址偏移量）记录到相应的网络缓冲区（ socket buffer）中，由 DMA 根据内存地址、地址偏移量将数据批量地从读缓冲区（read buffer）拷贝到网卡设备中，这样就省去了内核空间中仅剩的 1 次 CPU 拷贝操作，sendfile 的伪代码如下：
>
>```cpp
>sendfile(socket_fd, file_fd, len);
>```
>
>在硬件的支持下，sendfile 拷贝方式不再从内核缓冲区的数据拷贝到 socket 缓冲区，取而代之的仅仅是缓冲区文件描述符和数据长度的拷贝，这样 DMA 引擎直接利用 gather 操作将页缓存中数据打包发送到网络中即可，本质就是和虚拟内存映射的思路类似。
>
>![img](https://pic4.zhimg.com/80/v2-15edf2971101883e2a90253225a3b0d3_1440w.jpg)
>
>基于 sendfile + DMA gather copy 系统调用的零拷贝方式，整个拷贝过程会发生 2 次上下文切换、0 次 CPU 拷贝以及 2 次 DMA 拷贝，用户程序读写数据的流程如下：
>
>1. 用户进程通过 sendfile() 函数向内核（kernel）发起系统调用，上下文从用户态（user space）切换为内核态（kernel space）。
>2. CPU 利用 DMA 控制器将数据从主存或硬盘拷贝到内核空间（kernel space）的读缓冲区（read buffer）。
>3. CPU 把读缓冲区（read buffer）的文件描述符（file descriptor）和数据长度拷贝到网络缓冲区（socket buffer）。
>4. 基于已拷贝的文件描述符（file descriptor）和数据长度，CPU 利用 DMA 控制器的 gather/scatter 操作直接批量地将数据从内核的读缓冲区（read buffer）拷贝到网卡进行数据传输。
>5. 上下文从内核态（kernel space）切换回用户态（user space），sendfile 系统调用执行返回。
>
>sendfile + DMA gather copy 拷贝方式同样存在用户程序不能对数据进行修改的问题，而且本身需要硬件的支持，它只适用于将数据从文件拷贝到 socket 套接字上的传输过程。





# ==操作系统场景提==

## frok一个子进程 如何分配资源

>fork（）会产生一个和父进程完全相同的子进程，但子进程在此后多会exec系统调用，出于效率考虑，linux中引入了“**写时复制“**技术，也就是**只有进程空间的各段的内容要发生变化时，才会将父进程的内容复制一份给子进程**。**在fork之后exec之前两个进程用的是相同的物理空间（内存区），子进程的代码段、数据段、堆栈都是指向父进程的物理空间**，也就是说，**两者的虚拟空间不同，但其对应的物理空间是同一个**。
>
>当父子进程中有更改相应段的行为发生时，再为子进程相应的段分配物理空间，如果不是因为exec，内核会给子进程的数据段、堆栈段分配相应的物理空间（至此两者有各自的进程空间，互不影响），而代码段继续共享父进程的物理空间（两者的代码完全相同）。而如果是因为exec，由于两者执行的代码不同，子进程的代码段也会分配单独的物理空间。
>
>------
>
>fork时子进程获得父进程数据空间、堆和栈的复制，所以变量的地址（当然是虚拟地址）也是一样的。
>
>**每个进程都有自己的虚拟地址空间，不同进程的相同的虚拟地址显然可以对应不同的物理地址。因此地址相同（虚拟地址）而值不同没什么奇怪。**
>
>
>**“写时复制”的具体过程是这样的：**
>
>fork子进程完全复制父进程的栈空间，也复制了页表，但没有复制物理页面，所以这时虚拟地址相同，物理地址也相同，但是会把父子共享的页面标记为“只读”（类似mmap的private的方式）
>
>如果父子进程一直对这个页面是同一个页面，直到其中任何一个进程要对共享的页面“写操作”，这时内核会复制一个物理页面给这个进程使用，同时修改页表。而把原来的只读页面标记为“可写”，留给另外一个进程使用。
>
>这就是所谓的“写时复制”。
>
> 
>
> 
>
>**正因为fork采用了这种写时复制的机制，所以fork出来子进程之后，父子进程哪个先调度呢？**
>
>- 内核一般会先调度子进程，因为很多情况下子进程是要马上执行exec，会清空栈、堆。这些和父进程共享的空间，加载新的代码段。这就避免了“写时复制”拷贝共享页面的机会。
>- 如果父进程先调度很可能写共享页面，会产生“写时复制”的无用功。所以，一般是子进程先调度。
>
> 
>
>假定父进程malloc的指针指向0x12345678, fork 后，子进程中的指针也是指向0x12345678，但是这两个地址都是虚拟内存地址 （virtual memory)，经过内存地址转换后所对应的 物理地址是不一样的。所以两个进城中的这两个地址相互之间没有任何关系。
>
>
>**（注：但实际上，linux为了提高 fork 的效率，采用了 copy-on-write 技术，fork后，这两个虚拟地址实际上指向相同的物理地址（内存页），只有任何一个进程试图修改这个虚拟地址里的内容前，两个虚拟地址才会指向不同的物理地址（新的物理地址的内容从原物理地址中复制得到））**





## 为什么要有L1 L2 L3三级缓存

>由于数据的局限性，CPU往往需要在短时间内重复多次读取数据，**内存的运行频率自然是远远跟不上CPU的处理速度的**，怎么办呢？缓存的重要性就凸显出来了，CPU可以避开内存在缓存里读取到想要的数据，称之为命中（hit）。L1的运行速度很快，但是它的数据容量很小，CPU能在L1里命中的概率大概在80%左右——日常使用的情况下；L2、L3的机制也类似如此，这样一来，CPU需要在内存中读取的数据大概为5%-10%，其余数据命中全部可以在L1、L2、L3中做到，大大减少了系统的响应时间，总的来说，所有CPU读取数据的顺序都是先缓存再内存。
>
>**CPU的设计理念之一，加速大概率事件。**





# ==----------------------------------------------------------------------------linux==

# linux常用命令

>（tail，less，more，ls，top，ps...）

>### [1：文件管理](https://www.linuxcool.com/category/file)
>
>- [ls命令 – 显示指定工作目录下的内容及属性信息](https://www.linuxcool.com/ls)
>- [cp命令 – 复制文件或目录](https://www.linuxcool.com/cp)
>- [mkdir命令 – 创建目录](https://www.linuxcool.com/mkdir)
>- [mv命令 – 移动或改名文件](https://www.linuxcool.com/mv)
>- [pwd命令 – 显示当前路径](https://www.linuxcool.com/pwd)
>
>### [2：文档编辑](https://www.linuxcool.com/category/document)
>
>- [cat命令 – 在终端设备上显示文件内容](https://www.linuxcool.com/cat)
>- [echo命令 – 输出字符串或提取Shell变量的值](https://www.linuxcool.com/echo)
>- [rm命令 – 移除文件或目录](https://www.linuxcool.com/rm)
>- [tail命令 – 查看文件尾部内容](https://www.linuxcool.com/tail)
>- [rmdir命令 – 删除空目录](https://www.linuxcool.com/rmdir)
>
>### [3：系统管理](https://www.linuxcool.com/category/system)
>
>- [rpm命令 – RPM软件包管理器](https://www.linuxcool.com/rpm)
>- [find命令 – 查找和搜索文件](https://www.linuxcool.com/find)
>- [startx命令 – 初始化X-windows](https://www.linuxcool.com/startx)
>- [uname命令 – 显示系统信息](https://www.linuxcool.com/uname)
>- [vmstat命令 – 显示虚拟内存状态](https://www.linuxcool.com/vmstat)
>
>### [4：磁盘管理](https://www.linuxcool.com/category/disk)
>
>- [df命令 – 显示磁盘空间使用情况](https://www.linuxcool.com/df)
>- [fdisk命令 – 磁盘分区](https://www.linuxcool.com/fdisk)
>- [lsblk命令 – 查看系统的磁盘](https://www.linuxcool.com/lsblk)
>- [hdparm命令 – 显示与设定硬盘参数](https://www.linuxcool.com/hdparm)
>- [vgextend命令 – 扩展卷组](https://www.linuxcool.com/vgextend)
>
>### [5：文件传输](https://www.linuxcool.com/category/transfer)
>
>- [tftp命令 – 上传及下载文件](https://www.linuxcool.com/tftp)
>- [curl命令 – 文件传输工具](https://www.linuxcool.com/curl)
>- [fsck命令 – 检查并修复Linux文件系统](https://www.linuxcool.com/fsck)
>- [ftpwho命令 – 显示ftp会话信息](https://www.linuxcool.com/ftpwho)
>- [lprm命令 – 删除打印队列中的打印任务](https://www.linuxcool.com/lprm)
>
>### [6：网络通讯](https://www.linuxcool.com/category/network)
>
>- [ssh命令 – 安全连接客户端](https://www.linuxcool.com/ssh)
>- [netstat命令 – 显示网络状态](https://www.linuxcool.com/netstat)
>- [ping命令 – 测试主机间网络连通性](https://www.linuxcool.com/ping)
>- [dhclient命令 – 动态获取或释放IP地址](https://www.linuxcool.com/dhclient)
>- [ifconfig命令 – 显示或设置网络设备](https://www.linuxcool.com/ifconfig)
>
>### [7：设备管理](https://www.linuxcool.com/category/device)
>
>- [mount命令 – 文件系统挂载](https://www.linuxcool.com/mount)
>- [MAKEDEV命令 – 建立设备](https://www.linuxcool.com/makedev)
>- [lspci命令 – 显示当前设备所有PCI总线信息](https://www.linuxcool.com/lspci)
>- [setleds命令 – 设定键盘上方三个 LED 的状态](https://www.linuxcool.com/setleds)
>- [sensors命令 – 检测服务器内部温度及电压](https://www.linuxcool.com/sensors)
>
>### [8：备份压缩](https://www.linuxcool.com/category/backup)
>
>- [zip命令 – 压缩文件](https://www.linuxcool.com/zip)
>- [zipinfo命令 – 查看压缩文件信息](https://www.linuxcool.com/zipinfo)
>- [gzip命令 – 压缩和解压文件](https://www.linuxcool.com/gzip)
>- [unzip命令 – 解压缩zip文件](https://www.linuxcool.com/unzip)
>- [unarj命令 – 解压.arj文件](https://www.linuxcool.com/unarj)
>
>### [9：其他命令](https://www.linuxcool.com/category/other)
>
>- [hash命令 – 显示与清除命令运行时查询的哈希表](https://www.linuxcool.com/hash)
>- [wait命令 – 等待指令](https://www.linuxcool.com/wait)
>- [bc命令 – 浮点运算](https://www.linuxcool.com/bc)
>- [rmmod命令 – 删除模块](https://www.linuxcool.com/rmmod)
>- [history命令 – 显示与操纵历史命令](https://www.linuxcool.com/history)
>
>### [扩展：知识干货](https://www.linuxcool.com/category/knowledge)
>
>- [Red Hat Enterprise Linux 8/7/6/5/4 合集下载地址](https://www.linuxcool.com/rhel-download)
>- [中国程序员最容易读错的单词汇总（带正确发音示范）](https://www.linuxcool.com/pronunciation)



## linux查看内存、CPU命令

>### `free`
>
>1. total 表示总共有 7822MB 的物理内存(RAM)，即7.6G。
>2. used 表示物理内存的使用量，大约是 322M。
>3. free 表示空闲内存;
>4. shared 表示共享内存?;
>5. buff/cache 表示缓存和缓冲内存量; Linux 系统会将很多东西缓存起来以提高性能，这部分内存可以在必要时进行释放，给其他程序使用
>6. available 表示可用内存;
>
>----
>
>### `top` 命令
>
>`top` 命令一般用于查看进程的CPU和内存使用情况；当然也会报告内存总量，以及内存使用情况，所以可用来监控物理内存的使用情况。
>



## linux查看磁盘容量

>`df`命令的英文全称即`Disk Free`，顾名思义功能是用于显示系统上可使用的磁盘空间。默认显示单位为`KB`
>
>
>
>`df -h` 以容易阅读的方式显示  各文件系统会显示已用 可用 已用挂载点





## linux grep命令

>Linux系统中grep命令是一种强大的文本搜索工具，它能使用**正则表达式**搜索文本，并把匹配的行打印出来。它的使用权限是所有用户。
>
>----
>
>- **grep**是如何进行通信的？
>
>**grep**就是使用管道的数据进行查找的命令
>
>```undefined
>ps -ef | grep java			查找Java进程   | 称为管道符
>```
>
>-----
>
>- **从文件内容查找匹配指定字符串的行：**
>
>```
>$ grep "被查找的字符串" 文件名
>```



## linux查看日志命令 并查找关键字

>**cat ：** 命令  就是非常关键
>
>`cat whig.log`    查看全部内容并显示在终端上
>
>`cat whig.log root.log > new.log`  将 whig root 两个log文件合并在一个new.log文件当中
>
>------
>
>**`head`** 查看日志前n行
>
>`head -n 100 *.log`   					  查看log日志前100行
>
>`head *.log  | grep "关键字"`  	查找关键字所在的行数
>
>`grep "字符串" *.log` 					  也可以查看到关键字所在日志的行数
>
>-----
>
>**tail**     查看日志尾行
>
>```js
>tail  -n  10   test.log   查询日志尾部最后10行的日志;
>tail  -n +10   test.log   查询10行之后的所有日志;
>tail  -fn 10   test.log   循环实时查看最后1000行记录(最常用的)
>```

## linux 文件权限

>Linux下权限的粒度有 **拥有者 、群组 、其它组** 三种。每个文件都可以针对三个粒度，设置不同的rwx(读写执行)权限  
>
>可以利用chmod命令来操作
>
>我们规定 数字 4 、2 和 1表示读、写、执行权限即 r=4，w=2，x=1 。此时其他的权限组合也可以用其他的八进制数字表示出来，
>
>若要同时设置 rwx (可读写运行） 权限则将该权限位 设置 为 4 + 2 + 1 = 7     rwx = 4 + 2 + 1 = 7
>
>若要同时设置 rw- （可读写不可运行）权限则将该权限位 设置 为 4 + 2 = 6   rw = 4 + 2 = 6
>
>若要同时设置 r-x （可读可运行不可写）权限则将该权限位 设置 为 4 +1 = 5 rx = 4 +1 = 5
>
>每个文件都可以针对三个粒度，设置不同的rwx(读写执行)权限。即我们可以用用三个8进制数字分别表示 拥有者 、群组 、其它组( u、 g 、o)的权限详情，并用chmod直接加三个8进制数字的方式直接改变文件权限。语法格式为 ：
>
>```linux
>chmod <abc> file...
>```
>
>Abc 表示 user拥有者 group群组 other其它者    u g o  后边跟随8进制数字来表示对类型的执行权限



## linux终止进程 使用什么命令

>**查找进程：**
>
>`ps -ef | grep java`  查找java进程			`pgrep java` 该命令也是返回一个Java进程
>
>
>
>**杀死进程：**
>
>`kill -s 9 pid`   其中-s 9 制定了传递给进程的信号是９，即强制、尽快终止进程



## linux查看端口、打开端口

>`netstat` 命令用于显示各种网络相关信息，如网络连接，路由表，接口状态 `(Interface Statistics)，masquerade `连接，多播成员 `(Multicast Memberships) `等等。
>
>`netstat -anp|grep 8080 `   查看8080端口是否信息
>
>| -a   | 显示所有连线中的Socket                   |
>| ---- | ---------------------------------------- |
>| -p   | 显示正在使用Socket的程序识别码和程序名称 |
>| -u   | 显示UDP传输协议的连线状况                |
>| -i   | 显示网络界面信息表单                     |
>| -n   | 直接使用IP地址，不通过域名服务器         |
>
>-----
>
>**查看防火墙是否开启**
>
>```bash
>systemctl status firewalld
>```
>
>**若没有开启则开启**
>
>```bash
>systemctl start firewalld  #关闭则start改为stop
>```
>
>**查看所有开启的端口**
>
>```bash
>firewall-cmd --list-ports   # 注：启动防火墙后，默认没有开启任何端口，需要手动开启端口
>```
>
>**防火墙开启端口访问**
>
>```bash
>firewall-cmd --zone=public --add-port=80/tcp --permanent
>```
>
>命令含义：  --zone #作用域   --add-port=80/tcp #添加端口，格式为：端口/通讯协议   --permanent #永久生效，没有此参数重启后失效
>
>**开启端口号之后需要重启防火墙**
>
>```bash
> firewall-cmd --reload
>```





# linux概念、场景

## 僵尸进程 孤儿进程

>　　　　**孤儿进程：一个父进程退出，而它的一个或多个子进程还在运行，那么那些子进程将成为孤儿进程。孤儿进程将被init进程(进程号为1)所收养，并由init进程对它们完成状态收集工作。**
>
>　　　　　**僵尸进程：一个进程使用fork创建子进程，如果子进程退出，而父进程并没有调用wait或waitpid获取子进程的状态信息，那么子进程的进程描述符仍然保存在系统中。这种进程称之为僵死进程。**
>
>　　　任何一个子进程(init除外)在exit()之后，并非马上就消失掉，而是留下一个称为僵尸进程(Zombie)的数据结构，等待父进程处理。

## 某个命令很慢，怎么排查

>**主要是判断哪个正在运行的进程、CPU 运行状态、内存运行是否过载、交换内存区是否已满、以及硬盘是否运行正常等相关因素来进行判断。**
>
>1. 查看CPU信息    `cat /proc/cpuinfo`  
>2. 使用`top`命令检查`cpu`负载： 资源使用最高的进程排在前边。
>3. 使用`free`命令来检查闲置内存时间 实体内存，虚拟交换内存，共享内存，以及系统核心使用的缓冲区等等；



## 文件太大 无法使用cat查看该如何

>**场景：** 系统报警显示了时间，但是日志文件太大无法直接 cat 查看。(查询含有特定文本的文件，并拿到这些文本所在的行)
>
>解决：
>
>```
>grep -n '2019-10-24 00:01:11' *.log
>```
>
>查看符合条件的日志条目。



>二、课程设计题目及具体要求：
>（一）要求
>每人单独完成。
>具体要求包括：
>（1）分析题目要求，确定开发环境，整理出数据需求。
>（2）完成软件设计和数据库设计。
>（3）代码编写和系统测试。
>（4）整理、撰写课程设计报告。
>（5）总结设计，进行答辩。
>
>（二）课程设计题目：
>A.图书资源管理系统 (选题人数占班级总人数的80%)
>1系统概述：
>图书资源管理系统通过校园网发布，成为全校教职工和学生共享的信息资源，系统要求具有书目检索，热门推荐，新书通报，我的图书馆等模块。
>2 功能要求：
>⑴ 书目检索：馆藏检索，简单检索，多字段检索等子模块。
>① 馆藏检索：可以按任意词、题名、责任者、主题词、ISBN、分类号、索书号、出版社、丛书名进行检索，检索结果可以按相关度、入藏日期、题名、责任者、索书号、出版社、出版日期分别按升序和降序排列，并可设置每页显示的数量。
>
>② 简单检索：可以按题名、责任者、主题词、ISBN、分类号、索书号、出版社、丛书名进行简单检索。
>③ 多字段检索：可以按照馆藏书目的多个字段结合起来查询。
>⑵ 热门推荐：该模块统计2个月以内热门借阅、热门评分、热门收藏、热门图书等。
>⑶ 新书通报：根据图书分类查看相应的新书列表，包括经济、政治、科学、化学、其他等。
>⑷ 我的图书馆：要求读者登录，包括查询读者借阅历史，借书、还书、预约借书等功能。
>⑸ 读者管理：要求管理员登录，添加、修改、删除读者的相关信息。 
>(6)图书管理：要求管理员登录，添加、修改、删除馆藏书目的相关信息。
>(7)借还书管理：要求管理员登录，借书、还书操作
>    以上是需要完成的基本功能，可以设计增加一些功能。
>3 数据要求
>根据上述功能要求，分析数据要求，即需要用到哪些数据。
>4 概念设计
>根据数据进行概念设计，画出E-R图。
>5 逻辑设计
>根据概念设计完成逻辑设计，设计出关系模式以及完整性约束。
>6 数据库实现
>   在数据库中实现关系模式，并编写相应程序完成上述功能。
>7 需要提交的材料及时间：
>  (1)14周周五下午17:00之前,完成数据库的设计，画出E-R图，完成关系模式的设计，每位录制一段5-10分钟的视频说明自己的设计过程和设计要点，完成数据库设计文档，由学习委员将文档和视频收齐后交给各班负责老师。
>  (2)15周周四下午17:00之前,完成程序的设计，每位录制一段5-10分钟的视频说明自己的编程思路和实现要点，完成课程设计报告，由学习委员将报告和视频收齐后交给各班负责老师。
>
>B.网络设备管理系统 (选题人数占班级总人数的20%)
>1系统概述：
>网络设备管理系统面向公司管理、维护人员开放，系统具有设备检索、设备位置检索、设备登录、我管理的设备、故障提醒、故障报修、故障维修记录等功能。
>2.主要信息
>主要管理路由器、服务器、交换机三种设备，描述信息如下：
>① 设备信息 设备号  ID 名称 类型{路由器,交换机,服务器,...}
>② 设备类型 {属性:属性值}
> 路由器 出厂号 描述 位置 型号 操作系统 管理人 管理 管理IP 账号 密码 配置文件 故障维修记录 各端口的属性与连接信息 厂家 报修电话 维保日期 购买时间 录入人
> 服务器 出厂号 描述 位置 型号 CPU  内存 硬盘 操作系统 管理人 管理IP 账号 密码 故障维修记录 支持的服务与端口  厂家 报修电话 维保日期 购买时间 录入人 IP地址
> 交换机  出厂号 描述 位置 型号 管理人 IP  故障维修记录  厂家 报修电话 购买时间 录入人 网段 VLAN
>     位置信息包括 楼 房间 机架 机架中位置 维保日期
>     管理员信息包括  姓名 性别 工号 电话  用户名 密码
>     录入员 信息包括  姓名 性别 工号 电话  用户名 密码 部门
> 报修人员信息包括 姓名 性别 电话 用户名 密码 部门
>     故障维修记录 包括  故障级别（轻，重）  维修结果 维修记录 。级别轻：管理员重启、修改配置可修正的问题。级别重：需要更换硬件、维修硬件修改问题。  维修结果：已解决，暂时解决需要进一步维护，未解决。
>     服务信息：服务名称 服务类型 服务描述 协议 端口号
>
>3.设备检索功能
> 检索所有设备，按设备号、设备名称厂家等信息查找设备
> 检索我的设备，管理员查找自己管理的设备
> 设备位置检索，按设备位置检索设备
> 设备分类检索，按设备类型检索设备
> 按服务检索 ，按服务类型、端口号、协议检索信息
>
> 查看设备信息，按设备类型显示设备详细信息
>4.设备管理
> 设备录入，管理员、录入员可以录入设备信息、
> 管理员可以维护设备信息、服务信息、管理信息、故障维修记录
> 设备移交，管理员可以将设备移交给其他管理员
>
>
